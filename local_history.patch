Index: Gan/GAN - Facedata-color.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Gan/GAN - Facedata-color.py	(date 1591012031949)
+++ Gan/GAN - Facedata-color.py	(date 1591012031949)
@@ -0,0 +1,291 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+# In[1]:
+
+
+#Import
+from keras.datasets import mnist
+from keras.utils import np_utils
+from keras.models import Sequential, Model
+from keras.layers import Input, Dense, Dropout, Activation, Flatten
+from keras.layers.advanced_activations import LeakyReLU
+from keras.optimizers import Adam, RMSprop
+import numpy as np
+import matplotlib.pyplot as plt
+from numpy.random import randn
+from numpy.random import randint
+from numpy.random import rand
+from numpy import zeros
+from numpy import ones
+from matplotlib import pyplot
+import random
+from numpy import vstack
+from keras.preprocessing.image import ImageDataGenerator
+from keras.layers import Conv2D
+from keras.layers import Conv2DTranspose
+from keras.layers import Reshape
+
+
+#Input dimension generator
+input_dim = 100
+
+
+# In[2]:
+
+
+
+# Optimizer
+adam = Adam(lr=0.0002, beta_1=0.5)
+
+def define_discriminator(in_shape=(128,128,3)):
+    discriminator = Sequential()
+    discriminator.add(Conv2D(64, (3,3), padding='same', data_format="channels_last", strides=(2, 2), input_shape=in_shape , activation=LeakyReLU(alpha=0.2)))
+    discriminator.add(Dropout(0.4))
+    discriminator.add(Conv2D(64, (3,3),padding='same', data_format="channels_last", strides=(2, 2), activation=LeakyReLU(alpha=0.2)))
+    discriminator.add(Dropout(0.4))
+    discriminator.add(Conv2D(64, (3,3),padding='same', data_format="channels_last", strides=(2, 2), activation=LeakyReLU(alpha=0.2)))
+    discriminator.add(Dropout(0.4))
+    discriminator.add(Flatten())
+    discriminator.add(Dense(1, activation='sigmoid'))  
+    discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
+    return discriminator
+
+
+# In[3]:
+
+
+discriminator = define_discriminator()
+discriminator.summary()
+
+
+# In[4]:
+
+
+def define_generator(latent_dim):
+    model = Sequential()
+    # foundation for 8x8 image
+    n_nodes = 128 * 8 * 8
+    model.add(Dense(n_nodes, input_dim=latent_dim))
+    model.add(LeakyReLU(alpha=0.2))
+    model.add(Reshape((8, 8, 128)))
+    # upsample to 16x16
+    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
+    model.add(LeakyReLU(alpha=0.2))
+    # upsample to 32x32
+    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
+    model.add(LeakyReLU(alpha=0.2))
+    # upsampling to 64x64
+    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
+    model.add(LeakyReLU(alpha=0.2))
+    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
+    model.add(LeakyReLU(alpha=0.2))
+    model.add(Conv2D(3, (7,7), activation='tanh', padding='same'))
+    return model
+
+
+# In[5]:
+
+
+generator = define_generator(100)
+generator.summary()
+
+
+# In[6]:
+
+
+#GAN-model
+discriminator.trainable = False
+inputs = Input(shape=(input_dim, ))
+hidden = generator(inputs)
+output = discriminator(hidden)
+gan = Model(inputs, output)
+gan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
+gan.summary()
+
+
+# In[7]:
+
+
+# create a data generator
+datagen = ImageDataGenerator()
+
+x_train = datagen.flow_from_directory('C:/Users/eitn35/Documents/EITN35/video_files/frames/CycleGan/DayPersons_medium/'
+                                       ,class_mode=None, color_mode = 'rgb',target_size=(128, 128), batch_size=38)
+
+
+# In[8]:
+
+
+
+def newImages(x_train=x_train):
+    x = next(x_train)
+    for i in range(38):
+        x[i] = x[i].astype('float32')
+        x[i] = x[i] / 127.5 - 1
+    return x
+
+
+# In[9]:
+
+
+x= newImages()
+
+
+# In[10]:
+
+
+
+# select real samples
+def generate_real_samples(x_train, n_samples):
+    # choose random instances
+    ix = randint(0, x_train.shape[0], n_samples)
+    # retrieve selected images
+    X = x_train[ix]
+    # generate 'real' class labels (1)
+    X.reshape(n_samples, 128,128,3)
+    y = ones((n_samples, 1))
+    return X, y
+   
+# generate n noise samples with class labels
+def generate_noise_samples(n_samples):
+    # generate uniform random numbers in [0,1]
+    X = rand(16384* 3 * n_samples)
+    # reshape into a batch of grayscale images
+    X = X.reshape((n_samples, 16384*3))
+    X = X.reshape((n_samples, 128, 128, 3))
+    # generate 'fake' class labels (0)
+    y = zeros((n_samples, 1))
+    return X, y
+
+    # generate points in latent space as input for the generator
+def generate_latent_points(input_dim, n_samples):
+    # generate points in the latent space
+    x_input = randn(input_dim * n_samples)
+    # reshape into a batch of inputs for the network
+    x_input = x_input.reshape(n_samples, input_dim)
+    return x_input
+
+# use the generator to generate n fake examples, with class labels
+def generate_fake_samples(model, latent_dim, n_samples):
+    # generate points in latent space
+    x_input = generate_latent_points(latent_dim, n_samples)
+    # predict outputs
+    x = model.predict(x_input)
+    # create 'fake' class labels (0)
+    y = zeros((n_samples, 1))
+    return x, y
+
+
+# In[11]:
+
+
+
+def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):
+    # prepare real samples
+    x_real, y_real = generate_real_samples(dataset, n_samples)
+    # evaluate discriminator on real examples
+    _, acc_real = d_model.evaluate(x_real, y_real, verbose=0)
+    # prepare fake examples
+    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)
+    # evaluate discriminator on fake examples
+    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)
+    # summarize discriminator performance
+    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))
+
+
+# In[12]:
+
+
+def plot_result(k):
+    n_samples = 5
+    latent_points = generate_latent_points(input_dim, n_samples)
+    x = generator.predict(latent_points)
+    x = (x + 1) / 2.0
+    for i in range(n_samples):
+        pyplot.subplot(2, n_samples, 1  + i)
+        pyplot.axis('off')
+        pyplot.imshow(x[i])
+    # save plot to file
+    filename1 = 'generated_plot_%06d.png' % ((k + 1))
+    pyplot.savefig(filename1)
+    pyplot.close()
+    
+
+# save the generator models to file
+def save_models(i, g_model_AtoB):
+	# save the first generator model
+	filename1 = 'g_model%06d.h5' % (i+1)
+	g_model_AtoB.save(filename1)
+	# save the second generator model
+	print('>Saved: %s' % (filename1))
+
+
+# In[13]:
+
+
+#Train discriminator
+def train_discriminator(model, number_iteration, batch_size, x_train):
+
+    half_batch = int(batch_size/2)
+    for i in range(number_iteration):
+        #x_train = newImages()
+        #Sample from real images
+        x_real, y_real = generate_real_samples(x_train, half_batch)
+        #Sample from noise
+        x_noise, y_noise = generate_noise_samples((half_batch))
+        #Train with real images
+        _, real_acc = model.train_on_batch(x_real, y_real)
+        #Train with noise
+        _, noise_acc = model.train_on_batch(x_noise, y_noise)
+        print('>%d real=%.0f%% noise=%.0f%%' % (i+1, real_acc*100, noise_acc*100))
+
+
+# In[14]:
+
+
+def train_gan(generator, discriminator, gan, input_dim, n_epochs, batch_size):
+    x_train = newImages()
+    half_batch = int(batch_size/2)
+    batch_per_epoch = int(136/x_train.shape[0])
+    for i in range(n_epochs):
+        if(i%20 == 0):
+            plot_result(i)
+            save_models(i, generator)
+        x_train = datagen.flow_from_directory('C:/Users/eitn35/Documents/EITN35/video_files/frames/CycleGan/DayPersons_medium/'
+                                       ,class_mode=None, color_mode = 'rgb',target_size=(128, 128), batch_size=38)
+        for k in range(batch_per_epoch):
+            x_train = newImages()
+            x_real, y_real = generate_real_samples(x_train, half_batch)
+            x_fake, y_fake = generate_fake_samples(generator, input_dim, half_batch)
+            x, y = vstack((x_real, x_fake)), vstack((y_real, y_fake))
+            discriminator_loss = discriminator.train_on_batch(x, y)
+            x_gan = generate_latent_points(input_dim, batch_size)
+            y_gan = ones((batch_size, 1))
+            generator_loss = gan.train_on_batch(x_gan, y_gan)
+            print('>%d, d=%.3f, g=%.3f' % (i+1, discriminator_loss[0], generator_loss[0]))
+            #if (j==1):
+            #summarize_performance(i, generator, discriminator, x_train, input_dim)
+
+
+# In[15]:
+
+
+x_train = newImages()
+train_discriminator(discriminator, 25, 34, x)
+
+#15ggr
+
+
+# In[16]:
+
+
+train_gan(generator,discriminator,gan,input_dim,5000, 34)
+
+
+# In[19]:
+
+
+
+
+
Index: Old/CNN_baseline.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Old/CNN_baseline.py	(date 1591012031999)
+++ Old/CNN_baseline.py	(date 1591012031999)
@@ -0,0 +1,366 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+# In[1]:
+
+
+import cv2
+import numpy as np
+import pandas as pd
+import sklearn
+import matplotlib.pyplot as plt
+
+import os
+import random
+import gc #garbage collector for cleaning deleted data from memory
+
+
+# In[2]:
+
+
+train_dir = 'C:/Users/eitn35/Documents/EITN35/video_files/frames/Small_train_set/'
+test_dir = 'C:/Users/eitn35/Documents/EITN35/video_files/frames/Small_test_set/'
+
+#rain_person = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'persons_1' in i]  #get person images
+#rain_dogs = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'dogs_1' in i]  #get dog images
+#rain_bikes = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'bikes_1' in i]  #get bike images
+#rain_empty = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if ('persons_0'and 'dogs_0' and 'bikes_0')in i]  #get bike images
+
+
+test_imgs = [test_dir+'{}'.format(i) for i in os.listdir(test_dir)] #get test images
+
+
+#train_imgs = train_person + train_dogs + train_bikes + train_empty   # slice the dataset and use 3 persons
+train_imgs = [train_dir+'{}'.format(i) for i in os.listdir(train_dir)] #get test images
+random.shuffle(train_imgs)  # shuffle it randomly
+random.shuffle(test_imgs)
+
+
+gc.collect()   #collect garbage to save memory
+
+
+# In[3]:
+
+
+import matplotlib.image as mpimg
+for ima in train_imgs[0:4]:
+     img=mpimg.imread(ima)
+     imgplot = plt.imshow(img)
+     plt.show()
+
+
+# In[4]:
+
+
+#Lets declare our image dimensions
+#we are using coloured images. 
+nrows = 640
+ncolumns = 360
+channels = 3  #change to 1 if you want to use grayscale image
+
+#A function to read and process the images to an acceptable format for our model
+def read_and_process_image(list_of_images):
+    """
+    Returns two arrays: 
+        X is an array of resized images
+        y is an array of labels
+    """
+    X = [] # images
+    y = []# labels
+    i = 0
+    for image in list_of_images:
+        #ändra här mellan COLOR och GRAYSCALE beroende på antal channels
+        X.append(cv2.resize(cv2.imread(image,cv2.IMREAD_COLOR), (nrows,ncolumns), interpolation=cv2.INTER_CUBIC))  #Read the image
+        #get the labels
+        if 'persons_1' in image:
+            y.append(1)
+        elif 'dogs_1' in image:
+            y.append(2)
+        elif 'bikes_1' in image:
+            y.append(3)
+        else:
+            y.append(0)
+        i += 1
+    return X, y
+
+
+# In[5]:
+
+
+class_names = ['empty', 'person', 'dogs', 'bikes']
+
+
+# In[6]:
+
+print('reading dataset...')
+X, y = read_and_process_image(train_imgs)
+X_test, y_test = read_and_process_image(test_imgs)
+
+
+# In[7]:
+
+
+y[0]
+
+
+# In[8]:
+
+
+#Lets view some of the pics
+plt.figure(figsize=(20,10))
+columns = 4
+for i in range(columns):
+    plt.subplot(5 / columns + 1, columns, i + 1)
+    plt.imshow(X[i])
+
+
+# In[9]:
+
+
+import seaborn as sns
+
+gc.collect()
+
+#Convert list to numpy array
+X = np.array(X)
+y = np.array(y)
+X_test = np.array(X_test)
+y_test = np.array(y_test)
+#Lets plot the label to be sure we just have two class
+#sns.countplot(y)
+#plt.title('Labels for Cats and Dogs')
+
+
+# In[10]:
+
+
+print("Shape of train images is:", X.shape)
+print("Shape of labels is:", y.shape)
+
+
+# In[11]:
+
+
+#Lets split the data into train and test set
+from sklearn.model_selection import train_test_split
+X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=2)
+
+print("Shape of train images is:", X_train.shape)
+print("Shape of validation images is:", X_val.shape)
+print("Shape of labels is:", y_train.shape)
+print("Shape of labels is:", y_val.shape)
+
+
+# In[12]:
+
+
+#clear memory
+#del X
+#del y
+gc.collect()
+
+#get the length of the train and validation data
+ntrain = len(X_train)
+nval = len(X_val)
+
+#We will use a batch size of 32. Note: batch size should be a factor of 2.***4,8,16,32,64...***
+batch_size = 32
+
+
+# In[13]:
+
+
+
+#from keras import models
+#from keras import optimizers
+from tensorflow.keras import models, layers
+from keras.preprocessing.image import ImageDataGenerator
+from keras.preprocessing.image import img_to_array, load_img
+import tensorflow as tf
+
+model = models.Sequential()
+model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(ncolumns, nrows, 3))) #input ska var (150, 150, 3)
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(64, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Flatten())
+model.add(layers.Dropout(0.4))  #Dropout for regularization
+model.add(layers.Dense(512, activation='relu'))
+model.add(layers.Dense(4))  #Sigmoid function at the end because we have just two classes
+
+
+# In[14]:
+
+
+#Lets see our model
+model.summary()
+
+
+# In[15]:
+
+
+#We'll use the RMSprop optimizer with a learning rate of 0.0001
+#We'll use binary_crossentropy loss because its a binary classification
+model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])
+
+
+# In[16]:
+
+
+#Lets create the augmentation configuration
+#This helps prevent overfitting, since we are using a small dataset
+#train_datagen = ImageDataGenerator(rescale=1./255,   #Scale the image between 0 and 1
+ #                                   rotation_range=40,
+  #                                  width_shift_range=0.2,
+   #                                 height_shift_range=0.2,
+    #                                shear_range=0.2,
+     #                               zoom_range=0.2,
+      #                              horizontal_flip=True,)
+
+#val_datagen = ImageDataGenerator(rescale=1./255)  #We do not augment validation data. we only perform rescale
+
+
+# In[17]:
+
+
+
+#Create the image generators
+#train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)
+#val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)
+
+
+# In[18]:
+
+
+
+#The training part
+#We train for 64 epochs with about 100 steps per epoch
+history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))
+
+
+# In[ ]:
+
+
+#Save the model
+model.save_weights('model_cat&dog1_weights.h5')
+model.save('model_cat&dog1_keras.h5')
+
+
+# In[ ]:
+
+
+#lets plot the train and val curve
+#get the details form the history object
+acc = history.history['acc']
+val_acc = history.history['val_acc']
+loss = history.history['loss']
+val_loss = history.history['val_loss']
+
+epochs = range(1, len(acc) + 1)
+
+#Train and validation accuracy
+#plt.plot(epochs, acc, 'b', label='Training accurarcy')
+#plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
+#plt.title('Training and Validation accurarcy')
+#plt.legend()
+
+#plt.figure()
+
+#Train and validation loss
+#plt.plot(epochs, loss, 'b', label='Training loss')
+#plt.plot(epochs, val_loss, 'r', label='Validation loss')
+#plt.title('Training and Validation loss')
+#plt.legend()
+
+#plt.show()
+
+
+# In[ ]:
+
+
+probability_model = tf.keras.Sequential([model,
+                                         tf.keras.layers.Softmax()])
+
+predictions = probability_model.predict(X_test)
+
+
+
+
+
+def plot_image(i, predictions_array, true_label, img):
+    predictions_array, true_label, img = predictions_array, true_label[i], img[i]
+    plt.grid(False)
+    plt.xticks([])
+    plt.yticks([])
+    plt.imshow(img, cmap=plt.cm.binary)
+
+    predicted_label = np.argmax(predictions_array)
+    if predicted_label == true_label:
+        color = 'blue'
+    else:
+        color = 'red'
+
+    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
+                                100*np.max(predictions_array),
+                                class_names[true_label]),
+                                color=color)
+
+def plot_value_array(i, predictions_array, true_label):
+    predictions_array, true_label = predictions_array, true_label[i]
+    plt.grid(False)
+    plt.xticks(range(4))
+    plt.yticks([])
+    thisplot = plt.bar(range(4), predictions_array, color="#777777")
+    plt.ylim([0, 1])
+    predicted_label = np.argmax(predictions_array)
+
+    thisplot[predicted_label].set_color('red')
+    thisplot[true_label].set_color('blue')
+
+# Plot the first X test images, their predicted labels, and the true labels.
+# Color correct predictions in blue and incorrect predictions in red.
+num_rows = 5
+num_cols = 3
+num_images = num_rows*num_cols
+plt.figure(figsize=(2*2*num_cols, 2*num_rows))
+for i in range(num_images):
+  plt.subplot(num_rows, 2*num_cols, 2*i+1)
+  plot_image(i, predictions[i], y_test, X_test)
+  plt.subplot(num_rows, 2*num_cols, 2*i+2)
+  plot_value_array(i, predictions[i], y_test)
+plt.tight_layout()
+plt.show()
+
+
+# In[ ]:
+
+
+probability_model = tf.keras.Sequential([model,
+                                         tf.keras.layers.Softmax()])
+
+predictions = probability_model.predict(X_train)
+
+predictions[0]
+
+np.argmax(predictions[0])
+
+y_train[0]
+
+num_rows = 5
+num_cols = 3
+num_images = num_rows*num_cols
+plt.figure(figsize=(2*2*num_cols, 2*num_rows))
+for i in range(num_images):
+  plt.subplot(num_rows, 2*num_cols, 2*i+1)
+  plot_image(i, predictions[i], y_train, X_train)
+  plt.subplot(num_rows, 2*num_cols, 2*i+2)
+  plot_value_array(i, predictions[i], y_train)
+plt.tight_layout()
+plt.show()
+
+
+
Index: Old/object_detection_v4.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Old/object_detection_v4.py	(date 1591012031993)
+++ Old/object_detection_v4.py	(date 1591012031993)
@@ -0,0 +1,420 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+# Object Detection With YOLOv3 in Keras
+
+# load yolov3 model and perform object detection
+# based on https://github.com/experiencor/keras-yolo3
+import numpy as np
+from numpy import expand_dims
+from keras.models import load_model
+from keras.preprocessing.image import load_img
+from keras.preprocessing.image import img_to_array
+from matplotlib import pyplot
+from matplotlib.patches import Rectangle
+import pandas as pd
+import os
+import math
+#import tensorflow.compat.v1 as tf
+#tf.disable_v2_behavior()
+
+#directory_1 = "/Users/august/Documents/EITN35_AIQ"
+directory_1 = "../EITN35_Resources/"
+os.chdir(directory_1)
+
+
+class BoundBox:
+    def __init__(self, xmin, ymin, xmax, ymax, objness=None, classes=None):
+        self.xmin = xmin
+        self.ymin = ymin
+        self.xmax = xmax
+        self.ymax = ymax
+        self.objness = objness
+        self.classes = classes
+        self.label = -1
+        self.score = -1
+
+    def get_label(self):
+        if self.label == -1:
+            self.label = np.argmax(self.classes)
+
+        return self.label
+
+    def get_score(self):
+        if self.score == -1:
+            self.score = self.classes[self.get_label()]
+
+        return self.score
+
+
+def _sigmoid(x):
+    return 1. / (1. + np.exp(-x))
+
+
+def decode_netout(netout, anchors, obj_thresh, net_h, net_w):
+    grid_h, grid_w = netout.shape[:2]
+    nb_box = 3
+    netout = netout.reshape((grid_h, grid_w, nb_box, -1))
+    nb_class = netout.shape[-1] - 5
+    boxes = []
+    netout[..., :2] = _sigmoid(netout[..., :2])
+    netout[..., 4:] = _sigmoid(netout[..., 4:])
+    netout[..., 5:] = netout[..., 4][..., np.newaxis] * netout[..., 5:]
+    netout[..., 5:] *= netout[..., 5:] > obj_thresh
+
+    for i in range(grid_h * grid_w):
+        row = i / grid_w
+        col = i % grid_w
+        for b in range(nb_box):
+            # 4th element is objectness score
+            objectness = netout[int(row)][int(col)][b][4]
+            if (objectness.all() <= obj_thresh): continue
+            # first 4 elements are x, y, w, and h
+            x, y, w, h = netout[int(row)][int(col)][b][:4]
+            x = (col + x) / grid_w  # center position, unit: image width
+            y = (row + y) / grid_h  # center position, unit: image height
+            w = anchors[2 * b + 0] * np.exp(w) / net_w  # unit: image width
+            h = anchors[2 * b + 1] * np.exp(h) / net_h  # unit: image height
+            # last elements are class probabilities
+            classes = netout[int(row)][col][b][5:]
+            box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness, classes)
+            boxes.append(box)
+    return boxes
+
+
+def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):
+    new_w, new_h = net_w, net_h
+    for i in range(len(boxes)):
+        x_offset, x_scale = (net_w - new_w) / 2. / net_w, float(new_w) / net_w
+        y_offset, y_scale = (net_h - new_h) / 2. / net_h, float(new_h) / net_h
+        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)
+        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)
+        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)
+        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)
+
+
+def _interval_overlap(interval_a, interval_b):
+    x1, x2 = interval_a
+    x3, x4 = interval_b
+    if x3 < x1:
+        if x4 < x1:
+            return 0
+        else:
+            return min(x2, x4) - x1
+    else:
+        if x2 < x3:
+            return 0
+        else:
+            return min(x2, x4) - x3
+
+
+def bbox_iou(box1, box2):
+    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])
+    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])
+    intersect = intersect_w * intersect_h
+    w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin
+    w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin
+    union = w1 * h1 + w2 * h2 - intersect
+    return float(intersect) / union
+
+
+def do_nms(boxes, nms_thresh):
+    if len(boxes) > 0:
+        nb_class = len(boxes[0].classes)
+    else:
+        return
+    for c in range(nb_class):
+        sorted_indices = np.argsort([-box.classes[c] for box in boxes])
+        for i in range(len(sorted_indices)):
+            index_i = sorted_indices[i]
+            if boxes[index_i].classes[c] == 0: continue
+            for j in range(i + 1, len(sorted_indices)):
+                index_j = sorted_indices[j]
+                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:
+                    boxes[index_j].classes[c] = 0
+
+
+# load and prepare an image
+def load_image_pixels(filename, shape):
+    # load the image to get its shape
+    image = load_img(filename)
+    width, height = image.size
+    # load the image with the required size
+    image = load_img(filename, target_size=shape)
+    # convert to numpy array
+    image = img_to_array(image)
+    # scale pixel values to [0, 1]
+    image = image.astype('float32')
+    image /= 255.0
+    # add a dimension so that we have one sample
+    image = expand_dims(image, 0)
+    return image, width, height
+
+
+# get all of the results above a threshold
+def get_boxes(boxes, labels, thresh):
+    v_boxes, v_labels, v_scores = list(), list(), list()
+    # enumerate all boxes
+    for box in boxes:
+        # enumerate all possible labels
+        for i in range(len(labels)):
+            # check if the threshold for this label is high enough
+            if box.classes[i] > thresh:
+                v_boxes.append(box)
+                v_labels.append(labels[i])
+                v_scores.append(box.classes[i] * 100)
+            # don't break, many labels may trigger for one box
+    return v_boxes, v_labels, v_scores
+
+
+# draw all results
+def draw_boxes(filename, v_boxes, v_labels, v_scores):
+    # load the image
+    data = pyplot.imread(filename)
+    # plot the image
+    pyplot.imshow(data)
+    # get the context for drawing boxes
+    ax = pyplot.gca()
+    # plot each box
+    for i in range(len(v_boxes)):
+        box = v_boxes[i]
+        # get coordinates
+        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax
+        # calculate width and height of the box
+        width, height = x2 - x1, y2 - y1
+        # create the shape
+        rect = Rectangle((x1, y1), width, height, fill=False, color='white', lw=2)
+        # draw the box
+        ax.add_patch(rect)
+        # draw text and score in top left corner
+        label = "%s (%.3f)" % (v_labels[i], v_scores[i])
+        pyplot.text(x1, y1, label, color='white')
+    # show the plot
+    #pyplot.show()
+
+
+# load yolov3 model
+model = load_model('model.h5', compile=False)
+# define the expected input shape for the model
+input_w, input_h = 416, 416
+
+# In[77]:
+
+
+# define the labels
+labels = ["person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck",
+          "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench",
+          "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe",
+          "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard",
+          "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",
+          "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana",
+          "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake",
+          "chair", "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse",
+          "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator",
+          "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"]
+
+# define the probability threshold for detected objects
+class_threshold = 0.6
+dist_threshold = 150  # gör denna beroende på framerate som valts
+
+
+# In[78]:
+
+
+class Mem_Object:
+    def __init__(self, label, xmin, xmax, ymin, ymax):
+        self.label = label
+        self.xmin = xmin
+        self.xmax = xmax
+        self.ymin = ymin
+        self.ymax = ymax
+        self.mid = [(xmin + xmax) / 2, (ymin + ymax) / 2]
+
+
+# In[79]:
+
+
+def dist(p1, p2):
+    distance = math.sqrt(((p1[0] - p2[0]) ** 2) + ((p1[1] - p2[1]) ** 2))
+    return distance
+
+
+# In[80]:
+
+
+def check_new_object(v_labels, v_boxes, v_scores, frames_memory):
+    print("Inside check_new_object")
+
+    # create temp counter for this frames
+    temp_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+    new_objects = pd.DataFrame(data=0, columns=range(10), index=['T1'])
+
+    # add all new objects from v_labels with attributes
+    for i in range(len(v_labels)):
+        new_objects[i]['T1'] = Mem_Object(v_labels[i], v_boxes[i].xmin, v_boxes[i].xmax, v_boxes[i].ymin,
+                                          v_boxes[i].ymax)
+        print("Label i objects" + v_labels[i])
+
+    print("Length of new_objects: " + str(len(new_objects)))
+    print("Content of new_objects" + str(new_objects) + "... End of contents")
+    # print("Content of new_objects" + new_objects)
+
+    # loop through new objects and compare to last frame
+    for i in new_objects:
+        # loop until zero -> no more objects
+        if new_objects[i][0] == 0: break  # eller continue
+
+        temp_counter.loc[str(v_labels[i])] += 1
+        # print("New object up counted!")
+        # print("Temp counter inside function: ")
+        # print(temp_counter)
+
+        # placehold_count += 1 #hundar eller persons
+        uniqueObj = True
+
+        # current frame obj
+        cf_obj = new_objects[i][0]
+        print("Mid new object" + str(cf_obj.mid))
+
+        # Check memory for objects in last frame, if same type of object but too close, count down!
+        for j in frames_memory:
+            lf_obj = frames_memory[j][0]  # index 0 is 'F1'
+
+            if (lf_obj != 0):
+
+                if (lf_obj.label == cf_obj.label):
+                    print("Mid old object" + str(lf_obj.mid))
+                    distance = dist(lf_obj.mid, cf_obj.mid)
+                    print("Distance between objects is: " + str(distance))
+
+                    # if same type of object is too close, conclude same object, count down.
+                    if (distance < dist_threshold):
+                        temp_counter.loc[str(v_labels[i])] -= 1
+                        print("Same object!")
+                        uniqueObj = False
+
+        if uniqueObj:
+            print("New object!")  # syns ej på första
+
+    # Update frames_memory
+    for i in range(4):
+        frames_memory.loc['F' + str(5 - i)] = frames_memory.loc['F' + str(4 - i)]
+
+    # Overwrite first row with new objetcts
+    frames_memory.loc['F1'] = new_objects.loc['T1']
+
+    # frames_memory[rand.randint(5,9)]['F1'] = Mem_Object("person", 1, 1, 1, 1)
+    # print("Temp counter")
+    # print(temp_counter)
+    # print("Frames memory")
+    # print(frames_memory)
+    return temp_counter
+
+frames_memory = pd.DataFrame(data=0, columns=range(10), index=['F1', 'F2', 'F3', 'F4', 'F5'])
+# Ladda in ett objekt
+frames_memory
+
+big_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+
+#Dir to be iterated
+directory_2 = "/Users/august/Documents/EITN35_AIQ/video_files/frames/"
+directory_3 = "/Users/august/Documents/EITN35_AIQ/video_files/test_set/"
+os.chdir(directory_2)
+
+# Create "unlabeled_images" folder if it does not exist
+try:
+    if not os.path.exists('unlabeled_images'):
+        os.makedirs('unlabeled_images')
+    if not os.path.exists('autolabeled_images'):
+        os.makedirs('autolabeled_images')
+except OSError:
+    print('Error: Creating directory of data')
+
+#MÅSTE GÖRAS LÄNGRE OM VI HITTAR MER ÄN ETT OBJEKT I VARJE BILD, GÖR HELLRE FÖR LÅNG OCH DROPPA
+annotation_df = pd.DataFrame(data=0,index=np.arange(len(os.listdir(directory_3))),columns="image xmin ymin xmax ymax label".split())
+index = 0
+
+# Store found objects and their positions from previous frame in vector.
+# When new object found compare type and position, then decide to count or not
+# lf_memory = pd.DataFrame(0,columns=['Label','Middle'],index=range(10))
+
+
+for photo_filename in os.listdir(directory_2):
+    if not photo_filename.endswith('jpg'):continue
+    #photo_filename = 'frame_' + str(i + 8) + '.jpg'
+    # define our new photoc
+    # photo_filename = 'man_on_scooter.jpg'
+    # load and prepare image
+    image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))
+    # make prediction
+    yhat = model.predict(image)
+
+    # summarize the shape of the list of arrays
+    print([a.shape for a in yhat])
+
+    # define the anchors
+    anchors = [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]
+
+    boxes = list()
+    for i in range(len(yhat)):
+        # decode the output of the network
+        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)
+    # correct the sizes of the bounding boxes for the shape of the image
+    correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)
+    # suppress non-maximal boxes
+    do_nms(boxes, 0.5)
+
+    # get the details of the detected objects
+    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)
+
+    contains_person = False
+    #load dataframe for csv export
+    for i in range(len(v_labels)):
+        if v_labels[i] == 'person':
+            annotation_df['image'][index] = photo_filename
+            annotation_df['xmin'][index] = v_boxes[i].xmin
+            annotation_df['ymin'][index] = v_boxes[i].ymin
+            annotation_df['xmax'][index] = v_boxes[i].xmax
+            annotation_df['ymax'][index] = v_boxes[i].ymax
+            annotation_df['label'][index] = v_labels[i]
+            index += 1
+            contains_person = True #Om vi har hittat en instance av person flytta inte file till unlabled_files
+
+    # summarize what we found
+    temp_2_counter = check_new_object(v_labels, v_boxes, v_scores, frames_memory)
+
+    # print(temp_2_counter)
+    big_counter.loc["person"] = big_counter.loc["person"] + temp_2_counter.loc["person"]
+    big_counter.loc["dog"] = big_counter.loc["dog"] + temp_2_counter.loc["dog"]
+
+    # draw what we found
+    draw_boxes(photo_filename, v_boxes, v_labels, v_scores)
+    print(str(big_counter))
+
+    #move files that couldn't be labeled with YOLO
+    if (not contains_person):
+        os.rename(
+            directory_3 + photo_filename,
+            directory_3 + 'unlabeled_images/' + photo_filename
+        )
+
+    #print("THIS IS ANNOTATIONS DF")
+    print(str(annotation_df))
+
+
+
+
+#Make CSV-file from auto-annotations
+annotation_df.to_csv('/Users/august/Documents/EITN35_AIQ/Annotations-export_YOLO_auto.csv', index=False, header=True)
+
+
+# In[90]:
+
+
+big_counter.loc['dog']
+
+# In[ ]:
+
+
+
+
Index: Old/object_detection_v5.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Old/object_detection_v5.py	(date 1591012031990)
+++ Old/object_detection_v5.py	(date 1591012031990)
@@ -0,0 +1,430 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+# Object Detection With YOLOv3 in Keras
+
+# load yolov3 model and perform object detection
+# based on https://github.com/experiencor/keras-yolo3
+import numpy as np
+from numpy import expand_dims
+from keras.models import load_model
+from keras.preprocessing.image import load_img
+from keras.preprocessing.image import img_to_array
+from matplotlib import pyplot
+from matplotlib.patches import Rectangle
+import pandas as pd
+import os
+import math
+#import tensorflow.compat.v1 as tf
+#tf.disable_v2_behavior()
+
+#directory_1 = "/Users/august/Documents/EITN35_AIQ"
+directory_1 = "../EITN35_Resources/"
+directory_1 = ""
+os.chdir(directory_1)
+
+
+class BoundBox:
+    def __init__(self, xmin, ymin, xmax, ymax, objness=None, classes=None):
+        self.xmin = xmin
+        self.ymin = ymin
+        self.xmax = xmax
+        self.ymax = ymax
+        self.objness = objness
+        self.classes = classes
+        self.label = -1
+        self.score = -1
+
+    def get_label(self):
+        if self.label == -1:
+            self.label = np.argmax(self.classes)
+
+        return self.label
+
+    def get_score(self):
+        if self.score == -1:
+            self.score = self.classes[self.get_label()]
+
+        return self.score
+
+
+def _sigmoid(x):
+    return 1. / (1. + np.exp(-x))
+
+
+def decode_netout(netout, anchors, obj_thresh, net_h, net_w):
+    grid_h, grid_w = netout.shape[:2]
+    nb_box = 3
+    netout = netout.reshape((grid_h, grid_w, nb_box, -1))
+    nb_class = netout.shape[-1] - 5
+    boxes = []
+    netout[..., :2] = _sigmoid(netout[..., :2])
+    netout[..., 4:] = _sigmoid(netout[..., 4:])
+    netout[..., 5:] = netout[..., 4][..., np.newaxis] * netout[..., 5:]
+    netout[..., 5:] *= netout[..., 5:] > obj_thresh
+
+    for i in range(grid_h * grid_w):
+        row = i / grid_w
+        col = i % grid_w
+        for b in range(nb_box):
+            # 4th element is objectness score
+            objectness = netout[int(row)][int(col)][b][4]
+            if (objectness.all() <= obj_thresh): continue
+            # first 4 elements are x, y, w, and h
+            x, y, w, h = netout[int(row)][int(col)][b][:4]
+            x = (col + x) / grid_w  # center position, unit: image width
+            y = (row + y) / grid_h  # center position, unit: image height
+            w = anchors[2 * b + 0] * np.exp(w) / net_w  # unit: image width
+            h = anchors[2 * b + 1] * np.exp(h) / net_h  # unit: image height
+            # last elements are class probabilities
+            classes = netout[int(row)][col][b][5:]
+            box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness, classes)
+            boxes.append(box)
+    return boxes
+
+
+def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):
+    new_w, new_h = net_w, net_h
+    for i in range(len(boxes)):
+        x_offset, x_scale = (net_w - new_w) / 2. / net_w, float(new_w) / net_w
+        y_offset, y_scale = (net_h - new_h) / 2. / net_h, float(new_h) / net_h
+        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)
+        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)
+        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)
+        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)
+
+
+def _interval_overlap(interval_a, interval_b):
+    x1, x2 = interval_a
+    x3, x4 = interval_b
+    if x3 < x1:
+        if x4 < x1:
+            return 0
+        else:
+            return min(x2, x4) - x1
+    else:
+        if x2 < x3:
+            return 0
+        else:
+            return min(x2, x4) - x3
+
+
+def bbox_iou(box1, box2):
+    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])
+    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])
+    intersect = intersect_w * intersect_h
+    w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin
+    w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin
+    union = w1 * h1 + w2 * h2 - intersect
+    return float(intersect) / union
+
+
+def do_nms(boxes, nms_thresh):
+    if len(boxes) > 0:
+        nb_class = len(boxes[0].classes)
+    else:
+        return
+    for c in range(nb_class):
+        sorted_indices = np.argsort([-box.classes[c] for box in boxes])
+        for i in range(len(sorted_indices)):
+            index_i = sorted_indices[i]
+            if boxes[index_i].classes[c] == 0: continue
+            for j in range(i + 1, len(sorted_indices)):
+                index_j = sorted_indices[j]
+                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:
+                    boxes[index_j].classes[c] = 0
+
+
+# load and prepare an image
+def load_image_pixels(filename, shape):
+    # load the image to get its shape
+    image = load_img(filename)
+    width, height = image.size
+    # load the image with the required size
+    image = load_img(filename, target_size=shape)
+    # convert to numpy array
+    image = img_to_array(image)
+    # scale pixel values to [0, 1]
+    image = image.astype('float32')
+    image /= 255.0
+    # add a dimension so that we have one sample
+    image = expand_dims(image, 0)
+    return image, width, height
+
+
+# get all of the results above a threshold
+def get_boxes(boxes, labels, thresh):
+    v_boxes, v_labels, v_scores = list(), list(), list()
+    # enumerate all boxes
+    for box in boxes:
+        # enumerate all possible labels
+        for i in range(len(labels)):
+            # check if the threshold for this label is high enough
+            if box.classes[i] > thresh:
+                v_boxes.append(box)
+                v_labels.append(labels[i])
+                v_scores.append(box.classes[i] * 100)
+            # don't break, many labels may trigger for one box
+    return v_boxes, v_labels, v_scores
+
+
+# draw all results
+def draw_boxes(filename, v_boxes, v_labels, v_scores):
+    # load the image
+    data = pyplot.imread(filename)
+    # plot the image
+    pyplot.imshow(data)
+    # get the context for drawing boxes
+    ax = pyplot.gca()
+    # plot each box
+    for i in range(len(v_boxes)):
+        box = v_boxes[i]
+        # get coordinates
+        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax
+        # calculate width and height of the box
+        width, height = x2 - x1, y2 - y1
+        # create the shape
+        rect = Rectangle((x1, y1), width, height, fill=False, color='white', lw=2)
+        # draw the box
+        ax.add_patch(rect)
+        # draw text and score in top left corner
+        label = "%s (%.3f)" % (v_labels[i], v_scores[i])
+        pyplot.text(x1, y1, label, color='white')
+    # show the plot
+    #pyplot.show()
+
+
+# load yolov3 model
+model = load_model('model.h5', compile=False)
+# define the expected input shape for the model
+input_w, input_h = 416, 416
+
+# In[77]:
+
+
+# define the labels
+labels = ["person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck",
+          "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench",
+          "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe",
+          "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard",
+          "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",
+          "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana",
+          "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake",
+          "chair", "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse",
+          "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator",
+          "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"]
+
+# define the probability threshold for detected objects
+class_threshold = 0.6
+dist_threshold = 150  # gör denna beroende på framerate som valts
+
+
+# In[78]:
+
+
+class Mem_Object:
+    def __init__(self, label, xmin, xmax, ymin, ymax):
+        self.label = label
+        self.xmin = xmin
+        self.xmax = xmax
+        self.ymin = ymin
+        self.ymax = ymax
+        self.mid = [(xmin + xmax) / 2, (ymin + ymax) / 2]
+
+
+# In[79]:
+
+
+def dist(p1, p2):
+    distance = math.sqrt(((p1[0] - p2[0]) ** 2) + ((p1[1] - p2[1]) ** 2))
+    return distance
+
+
+# In[80]:
+
+
+def check_new_object(v_labels, v_boxes, v_scores, frames_memory):
+    print("Inside check_new_object")
+
+    # create temp counter for this frames
+    temp_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+    new_objects = pd.DataFrame(data=0, columns=range(10), index=['T1'])
+
+    # add all new objects from v_labels with attributes
+    for i in range(len(v_labels)):
+        new_objects[i]['T1'] = Mem_Object(v_labels[i], v_boxes[i].xmin, v_boxes[i].xmax, v_boxes[i].ymin,
+                                          v_boxes[i].ymax)
+        print("Label i objects" + v_labels[i])
+
+    print("Length of new_objects: " + str(len(new_objects)))
+    print("Content of new_objects" + str(new_objects) + "... End of contents")
+    # print("Content of new_objects" + new_objects)
+
+    # loop through new objects and compare to last frame
+    for i in new_objects:
+        # loop until zero -> no more objects
+        if new_objects[i][0] == 0: break  # eller continue
+
+        temp_counter.loc[str(v_labels[i])] += 1
+        # print("New object up counted!")
+        # print("Temp counter inside function: ")
+        # print(temp_counter)
+
+        # placehold_count += 1 #hundar eller persons
+        uniqueObj = True
+
+        # current frame obj
+        cf_obj = new_objects[i][0]
+        print("Mid new object" + str(cf_obj.mid))
+
+        # Check memory for objects in last frame, if same type of object but too close, count down!
+        for j in frames_memory:
+            lf_obj = frames_memory[j][0]  # index 0 is 'F1'
+
+            if (lf_obj != 0):
+
+                if (lf_obj.label == cf_obj.label):
+                    print("Mid old object" + str(lf_obj.mid))
+                    distance = dist(lf_obj.mid, cf_obj.mid)
+                    print("Distance between objects is: " + str(distance))
+
+                    # if same type of object is too close, conclude same object, count down.
+                    if (distance < dist_threshold):
+                        temp_counter.loc[str(v_labels[i])] -= 1
+                        print("Same object!")
+                        uniqueObj = False
+
+        if uniqueObj:
+            print("New object!")  # syns ej på första
+
+    # Update frames_memory
+    for i in range(4):
+        frames_memory.loc['F' + str(5 - i)] = frames_memory.loc['F' + str(4 - i)]
+
+    # Overwrite first row with new objetcts
+    frames_memory.loc['F1'] = new_objects.loc['T1']
+
+    # frames_memory[rand.randint(5,9)]['F1'] = Mem_Object("person", 1, 1, 1, 1)
+    # print("Temp counter")
+    # print(temp_counter)
+    # print("Frames memory")
+    # print(frames_memory)
+    return temp_counter
+
+frames_memory = pd.DataFrame(data=0, columns=range(10), index=['F1', 'F2', 'F3', 'F4', 'F5'])
+# Ladda in ett objekt
+frames_memory
+
+big_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+
+#Dir to be iterated
+directory_2 = 'C:/Users/eitn35/Documents/EITN35/video_files/frames'
+#directory_2 = "/Users/august/Documents/EITN35_AIQ/video_files/frames/"
+directory_3 = "C:/Users/eitn35/Documents/EITN35/video_files/test_set/"
+os.chdir(directory_3)
+
+# Create "unlabeled_images" folder if it does not exist
+try:
+    if not os.path.exists('unlabeled_images'):
+        os.makedirs('unlabeled_images')
+    if not os.path.exists('autolabeled_images'):
+        os.makedirs('autolabeled_images')
+except OSError:
+    print('Error: Creating directory of data')
+
+#MÅSTE GÖRAS LÄNGRE OM VI HITTAR MER ÄN ETT OBJEKT I VARJE BILD, GÖR HELLRE FÖR LÅNG OCH DROPPA
+annotation_df = pd.DataFrame(data=0,index=np.arange(len(os.listdir(directory_3))),columns="image xmin ymin xmax ymax label".split())
+index = 0
+
+# Store found objects and their positions from previous frame in vector.
+# When new object found compare type and position, then decide to count or not
+# lf_memory = pd.DataFrame(0,columns=['Label','Middle'],index=range(10))
+
+
+for photo_filename in os.listdir(directory_3):
+    if not photo_filename.endswith('jpg'):continue
+    #photo_filename = 'frame_' + str(i + 8) + '.jpg'
+    # define our new photoc
+    # photo_filename = 'man_on_scooter.jpg'
+    # load and prepare image
+    image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))
+    # make prediction
+    yhat = model.predict(image)
+
+    # summarize the shape of the list of arrays
+    print([a.shape for a in yhat])
+
+    # define the anchors
+    anchors = [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]
+
+    boxes = list()
+    for i in range(len(yhat)):
+        # decode the output of the network
+        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)
+    # correct the sizes of the bounding boxes for the shape of the image
+    correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)
+    # suppress non-maximal boxes
+    do_nms(boxes, 0.5)
+
+    # get the details of the detected objects
+    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)
+
+    contains_person = False
+    #load dataframe for csv export
+    for i in range(len(v_labels)):
+        if v_labels[i] == 'person':
+            annotation_df['image'][index] = photo_filename
+            annotation_df['xmin'][index] = v_boxes[i].xmin
+            annotation_df['ymin'][index] = v_boxes[i].ymin
+            annotation_df['xmax'][index] = v_boxes[i].xmax
+            annotation_df['ymax'][index] = v_boxes[i].ymax
+            annotation_df['label'][index] = v_labels[i]
+            index += 1
+            contains_person = True #Om vi har hittat en instance av person flytta inte file till unlabled_files
+
+    # summarize what we found
+    temp_2_counter = check_new_object(v_labels, v_boxes, v_scores, frames_memory)
+
+    # print(temp_2_counter)
+    big_counter.loc["person"] = big_counter.loc["person"] + temp_2_counter.loc["person"]
+    big_counter.loc["dog"] = big_counter.loc["dog"] + temp_2_counter.loc["dog"]
+
+    # draw what we found
+    draw_boxes(photo_filename, v_boxes, v_labels, v_scores)
+    print(str(big_counter))
+
+    #move files that couldn't be labeled with YOLO
+    if (not contains_person):
+        os.rename(
+            directory_3 + photo_filename,
+            directory_3 + 'unlabeled_images/' + photo_filename
+        )
+
+
+
+    #print("THIS IS ANNOTATIONS DF")
+    print(str(annotation_df))
+
+
+
+
+#Make CSV-file from auto-annotations
+annotation_df.to_csv('/Users/august/Documents/EITN35_AIQ/Annotations-export_YOLO_auto.csv', index=False, header=True)
+
+for labeled in os.listdir(directory_3):
+    if not labeled.endswith('jpg'): continue
+    os.rename(
+        directory_3 + labeled,
+        directory_3 + 'autolabeled_images/' + labeled
+    )
+
+# In[90]:
+
+
+big_counter.loc['dog']
+
+# In[ ]:
+
+
+
+
Index: Old/object_detection_labeling.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Old/object_detection_labeling.py	(date 1591012031986)
+++ Old/object_detection_labeling.py	(date 1591012031986)
@@ -0,0 +1,442 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+# Object Detection With YOLOv3 in Keras
+
+# load yolov3 model and perform object detection
+# based on https://github.com/experiencor/keras-yolo3
+import numpy as np
+from numpy import expand_dims
+from keras.models import load_model
+from keras.preprocessing.image import load_img
+from keras.preprocessing.image import img_to_array
+from matplotlib import pyplot
+from matplotlib.patches import Rectangle
+import pandas as pd
+import os
+import math
+#import tensorflow.compat.v1 as tf
+#tf.disable_v2_behavior()
+
+#directory_1 = "/Users/august/Documents/EITN35_AIQ"
+directory_1 = "../EITN35_Resources/"
+os.chdir(directory_1)
+
+
+class BoundBox:
+    def __init__(self, xmin, ymin, xmax, ymax, objness=None, classes=None):
+        self.xmin = xmin
+        self.ymin = ymin
+        self.xmax = xmax
+        self.ymax = ymax
+        self.objness = objness
+        self.classes = classes
+        self.label = -1
+        self.score = -1
+
+    def get_label(self):
+        if self.label == -1:
+            self.label = np.argmax(self.classes)
+
+        return self.label
+
+    def get_score(self):
+        if self.score == -1:
+            self.score = self.classes[self.get_label()]
+
+        return self.score
+
+
+def _sigmoid(x):
+    return 1. / (1. + np.exp(-x))
+
+
+def decode_netout(netout, anchors, obj_thresh, net_h, net_w):
+    grid_h, grid_w = netout.shape[:2]
+    nb_box = 3
+    netout = netout.reshape((grid_h, grid_w, nb_box, -1))
+    nb_class = netout.shape[-1] - 5
+    boxes = []
+    netout[..., :2] = _sigmoid(netout[..., :2])
+    netout[..., 4:] = _sigmoid(netout[..., 4:])
+    netout[..., 5:] = netout[..., 4][..., np.newaxis] * netout[..., 5:]
+    netout[..., 5:] *= netout[..., 5:] > obj_thresh
+
+    for i in range(grid_h * grid_w):
+        row = i / grid_w
+        col = i % grid_w
+        for b in range(nb_box):
+            # 4th element is objectness score
+            objectness = netout[int(row)][int(col)][b][4]
+            if (objectness.all() <= obj_thresh): continue
+            # first 4 elements are x, y, w, and h
+            x, y, w, h = netout[int(row)][int(col)][b][:4]
+            x = (col + x) / grid_w  # center position, unit: image width
+            y = (row + y) / grid_h  # center position, unit: image height
+            w = anchors[2 * b + 0] * np.exp(w) / net_w  # unit: image width
+            h = anchors[2 * b + 1] * np.exp(h) / net_h  # unit: image height
+            # last elements are class probabilities
+            classes = netout[int(row)][col][b][5:]
+            box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness, classes)
+            boxes.append(box)
+    return boxes
+
+
+def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):
+    new_w, new_h = net_w, net_h
+    for i in range(len(boxes)):
+        x_offset, x_scale = (net_w - new_w) / 2. / net_w, float(new_w) / net_w
+        y_offset, y_scale = (net_h - new_h) / 2. / net_h, float(new_h) / net_h
+        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)
+        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)
+        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)
+        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)
+
+
+def _interval_overlap(interval_a, interval_b):
+    x1, x2 = interval_a
+    x3, x4 = interval_b
+    if x3 < x1:
+        if x4 < x1:
+            return 0
+        else:
+            return min(x2, x4) - x1
+    else:
+        if x2 < x3:
+            return 0
+        else:
+            return min(x2, x4) - x3
+
+
+def bbox_iou(box1, box2):
+    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])
+    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])
+    intersect = intersect_w * intersect_h
+    w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin
+    w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin
+    union = w1 * h1 + w2 * h2 - intersect
+    return float(intersect) / union
+
+
+def do_nms(boxes, nms_thresh):
+    if len(boxes) > 0:
+        nb_class = len(boxes[0].classes)
+    else:
+        return
+    for c in range(nb_class):
+        sorted_indices = np.argsort([-box.classes[c] for box in boxes])
+        for i in range(len(sorted_indices)):
+            index_i = sorted_indices[i]
+            if boxes[index_i].classes[c] == 0: continue
+            for j in range(i + 1, len(sorted_indices)):
+                index_j = sorted_indices[j]
+                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:
+                    boxes[index_j].classes[c] = 0
+
+
+# load and prepare an image
+def load_image_pixels(filename, shape):
+    # load the image to get its shape
+    image = load_img(filename)
+    width, height = image.size
+    # load the image with the required size
+    image = load_img(filename, target_size=shape)
+    # convert to numpy array
+    image = img_to_array(image)
+    # scale pixel values to [0, 1]
+    image = image.astype('float32')
+    image /= 255.0
+    # add a dimension so that we have one sample
+    image = expand_dims(image, 0)
+    return image, width, height
+
+
+# get all of the results above a threshold
+def get_boxes(boxes, labels, thresh):
+    v_boxes, v_labels, v_scores = list(), list(), list()
+    # enumerate all boxes
+    for box in boxes:
+        # enumerate all possible labels
+        for i in range(len(labels)):
+            # check if the threshold for this label is high enough
+            if box.classes[i] > thresh:
+                v_boxes.append(box)
+                v_labels.append(labels[i])
+                v_scores.append(box.classes[i] * 100)
+            # don't break, many labels may trigger for one box
+    return v_boxes, v_labels, v_scores
+
+
+# draw all results
+def draw_boxes(filename, v_boxes, v_labels, v_scores):
+    # load the image
+    data = pyplot.imread(filename)
+    # plot the image
+    pyplot.imshow(data)
+    # get the context for drawing boxes
+    ax = pyplot.gca()
+    # plot each box
+    for i in range(len(v_boxes)):
+        box = v_boxes[i]
+        # get coordinates
+        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax
+        # calculate width and height of the box
+        width, height = x2 - x1, y2 - y1
+        # create the shape
+        rect = Rectangle((x1, y1), width, height, fill=False, color='white', lw=2)
+        # draw the box
+        ax.add_patch(rect)
+        # draw text and score in top left corner
+        label = "%s (%.3f)" % (v_labels[i], v_scores[i])
+        pyplot.text(x1, y1, label, color='white')
+    # show the plot
+    #pyplot.show()
+
+
+# load yolov3 model
+model = load_model('model.h5', compile=False)
+# define the expected input shape for the model
+input_w, input_h = 416, 416
+
+# In[77]:
+
+
+# define the labels
+labels = ["person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck",
+          "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench",
+          "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe",
+          "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard",
+          "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",
+          "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana",
+          "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake",
+          "chair", "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse",
+          "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator",
+          "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"]
+
+# define the probability threshold for detected objects
+class_threshold = 0.6
+dist_threshold = 150  # gör denna beroende på framerate som valts
+
+
+# In[78]:
+
+
+class Mem_Object:
+    def __init__(self, label, xmin, xmax, ymin, ymax):
+        self.label = label
+        self.xmin = xmin
+        self.xmax = xmax
+        self.ymin = ymin
+        self.ymax = ymax
+        self.mid = [(xmin + xmax) / 2, (ymin + ymax) / 2]
+
+
+# In[79]:
+
+
+def dist(p1, p2):
+    distance = math.sqrt(((p1[0] - p2[0]) ** 2) + ((p1[1] - p2[1]) ** 2))
+    return distance
+
+
+# In[80]:
+
+
+def check_new_object(v_labels, v_boxes, v_scores, frames_memory):
+    print("Inside check_new_object")
+
+    # create temp counter for this frames
+    temp_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+    new_objects = pd.DataFrame(data=0, columns=range(10), index=['T1'])
+
+    # add all new objects from v_labels with attributes
+    for i in range(len(v_labels)):
+        new_objects[i]['T1'] = Mem_Object(v_labels[i], v_boxes[i].xmin, v_boxes[i].xmax, v_boxes[i].ymin,
+                                          v_boxes[i].ymax)
+        print("Label i objects" + v_labels[i])
+
+    print("Length of new_objects: " + str(len(new_objects)))
+    print("Content of new_objects" + str(new_objects) + "... End of contents")
+    # print("Content of new_objects" + new_objects)
+
+    # loop through new objects and compare to last frame
+    for i in new_objects:
+        # loop until zero -> no more objects
+        if new_objects[i][0] == 0: break  # eller continue
+
+        temp_counter.loc[str(v_labels[i])] += 1
+        # print("New object up counted!")
+        # print("Temp counter inside function: ")
+        # print(temp_counter)
+
+        # placehold_count += 1 #hundar eller persons
+        uniqueObj = True
+
+        # current frame obj
+        cf_obj = new_objects[i][0]
+        print("Mid new object" + str(cf_obj.mid))
+
+        # Check memory for objects in last frame, if same type of object but too close, count down!
+        for j in frames_memory:
+            lf_obj = frames_memory[j][0]  # index 0 is 'F1'
+
+            if (lf_obj != 0):
+
+                if (lf_obj.label == cf_obj.label):
+                    print("Mid old object" + str(lf_obj.mid))
+                    distance = dist(lf_obj.mid, cf_obj.mid)
+                    print("Distance between objects is: " + str(distance))
+
+                    # if same type of object is too close, conclude same object, count down.
+                    if (distance < dist_threshold):
+                        temp_counter.loc[str(v_labels[i])] -= 1
+                        print("Same object!")
+                        uniqueObj = False
+
+        if uniqueObj:
+            print("New object!")  # syns ej på första
+
+    # Update frames_memory
+    for i in range(4):
+        frames_memory.loc['F' + str(5 - i)] = frames_memory.loc['F' + str(4 - i)]
+
+    # Overwrite first row with new objetcts
+    frames_memory.loc['F1'] = new_objects.loc['T1']
+
+    # frames_memory[rand.randint(5,9)]['F1'] = Mem_Object("person", 1, 1, 1, 1)
+    # print("Temp counter")
+    # print(temp_counter)
+    # print("Frames memory")
+    # print(frames_memory)
+    return temp_counter
+
+frames_memory = pd.DataFrame(data=0, columns=range(10), index=['F1', 'F2', 'F3', 'F4', 'F5'])
+# Ladda in ett objekt
+frames_memory
+
+big_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+
+#Dir to be iterated
+directory_2 = 'C:/Users/eitn35/Documents/EITN35/video_files/frames_night/'
+#directory_2 = "/Users/august/Documents/EITN35_AIQ/video_files/frames/"
+directory_3 = "C:/Users/eitn35/Documents/EITN35/video_files/test_set/"
+os.chdir(directory_2)
+
+# Create "unlabeled_images" folder if it does not exist
+try:
+    if not os.path.exists('unlabeled_images_night'):
+        os.makedirs('unlabeled_images_night')
+    if not os.path.exists('autolabeled_images_night'):
+        os.makedirs('autolabeled_images_night')
+except OSError:
+    print('Error: Creating directory of data')
+
+#MÅSTE GÖRAS LÄNGRE OM VI HITTAR MER ÄN ETT OBJEKT I VARJE BILD, GÖR HELLRE FÖR LÅNG OCH DROPPA
+annotation_df = pd.DataFrame(data=0,index=np.arange(len(os.listdir(directory_2))),columns="image xmin ymin xmax ymax label".split())
+index = 0
+
+# Store found objects and their positions from previous frame in vector.
+# When new object found compare type and position, then decide to count or not
+# lf_memory = pd.DataFrame(0,columns=['Label','Middle'],index=range(10))
+
+
+for photo_filename in os.listdir(directory_2):
+    if not photo_filename.endswith('jpg'):continue
+    #photo_filename = 'frame_' + str(i + 8) + '.jpg'
+    # define our new photoc
+    # photo_filename = 'man_on_scooter.jpg'
+    # load and prepare image
+    image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))
+    # make prediction
+    yhat = model.predict(image)
+
+    # summarize the shape of the list of arrays
+    #print([a.shape for a in yhat])
+
+    # define the anchors
+    anchors = [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]
+
+    boxes = list()
+    for i in range(len(yhat)):
+        # decode the output of the network
+        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)
+    # correct the sizes of the bounding boxes for the shape of the image
+    correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)
+    # suppress non-maximal boxes
+    do_nms(boxes, 0.5)
+
+    # get the details of the detected objects
+    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)
+
+    contains_person = False
+    #load dataframe for csv export
+
+    persons = 0
+    bikes = 0
+    dogs = 0
+
+    for i in range(len(v_labels)):
+       if v_labels[i] == 'person':
+           persons += 1
+
+       if v_labels[i] == 'bicycle':
+           bikes += 1
+
+       if v_labels[i] == 'dog':
+           dogs += 1
+
+
+
+    # summarize what we found
+    # temp_2_counter = check_new_object(v_labels, v_boxes, v_scores, frames_memory)
+
+    # print(temp_2_counter)
+    # big_counter.loc["person"] = big_counter.loc["person"] + temp_2_counter.loc["person"]
+    # big_counter.loc["dog"] = big_counter.loc["dog"] + temp_2_counter.loc["dog"]
+
+    # draw what we found
+    # draw_boxes(photo_filename, v_boxes, v_labels, v_scores)
+    # print(str(big_counter))
+
+    #move files that couldn't be labeled with YOLO
+    counter = persons + bikes + dogs
+    if (counter == 0):
+
+        os.rename(
+            directory_2 + photo_filename,
+            directory_2 + 'unlabeled_images_night/' + 'persons_0_dogs_0_bikes_0_' + photo_filename
+        )
+    else:
+        os.rename(
+            directory_2 + photo_filename,
+            directory_2 + 'autolabeled_images_night/' + 'persons_' + str(persons) + '_dogs_' + str(dogs) + '_bikes_' + str(bikes) + '_' + photo_filename
+        )
+
+
+
+    #print("THIS IS ANNOTATIONS DF")
+    #print(str(annotation_df))
+    print('labeling file '+ photo_filename)
+
+
+
+#Make CSV-file from auto-annotations
+#annotation_df.to_csv('/Users/august/Documents/EITN35_AIQ/Annotations-export_YOLO_auto.csv', index=False, header=True)
+
+#for labeled in os.listdir(directory_3):
+ #   if not labeled.endswith('jpg'): continue
+  #  os.rename(
+   #     directory_3 + labeled,
+    #    directory_3 + 'autolabeled_images/' + labeled
+    #)
+
+# In[90]:
+
+
+#big_counter.loc['dog']
+
+# In[ ]:
+
+
+
+
Index: Old/CNN_from_scratch_cats_dogs.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Old/CNN_from_scratch_cats_dogs.py	(date 1591012031982)
+++ Old/CNN_from_scratch_cats_dogs.py	(date 1591012031982)
@@ -0,0 +1,360 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+# In[1]:
+
+
+import cv2
+import numpy as np
+import pandas as pd
+import sklearn
+import matplotlib.pyplot as plt
+
+import os
+import random
+import gc #garbage collector for cleaning deleted data from memory
+
+
+# In[2]:
+
+
+#train_dir = 'C:/Users/eitn35/Documents/EITN35/video_files/frames/train_images/'
+#test_dir = 'C:/Users/eitn35/Documents/EITN35/video_files/frames/test_images/'
+train_dir = '/Users/august/PycharmProjects/EITN35_Resources/temp_train/'
+test_dir = '/Users/august/PycharmProjects/EITN35_Resources/temp_test/'
+
+#rain_person = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'persons_1' in i]  #get person images
+#rain_dogs = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'dogs_1' in i]  #get dog images
+#rain_bikes = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'bikes_1' in i]  #get bike images
+#rain_empty = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if ('persons_0'and 'dogs_0' and 'bikes_0')in i]  #get bike images
+
+
+test_imgs = [test_dir+'{}'.format(i) for i in os.listdir(test_dir)] #get test images
+
+
+#train_imgs = train_person + train_dogs + train_bikes + train_empty   # slice the dataset and use 3 persons
+train_imgs = [train_dir+'{}'.format(i) for i in os.listdir(train_dir)] #get test images
+random.shuffle(train_imgs)  # shuffle it randomly
+random.shuffle(test_imgs)
+
+
+gc.collect()   #collect garbage to save memory
+
+
+# In[3]:
+
+
+import matplotlib.image as mpimg
+for ima in train_imgs[0:4]:
+     img=mpimg.imread(ima)
+     imgplot = plt.imshow(img)
+     plt.show()
+
+
+# In[4]:
+
+
+#Lets declare our image dimensions
+#we are using coloured images. 
+nrows = 416
+ncolumns = 416
+channels = 3  #change to 1 if you want to use grayscale image
+
+#A function to read and process the images to an acceptable format for our model
+def read_and_process_image(list_of_images):
+    """
+    Returns two arrays: 
+        X is an array of resized images
+        y is an array of labels
+    """
+    X = [] # images
+    y = []# labels
+    i = 0
+    for image in list_of_images:
+        #ändra här mellan COLOR och GRAYSCALE beroende på antal channels
+        X.append(cv2.resize(cv2.imread(image,cv2.IMREAD_COLOR), (nrows,ncolumns), interpolation=cv2.INTER_CUBIC))  #Read the image
+        #get the labels
+        if 'persons_1' in image:
+            y.append(1)
+        elif 'dogs_1' in image:
+            y.append(2)
+        elif 'bikes_1' in image:
+            y.append(3)
+        else:
+            y.append(0)
+        i += 1
+    return X, y
+
+
+# In[5]:
+
+
+class_names = ['empty', 'person', 'dogs', 'bikes']
+
+
+# In[6]:
+
+
+X, y = read_and_process_image(train_imgs)
+X_test, y_test = read_and_process_image(test_imgs)
+
+
+# In[7]:
+
+
+y[0]
+
+
+# In[8]:
+
+
+#Lets view some of the pics
+plt.figure(figsize=(20,10))
+columns = 4
+for i in range(columns):
+    plt.subplot(5 / columns + 1, columns, i + 1)
+    plt.imshow(X[i])
+
+
+# In[9]:
+
+
+import seaborn as sns
+
+gc.collect()
+
+#Convert list to numpy array
+X = np.array(X)
+y = np.array(y)
+X_test = np.array(X_test)
+y_test = np.array(y_test)
+#Lets plot the label to be sure we just have two class
+#sns.countplot(y)
+#plt.title('Labels for Cats and Dogs')
+
+
+# In[10]:
+
+
+print("Shape of train images is:", X.shape)
+print("Shape of labels is:", y.shape)
+
+
+# In[11]:
+
+
+#Lets split the data into train and test set
+from sklearn.model_selection import train_test_split
+X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=2)
+
+print("Shape of train images is:", X_train.shape)
+print("Shape of validation images is:", X_val.shape)
+print("Shape of labels is:", y_train.shape)
+print("Shape of labels is:", y_val.shape)
+
+
+# In[12]:
+
+
+#clear memory
+#del X
+#del y
+gc.collect()
+
+#get the length of the train and validation data
+ntrain = len(X_train)
+nval = len(X_val)
+
+#We will use a batch size of 32. Note: batch size should be a factor of 2.***4,8,16,32,64...***
+batch_size = 32
+
+
+# In[13]:
+
+
+
+#from keras import models
+from keras import optimizers
+from tensorflow.keras import models, layers
+from keras.preprocessing.image import ImageDataGenerator
+from keras.preprocessing.image import img_to_array, load_img
+import tensorflow as tf
+
+model = models.Sequential()
+model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(ncolumns, nrows, 3))) #input ska var (150, 150, 3)
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(64, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Flatten())
+model.add(layers.Dropout(0.5))  #Dropout for regularization
+model.add(layers.Dense(512, activation='relu'))
+model.add(layers.Dense(4))  #Sigmoid function at the end because we have just two classes
+
+
+# In[14]:
+
+
+#Lets see our model
+model.summary()
+
+
+# In[15]:
+
+
+#We'll use the RMSprop optimizer with a learning rate of 0.0001
+#We'll use binary_crossentropy loss because its a binary classification
+model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['acc'])
+
+
+# In[16]:
+
+
+#Lets create the augmentation configuration
+#This helps prevent overfitting, since we are using a small dataset
+train_datagen = ImageDataGenerator(rescale=1./255,   #Scale the image between 0 and 1
+                                    rotation_range=40,
+                                    width_shift_range=0.2,
+                                    height_shift_range=0.2,
+                                    shear_range=0.2,
+                                    zoom_range=0.2,
+                                    horizontal_flip=True,)
+
+val_datagen = ImageDataGenerator(rescale=1./255)  #We do not augment validation data. we only perform rescale
+
+
+# In[17]:
+
+
+
+#Create the image generators
+train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)
+val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)
+
+
+# In[18]:
+
+#The training part
+#We train for 64 epochs with about 100 steps per epoch
+history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))
+
+
+# In[19]:
+
+
+#Save the model
+model.save_weights('model_cat&dog1_weights.h5')
+model.save('model_cat&dog1_keras.h5')
+
+
+# In[20]:
+
+#lets plot the train and val curve
+#get the details form the history object
+acc = history.history['acc']
+val_acc = history.history['val_acc']
+loss = history.history['loss']
+val_loss = history.history['val_loss']
+
+epochs = range(1, len(acc) + 1)
+
+#Train and validation accuracy
+plt.plot(epochs, acc, 'b', label='Training accurarcy')
+plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
+plt.title('Training and Validation accurarcy')
+plt.legend()
+
+plt.figure()
+#Train and validation loss
+plt.plot(epochs, loss, 'b', label='Training loss')
+plt.plot(epochs, val_loss, 'r', label='Validation loss')
+plt.title('Training and Validation loss')
+plt.legend()
+
+plt.show()
+
+
+# In[22]:
+
+
+probability_model = tf.keras.Sequential([model,
+                                         tf.keras.layers.Softmax()])
+
+predictions = probability_model.predict(X_test)
+
+predictions[0]
+
+np.argmax(predictions[0])
+
+y_test[0]
+
+def plot_image(i, predictions_array, true_label, img):
+    predictions_array, true_label, img = predictions_array, true_label[i], img[i]
+    plt.grid(False)
+    plt.xticks([])
+    plt.yticks([])
+    plt.imshow(img, cmap=plt.cm.binary)
+
+    predicted_label = np.argmax(predictions_array)
+    if predicted_label == true_label:
+        color = 'blue'
+    else:
+        color = 'red'
+
+    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
+                                100*np.max(predictions_array),
+                                class_names[true_label]),
+                                color=color)
+
+def plot_value_array(i, predictions_array, true_label):
+    predictions_array, true_label = predictions_array, true_label[i]
+    plt.grid(False)
+    plt.xticks(range(4))
+    plt.yticks([])
+    thisplot = plt.bar(range(4), predictions_array, color="#777777")
+    plt.ylim([0, 1])
+    predicted_label = np.argmax(predictions_array)
+
+    thisplot[predicted_label].set_color('red')
+    thisplot[true_label].set_color('blue')
+
+# Plot the first X test images, their predicted labels, and the true labels.
+# Color correct predictions in blue and incorrect predictions in red.
+num_rows = 5
+num_cols = 3
+num_images = num_rows*num_cols
+plt.figure(figsize=(2*2*num_cols, 2*num_rows))
+for i in range(num_images):
+  plt.subplot(num_rows, 2*num_cols, 2*i+1)
+  plot_image(i, predictions[i], y_test, X_test)
+  plt.subplot(num_rows, 2*num_cols, 2*i+2)
+  plot_value_array(i, predictions[i], y_test)
+plt.tight_layout()
+plt.show()
+
+
+import result_run_printer as printer
+layers = 12
+model_no = 3
+pr = printer
+#printer.print_to_file(0.88, 0.65, 0.01, 0.02, 400, 12, 1)
+
+#caculate the values from averages of the no_av last values
+v_size = len(acc)
+no_av = 5
+acc_av = 0
+val_acc_av = 0
+loss_av = 0
+val_loss_av = 0
+
+for i in range(no_av):
+    acc_av += acc[v_size-i-1]/no_av
+    val_acc_av += val_acc[v_size-i-1]/no_av
+    loss_av += loss[v_size-i-1]/no_av
+    val_loss_av += val_loss[v_size-i-1]/no_av
+
+#print the values of the run to a file
+printer.print_to_file(acc_av, val_acc_av, loss_av, val_loss_av, len(acc), layers, model_no)
\ No newline at end of file
Index: CycleGan/CycleGan.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- CycleGan/CycleGan.py	(date 1591012032025)
+++ CycleGan/CycleGan.py	(date 1591012032025)
@@ -0,0 +1,300 @@
+# example of training a cyclegan on the horse2zebra dataset
+from random import random
+from numpy import load
+from numpy import zeros
+from numpy import ones
+from numpy import asarray
+from numpy.random import randint
+from keras.optimizers import Adam
+from keras.initializers import RandomNormal
+from keras.models import Model
+from keras.models import Input
+from keras.layers import Conv2D
+from keras.layers import Conv2DTranspose
+from keras.layers import LeakyReLU
+from keras.layers import Activation
+from keras.layers import Concatenate
+from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization
+from matplotlib import pyplot
+from numpy import vstack
+
+def show_plot_train(X_realA, X_realB, X_fakeA,X_fakeB):
+	images = vstack((X_realA, X_realB, X_fakeA,X_fakeB))
+	titles = ['X_realA', 'X_realB', 'X_fakeA','X_fakeB']
+	# scale from [-1,1] to [0,1]
+	images = (images + 1) / 2.0
+	# plot images row by row
+	for i in range(len(images)):
+		# define subplot
+		pyplot.subplot(1, len(images), 1 + i)
+		# turn off axis
+		pyplot.axis('off')
+		# plot raw pixel data
+		pyplot.imshow(images[i])
+		# title
+		pyplot.title(titles[i])
+	pyplot.show()
+
+# define the discriminator model
+def define_discriminator(image_shape):
+	# weight initialization
+	init = RandomNormal(stddev=0.02)
+	# source image input
+	in_image = Input(shape=image_shape)
+	# C64
+	d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)
+	d = LeakyReLU(alpha=0.2)(d)
+	# C128
+	d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)
+	d = InstanceNormalization(axis=-1)(d)
+	d = LeakyReLU(alpha=0.2)(d)
+	# C256
+	d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)
+	d = InstanceNormalization(axis=-1)(d)
+	d = LeakyReLU(alpha=0.2)(d)
+	# C512
+	d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)
+	d = InstanceNormalization(axis=-1)(d)
+	d = LeakyReLU(alpha=0.2)(d)
+	# second last output layer
+	d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)
+	d = InstanceNormalization(axis=-1)(d)
+	d = LeakyReLU(alpha=0.2)(d)
+	# patch output
+	patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)
+	# define model
+	model = Model(in_image, patch_out)
+	# compile model
+	model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])
+	return model
+
+# generator a resnet block
+def resnet_block(n_filters, input_layer):
+	# weight initialization
+	init = RandomNormal(stddev=0.02)
+	# first layer convolutional layer
+	g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# second convolutional layer
+	g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	# concatenate merge channel-wise with input layer
+	g = Concatenate()([g, input_layer])
+	return g
+
+# define the standalone generator model
+def define_generator(image_shape, n_resnet=9):
+	# weight initialization
+	init = RandomNormal(stddev=0.02)
+	# image input
+	in_image = Input(shape=image_shape)
+	# c7s1-64
+	g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# d128
+	g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# d256
+	g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# R256
+	for _ in range(n_resnet):
+		g = resnet_block(256, g)
+	# u128
+	g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# u64
+	g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# c7s1-3
+	g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	out_image = Activation('tanh')(g)
+	# define model
+	model = Model(in_image, out_image)
+	return model
+
+# define a composite model for updating generators by adversarial and cycle loss
+def define_composite_model(g_model_1, d_model, g_model_2, image_shape):
+	# ensure the model we're updating is trainable
+	g_model_1.trainable = True
+	# mark discriminator as not trainable
+	d_model.trainable = False
+	# mark other generator model as not trainable
+	g_model_2.trainable = False
+	# discriminator element
+	input_gen = Input(shape=image_shape)
+	gen1_out = g_model_1(input_gen)
+	output_d = d_model(gen1_out)
+	# identity element
+	input_id = Input(shape=image_shape)
+	output_id = g_model_1(input_id)
+	# forward cycle
+	output_f = g_model_2(gen1_out)
+	# backward cycle
+	gen2_out = g_model_2(input_id)
+	output_b = g_model_1(gen2_out)
+	# define model graph
+	model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])
+	# define optimization algorithm configuration
+	opt = Adam(lr=0.0002, beta_1=0.5)
+	# compile model with weighting of least squares loss and L1 loss
+	model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)
+	return model
+
+# load and prepare training images
+def load_real_samples(filename):
+	# load the dataset
+	data = load(filename)
+	# unpack arrays
+	X1, X2 = data['arr_0'], data['arr_1']
+	# scale from [0,255] to [-1,1]
+	X1 = (X1 - 127.5) / 127.5
+	X2 = (X2 - 127.5) / 127.5
+	return [X1, X2]
+
+# select a batch of random samples, returns images and target
+def generate_real_samples(dataset, n_samples, patch_shape):
+	# choose random instances
+	ix = randint(0, dataset.shape[0], n_samples)
+	# retrieve selected images
+	X = dataset[ix]
+	# generate 'real' class labels (1)
+	y = ones((n_samples, patch_shape, patch_shape, 1))
+	return X, y
+
+# generate a batch of images, returns images and targets
+def generate_fake_samples(g_model, dataset, patch_shape):
+	# generate fake instance
+	X = g_model.predict(dataset)
+	# create 'fake' class labels (0)
+	y = zeros((len(X), patch_shape, patch_shape, 1))
+	return X, y
+
+# save the generator models to file
+def save_models(step, g_model_AtoB, g_model_BtoA):
+	# save the first generator model
+	filename1 = 'g_model_AtoB_%06d_D2N208BigFrameSetV1.h5' % (step+1)
+	g_model_AtoB.save(filename1)
+	# save the second generator model
+	filename2 = 'g_model_BtoA_%06d_D2N208BigFrameSetV1.h5' % (step+1)
+	g_model_BtoA.save(filename2)
+	print('>Saved: %s and %s' % (filename1, filename2))
+
+# generate samples and save as a plot and save the model
+def summarize_performance(step, g_model, trainX, name, n_samples=5):
+	# select a sample of input images
+	X_in, _ = generate_real_samples(trainX, n_samples, 0)
+	# generate translated images
+	X_out, _ = generate_fake_samples(g_model, X_in, 0)
+	# scale all pixels from [-1,1] to [0,1]
+	X_in = (X_in + 1) / 2.0
+	X_out = (X_out + 1) / 2.0
+	# plot real images
+	for i in range(n_samples):
+		pyplot.subplot(2, n_samples, 1 + i)
+		pyplot.axis('off')
+		pyplot.imshow(X_in[i])
+	# plot translated image
+	for i in range(n_samples):
+		pyplot.subplot(2, n_samples, 1 + n_samples + i)
+		pyplot.axis('off')
+		pyplot.imshow(X_out[i])
+	# save plot to file
+	filename1 = '%s_generated_plot_%06d_D2N208BigFrameSetV1.png' % (name, (step+1))
+	pyplot.savefig(filename1)
+	pyplot.close()
+
+# update image pool for fake images
+def update_image_pool(pool, images, max_size=50):
+	selected = list()
+	for image in images:
+		if len(pool) < max_size:
+			# stock the pool
+			pool.append(image)
+			selected.append(image)
+		elif random() < 0.5:
+			# use image, but don't add it to the pool
+			selected.append(image)
+		else:
+			# replace an existing image and use replaced image
+			ix = randint(0, len(pool))
+			selected.append(pool[ix])
+			pool[ix] = image
+	return asarray(selected)
+
+# train cyclegan models
+def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):
+
+	# define properties of the training run
+	n_epochs, n_batch, = 1000, 1
+	# determine the output square shape of the discriminator
+	n_patch = d_model_A.output_shape[1]
+	# unpack dataset
+	trainA, trainB = dataset
+	# prepare image pool for fakes
+	poolA, poolB = list(), list()
+	# calculate the number of batches per training epoch
+	bat_per_epo = int(len(trainA) / n_batch)
+	# calculate the number of training iterations
+	n_steps = bat_per_epo * n_epochs
+	# manually enumerate epochs
+	for i in range(n_steps):
+		# select a batch of real samples
+		X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)
+		X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)
+		# generate a batch of fake samples
+		X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)
+		X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)
+		# update fakes from pool
+		X_fakeA = update_image_pool(poolA, X_fakeA)
+		X_fakeB = update_image_pool(poolB, X_fakeB)
+		# update generator B->A via adversarial and cycle loss
+		g_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])
+		# update discriminator for A -> [real/fake]
+		dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)
+		dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)
+		# update generator A->B via adversarial and cycle loss
+		g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])
+		# update discriminator for B -> [real/fake]
+		dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)
+		dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)
+		#show_plot_train(X_realA, X_realB, X_fakeA,X_fakeB)
+		# summarize performance
+		print('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))
+		# evaluate the model performance every so often
+		if (i+1) % (bat_per_epo * 5) == 0:
+			# plot A->B translation
+			summarize_performance(i, g_model_AtoB, trainA, 'AtoB')
+			# plot B->A translation
+			summarize_performance(i, g_model_BtoA, trainB, 'BtoA')
+			save_models(i, g_model_AtoB, g_model_BtoA)
+		#if (i+1) % (bat_per_epo * 5) == 0:
+			# save the models
+			#save_models(i, g_model_AtoB, g_model_BtoA)
+
+# load image data
+dataset = load_real_samples('../../Documents/EITN35/night2day208BigFrameSetV2.npz')
+print('Loaded', dataset[0].shape, dataset[1].shape)
+# define input shape based on the loaded dataset
+image_shape = dataset[0].shape[1:]
+# generator: A -> B
+g_model_AtoB = define_generator(image_shape)
+# generator: B -> A
+g_model_BtoA = define_generator(image_shape)
+# discriminator: A -> [real/fake]
+d_model_A = define_discriminator(image_shape)
+# discriminator: B -> [real/fake]
+d_model_B = define_discriminator(image_shape)
+# composite: A -> B -> [real/fake, A]
+c_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)
+# composite: B -> A -> [real/fake, B]
+c_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)
+# train models
+train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)
\ No newline at end of file
Index: CycleGan/Cycle-ImageGenerator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- CycleGan/Cycle-ImageGenerator.py	(date 1591012032022)
+++ CycleGan/Cycle-ImageGenerator.py	(date 1591012032022)
@@ -0,0 +1,63 @@
+
+# example of using saved cyclegan models for image translation
+from keras.models import load_model
+from numpy import load
+from numpy import vstack
+from matplotlib import pyplot
+from numpy.random import randint
+from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization
+
+# load and prepare training images
+def load_real_samples(filename):
+	# load the dataset
+	data = load(filename)
+	# unpack arrays
+	X1, X2 = data['arr_0'], data['arr_1']
+	# scale from [0,255] to [-1,1]
+	X1 = (X1 - 127.5) / 127.5
+	X2 = (X2 - 127.5) / 127.5
+	return [X1, X2]
+
+# select a random sample of images from the dataset
+def select_sample(dataset, n_samples):
+	# choose random instances
+	ix = randint(0, dataset.shape[0], n_samples)
+	# retrieve selected images
+	X = dataset[ix]
+	return X
+
+# plot the image, the translation, and the reconstruction
+def show_plot(imagesX, imagesY1, imagesY2):
+	images = vstack((imagesX, imagesY1, imagesY2))
+	titles = ['Real', 'Generated', 'Reconstructed']
+	# scale from [-1,1] to [0,1]
+	images = (images + 1) / 2.0
+	# plot images row by row
+	for i in range(len(images)):
+		# define subplot
+		pyplot.subplot(1, len(images), 1 + i)
+		# turn off axis
+		pyplot.axis('off')
+		# plot raw pixel data
+		pyplot.imshow(images[i])
+		# title
+		pyplot.title(titles[i])
+	pyplot.show()
+
+# load dataset
+A_data, B_data = load_real_samples('../../Documents/EITN35/day2night208all.npz')
+print('Loaded', A_data.shape, B_data.shape)
+# load the models
+cust = {'InstanceNormalization': InstanceNormalization}
+model_AtoB = load_model('g_model_AtoB_049300_D2N208allv3.h5', cust)
+model_BtoA = load_model('g_model_BtoA_049300_D2N208allv3.h5', cust)
+# plot A->B->A
+A_real = select_sample(A_data, 1)
+B_generated  = model_AtoB.predict(A_real)
+A_reconstructed = model_BtoA.predict(B_generated)
+show_plot(A_real, B_generated, A_reconstructed)
+# plot B->A->B
+B_real = select_sample(B_data, 1)
+A_generated  = model_BtoA.predict(B_real)
+B_reconstructed = model_AtoB.predict(A_generated)
+show_plot(B_real, A_generated, B_reconstructed)
Index: README.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- README.md	(date 1591012031977)
+++ README.md	(date 1591012031977)
@@ -0,0 +1,37 @@
+# EITN35 - ML Object Detector
+
+# This is a project for training a data object detector
+
+Training and testing an algorithm which will be used to detect persons and dogs in a tunnel.
+
+## Prerequisites
+- Anaconda
+- CUDA-enabled GPU
+
+## Installation
+
+Create an Anaconda environment with the packages listed in requirements.txt
+
+```bash
+conda create --name <NAME> --file requirements.txt
+```
+
+## Setup
+Organize input frames into two separate folders, one for the training set and one for the test set. Update `train_dir` and `test_dir` in `CNN_baseline.py` accordingly.
+
+## Usage
+`CNN_baseline.py` is the main file which trains a model from scratch given frames of a tunnel with objects: person, bike, dog and empty tunnel
+
+Run the command:
+
+```bash
+python CNN_baseline.py
+```
+
+## Contributing
+Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.
+
+Please make sure to update tests as appropriate.
+
+## License
+[MIT](https://choosealicense.com/licenses/mit/)
Index: Utilities/manual_label.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Utilities/manual_label.py	(date 1591012031972)
+++ Utilities/manual_label.py	(date 1591012031972)
@@ -0,0 +1,28 @@
+import cv2
+import numpy as np
+import os
+from matplotlib import pyplot
+from PIL import Image
+
+
+# Dir where video files to be split are located
+unlabeled_directory = '/Users/august/Documents/EITN35_AIQ/video_files/test_set/unlabeled_images/'
+
+for frame_file in os.listdir(unlabeled_directory):
+    if(frame_file.endswith("jpg")):
+        data = pyplot.imread(unlabeled_directory+frame_file)
+        # plot the image
+        pyplot.imshow(data)
+        pyplot.show()
+        # input
+        input1 = input()
+        print("First input was: " + input1)
+
+        # output
+        if input1 == str(1):
+            print("Input was: " + input1)
+            os.rename(
+                unlabeled_directory + frame_file,
+                unlabeled_directory + 'done' + frame_file)
+
+    pyplot.close()
\ No newline at end of file
Index: Utilities/mp4_conversion.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Utilities/mp4_conversion.py	(date 1591012031968)
+++ Utilities/mp4_conversion.py	(date 1591012031968)
@@ -0,0 +1,30 @@
+import os
+import ffmpy
+
+PRINT_DEBUG = True
+
+#Dir where video files to be converted are located
+directory = 'C:/Users/eitn35/Documents/EITN35/Archive/'
+os.chdir(directory)
+
+#Create "converted" folder if it does not exist
+try:
+    if not os.path.exists('converted'):
+        os.makedirs('converted')
+except OSError:
+    print ('Error: Creating directory of data')
+
+#Iterate over video files and convert, then move to converted folder
+for filename in os.listdir(directory):
+    if (filename.endswith(".asf") | filename.endswith(".avi")):
+        if PRINT_DEBUG: print("Starting conversion of " + filename + "...")
+        ff = ffmpy.FFmpeg(
+            inputs={filename: None},
+            outputs={"converted_" + filename.split('.')[0]+'.mp4': None}
+        )
+        ff.run()
+        if PRINT_DEBUG: print("Finished conversion of " + filename + ".")
+        os.rename(
+            directory + "/"+ "converted_" + filename.split(".")[0]+".mp4",
+            directory + "/converted/" + "converted_" +filename.split(".")[0]+".mp4")
+        if PRINT_DEBUG: print("File " + filename + " moved to converted folder.")
\ No newline at end of file
Index: Utilities/run_all_scripts.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Utilities/run_all_scripts.py	(date 1591012031965)
+++ Utilities/run_all_scripts.py	(date 1591012031965)
@@ -0,0 +1,8 @@
+import os
+directory_1 = '/Users/august/PycharmProjects/EITN35'
+
+os.chdir(directory_1)
+os.system("python mp4_conversion.py")
+os.system("python video_splitting.py")
+os.system("python test_set_creator.py")
+os.system("python object_detection_v5.py")
\ No newline at end of file
Index: Utilities/video_splitting.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Utilities/video_splitting.py	(date 1591012031963)
+++ Utilities/video_splitting.py	(date 1591012031963)
@@ -0,0 +1,64 @@
+import cv2
+import numpy as np
+import os
+
+# Dir where video files to be split are located
+#directory_1 = '../../Documents/EITN35/video_files/converted'
+directory_1 = 'C:/Users/eitn35/Documents/EITN35/video_files/Archive'
+# Dir where frames should be saved
+#directory_2 = '../../Documents/EITN35/video_files'
+
+directory_2 = 'C:/Users/eitn35/Documents/EITN35/video_files'
+os.chdir(directory_2)
+
+# Display time stamps of saved frames and progress
+PRINT_DEBUG = True
+
+# Create "frames" folder if it does not exist
+try:
+    if not os.path.exists('frames'):
+        os.makedirs('frames')
+except OSError:
+    print('Error: Creating directory of data')
+
+
+# Splitting one video file. Input: video fiel, Output: frames
+def split_video(video_file):
+    vidcap = cv2.VideoCapture(directory_1 + video_file)
+    sec = 0
+    frameRate = 0.25  # number of seconds between each capture
+    count = 1
+    vidcap.get(cv2.CAP_PROP_POS_MSEC)
+
+    def getFrame(sec):
+        vidcap.set(cv2.CAP_PROP_POS_MSEC, sec*1000)
+        hasFrames, image = vidcap.read()
+        if hasFrames:
+            date_stamp = video_file.split('_')[2]
+            time_stamp = video_file.split('_')[3].split('.')[0] + "+" + str(sec)
+            if PRINT_DEBUG: print("Time stamp " + time_stamp)
+
+            # save frame as JPG file
+            cv2.imwrite("./frames/frame_"+date_stamp+"_"+time_stamp+"nr"+str(count)+"_"+".jpg", image)
+            #cv2.imwrite("./frames/frame_" + str(count) + ".jpg", image)
+        return hasFrames
+
+    success = getFrame(sec)
+    while success and (sec*1000< vidcap.get(cv2.CAP_PROP_POS_MSEC)):
+        count = count + 1
+        sec = sec + frameRate
+        sec = round(sec, 2)
+        success = getFrame(sec)
+
+    # When everything done, release the capture
+    vidcap.release()
+    cv2.destroyAllWindows()
+
+#Dir where video files to be split are located
+
+
+for video_file in os.listdir(directory_1):
+    #if(video_file.endswith("mp4")):
+    if PRINT_DEBUG: print("Starting splitting of " + video_file + "...")
+    split_video('/' + video_file)
+    if PRINT_DEBUG: print("Splitting of " + video_file + " done.")
\ No newline at end of file
Index: Utilities/test_set_creator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Utilities/test_set_creator.py	(date 1591012031960)
+++ Utilities/test_set_creator.py	(date 1591012031960)
@@ -0,0 +1,55 @@
+import os
+import random as rand
+
+#Dir where the test_set folder should be placed
+#directory_1 = '/Users/august/Documents/EITN35_AIQ/video_files/frames/'
+directory_1 = 'C:/Users/eitn35/Documents/EITN35 EVOLVE/image_frames/'
+
+#Dir where the frames folder is located
+#directory_2 = '/Users/august/Documents/EITN35_AIQ/video_files/frames/test_set'
+directory_2 = '/Users/august/Documents/EITN35_AIQ/video_files/frames/test_set'
+os.chdir(directory_1)
+
+try:
+    if not os.path.exists('test_set'):
+        os.makedirs('test_set')
+        os.chdir(directory_2)
+        os.makedirs('person_set')
+        os.makedirs('bike_set')
+        os.makedirs('dog_set')
+        os.makedirs('empty_set')
+except OSError:
+    print('Error: Creating directory of data')
+
+os.chdir(directory_1 + 'train_set')
+
+count = 0
+wantedSplit = [0.8, 0.1, 0.1] #training, validation, test
+noExtract = len(os.listdir(directory_2))*wantedSplit[2]
+PRINT_DEBUG = True
+
+persons = 0
+bikes = 0
+dogs = 0
+emptys = 0
+done = False
+
+while not done:
+    index = rand.randint(0, len(os.listdir(directory_1 + 'train_set')))
+    img_pick = os.listdir(directory_1 + 'train_set').sort()[index]
+    no_persons =
+
+while count < noExtract:
+    index = rand.randint(1, len(os.listdir(directory_2)))
+    file_list = os.listdir(directory_2)
+
+    #if PRINT_DEBUG : print(str(file_list[index]) + " exported to test_set...")
+
+    os.rename(
+        directory_2 + str(file_list[index]),
+        directory_1 + '/test_set/' + str(file_list[index])
+    )
+    count += 1
+
+print(str(noExtract) + " files exported to test_set")
+
Index: Utilities/result_run_printer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Utilities/result_run_printer.py	(date 1591012031957)
+++ Utilities/result_run_printer.py	(date 1591012031957)
@@ -0,0 +1,32 @@
+import os
+
+def print_to_file(acc, val_acc, loss, val_loss, epochs, layers, model):
+    # Create "training_results" folder if it does not exist
+
+    os.chdir("C:/Users/eitn35/Documents/EITN35_EVOLVE/models_and_weights_EVOLVE/models")
+
+    try:
+        if not os.path.exists('training_results'):
+            os.makedirs('training_results')
+    except OSError:
+        print('Error: Creating directory of data')
+
+    results = "C:/Users/eitn35/Documents/EITN35_EVOLVE/models_and_weights_EVOLVE/models/training_results/"
+
+    os.chdir(results)
+    current_results = os.listdir(results)
+    current_results.sort()
+    working_file = current_results[len(os.listdir(results)) - 1]
+    print("Latest file is: " + str(working_file))
+
+    file = open(working_file, "a+")
+    file.write("\n \n -----------------------------------")
+    file.write("\n Model " + str(model) + " Run Result")
+    file.write("\n -----------------------------------")
+    file.write("\n Hidden Layers:           " + str(layers))
+    file.write("\n Epochs:                  " + str(epochs))
+    file.write("\n Training Set Accuracy:   " + str(acc))
+    file.write("\n Validation Set Accuracy: " + str(val_acc))
+    file.write("\n Training Loss:           " + str(loss))
+    file.write("\n Validation Loss:         " + str(val_loss))
+    file.close()
\ No newline at end of file
Index: Utilities/train_val_test_set_creator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Utilities/train_val_test_set_creator.py	(date 1591012031954)
+++ Utilities/train_val_test_set_creator.py	(date 1591012031954)
@@ -0,0 +1,82 @@
+import os
+import random as rand
+
+#Dir where the test_set folder should be placed
+#directory_1 = '/Users/august/Documents/EITN35_AIQ/video_files/frames/'
+directory_1 = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/image_frames/'
+
+#Dir where the frames folder are located
+#directory_2 = '/Users/august/Documents/EITN35_AIQ/video_files/frames/test_set'
+directory_2 = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/image_frames/all_labeled_frames_used/'
+
+os.chdir(directory_1)
+
+try:
+    if not os.path.exists('test_set'):
+        os.makedirs('train_set')
+        os.makedirs('val_set')
+        os.makedirs('test_set')
+except OSError:
+    print('Error: Creating directory of data')
+
+dir1 = directory_2 + 'person_set/'
+dir2 = directory_2 + 'person_night_set/'
+dir3 = directory_2 + 'dog_set/'
+dir4 = directory_2 + 'bike_set/'
+dir5 = directory_2 + 'empty_set/'
+dir6 = directory_2 + 'empty_night_set/'
+
+dirs = [dir1, dir2, dir3, dir4, dir5, dir6]
+
+dest1 = directory_1 + 'train_set/'
+dest2 = directory_1 + 'val_set/'
+dest3 = directory_1 + 'test_set/'
+
+for i in range(len(dirs)):
+    curr_dir = dirs[i]
+    os.chdir(curr_dir)
+
+    count = 0
+    wantedSplit = [0.64, 0.16, 0.2] #training, validation, test
+    noExtract_train = len(os.listdir(curr_dir))*wantedSplit[0] #train set
+    noExtract_val = len(os.listdir(curr_dir))*wantedSplit[1] #val set
+    noExtract_test = len(os.listdir(curr_dir))*wantedSplit[2] #test set
+
+    PRINT_DEBUG = True
+
+    while count < noExtract_train-1:
+        index = rand.randint(1, len(os.listdir(curr_dir))-1)
+        file_list = os.listdir(curr_dir)
+
+        os.rename(
+            curr_dir + str(file_list[index]),
+            dest1 + str(file_list[index])
+        )
+        count += 1
+
+    count = 0
+
+    while count < noExtract_val-1:
+        index = rand.randint(1, len(os.listdir(curr_dir))-1)
+        file_list = os.listdir(curr_dir)
+
+        os.rename(
+            curr_dir + str(file_list[index]),
+            dest2 + str(file_list[index])
+        )
+        count += 1
+
+    count = 0
+
+    while count < noExtract_test-1:
+        index = rand.randint(1, len(os.listdir(curr_dir))-1)
+        file_list = os.listdir(curr_dir)
+
+        os.rename(
+            curr_dir + str(file_list[index]),
+            dest3 + str(file_list[index])
+        )
+        count += 1
+
+#print(str(noExtract) + " files exported to test_set")
+
Index: CycleGan.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- CycleGan.py	(date 1591005416405)
+++ CycleGan.py	(date 1591005416405)
@@ -0,0 +1,300 @@
+# example of training a cyclegan on the horse2zebra dataset
+from random import random
+from numpy import load
+from numpy import zeros
+from numpy import ones
+from numpy import asarray
+from numpy.random import randint
+from keras.optimizers import Adam
+from keras.initializers import RandomNormal
+from keras.models import Model
+from keras.models import Input
+from keras.layers import Conv2D
+from keras.layers import Conv2DTranspose
+from keras.layers import LeakyReLU
+from keras.layers import Activation
+from keras.layers import Concatenate
+from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization
+from matplotlib import pyplot
+from numpy import vstack
+
+def show_plot_train(X_realA, X_realB, X_fakeA,X_fakeB):
+	images = vstack((X_realA, X_realB, X_fakeA,X_fakeB))
+	titles = ['X_realA', 'X_realB', 'X_fakeA','X_fakeB']
+	# scale from [-1,1] to [0,1]
+	images = (images + 1) / 2.0
+	# plot images row by row
+	for i in range(len(images)):
+		# define subplot
+		pyplot.subplot(1, len(images), 1 + i)
+		# turn off axis
+		pyplot.axis('off')
+		# plot raw pixel data
+		pyplot.imshow(images[i])
+		# title
+		pyplot.title(titles[i])
+	pyplot.show()
+
+# define the discriminator model
+def define_discriminator(image_shape):
+	# weight initialization
+	init = RandomNormal(stddev=0.02)
+	# source image input
+	in_image = Input(shape=image_shape)
+	# C64
+	d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)
+	d = LeakyReLU(alpha=0.2)(d)
+	# C128
+	d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)
+	d = InstanceNormalization(axis=-1)(d)
+	d = LeakyReLU(alpha=0.2)(d)
+	# C256
+	d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)
+	d = InstanceNormalization(axis=-1)(d)
+	d = LeakyReLU(alpha=0.2)(d)
+	# C512
+	d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)
+	d = InstanceNormalization(axis=-1)(d)
+	d = LeakyReLU(alpha=0.2)(d)
+	# second last output layer
+	d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)
+	d = InstanceNormalization(axis=-1)(d)
+	d = LeakyReLU(alpha=0.2)(d)
+	# patch output
+	patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)
+	# define model
+	model = Model(in_image, patch_out)
+	# compile model
+	model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])
+	return model
+
+# generator a resnet block
+def resnet_block(n_filters, input_layer):
+	# weight initialization
+	init = RandomNormal(stddev=0.02)
+	# first layer convolutional layer
+	g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# second convolutional layer
+	g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	# concatenate merge channel-wise with input layer
+	g = Concatenate()([g, input_layer])
+	return g
+
+# define the standalone generator model
+def define_generator(image_shape, n_resnet=9):
+	# weight initialization
+	init = RandomNormal(stddev=0.02)
+	# image input
+	in_image = Input(shape=image_shape)
+	# c7s1-64
+	g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# d128
+	g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# d256
+	g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# R256
+	for _ in range(n_resnet):
+		g = resnet_block(256, g)
+	# u128
+	g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# u64
+	g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	g = Activation('relu')(g)
+	# c7s1-3
+	g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)
+	g = InstanceNormalization(axis=-1)(g)
+	out_image = Activation('tanh')(g)
+	# define model
+	model = Model(in_image, out_image)
+	return model
+
+# define a composite model for updating generators by adversarial and cycle loss
+def define_composite_model(g_model_1, d_model, g_model_2, image_shape):
+	# ensure the model we're updating is trainable
+	g_model_1.trainable = True
+	# mark discriminator as not trainable
+	d_model.trainable = False
+	# mark other generator model as not trainable
+	g_model_2.trainable = False
+	# discriminator element
+	input_gen = Input(shape=image_shape)
+	gen1_out = g_model_1(input_gen)
+	output_d = d_model(gen1_out)
+	# identity element
+	input_id = Input(shape=image_shape)
+	output_id = g_model_1(input_id)
+	# forward cycle
+	output_f = g_model_2(gen1_out)
+	# backward cycle
+	gen2_out = g_model_2(input_id)
+	output_b = g_model_1(gen2_out)
+	# define model graph
+	model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])
+	# define optimization algorithm configuration
+	opt = Adam(lr=0.0002, beta_1=0.5)
+	# compile model with weighting of least squares loss and L1 loss
+	model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)
+	return model
+
+# load and prepare training images
+def load_real_samples(filename):
+	# load the dataset
+	data = load(filename)
+	# unpack arrays
+	X1, X2 = data['arr_0'], data['arr_1']
+	# scale from [0,255] to [-1,1]
+	X1 = (X1 - 127.5) / 127.5
+	X2 = (X2 - 127.5) / 127.5
+	return [X1, X2]
+
+# select a batch of random samples, returns images and target
+def generate_real_samples(dataset, n_samples, patch_shape):
+	# choose random instances
+	ix = randint(0, dataset.shape[0], n_samples)
+	# retrieve selected images
+	X = dataset[ix]
+	# generate 'real' class labels (1)
+	y = ones((n_samples, patch_shape, patch_shape, 1))
+	return X, y
+
+# generate a batch of images, returns images and targets
+def generate_fake_samples(g_model, dataset, patch_shape):
+	# generate fake instance
+	X = g_model.predict(dataset)
+	# create 'fake' class labels (0)
+	y = zeros((len(X), patch_shape, patch_shape, 1))
+	return X, y
+
+# save the generator models to file
+def save_models(step, g_model_AtoB, g_model_BtoA):
+	# save the first generator model
+	filename1 = 'g_model_AtoB_%06d_D2N208allv3.h5' % (step+1)
+	g_model_AtoB.save(filename1)
+	# save the second generator model
+	filename2 = 'g_model_BtoA_%06d_D2N208allv3.h5' % (step+1)
+	g_model_BtoA.save(filename2)
+	print('>Saved: %s and %s' % (filename1, filename2))
+
+# generate samples and save as a plot and save the model
+def summarize_performance(step, g_model, trainX, name, n_samples=5):
+	# select a sample of input images
+	X_in, _ = generate_real_samples(trainX, n_samples, 0)
+	# generate translated images
+	X_out, _ = generate_fake_samples(g_model, X_in, 0)
+	# scale all pixels from [-1,1] to [0,1]
+	X_in = (X_in + 1) / 2.0
+	X_out = (X_out + 1) / 2.0
+	# plot real images
+	#for i in range(n_samples):
+		#pyplot.subplot(2, n_samples, 1 + i)
+		#pyplot.axis('off')
+		#pyplot.imshow(X_in[i])
+	# plot translated image
+	#for i in range(n_samples):
+		#pyplot.subplot(2, n_samples, 1 + n_samples + i)
+		#pyplot.axis('off')
+		#pyplot.imshow(X_out[i])
+	# save plot to file
+	filename1 = '%s_generated_plot_%06d_D2N208allv3.png' % (name, (step+1))
+	pyplot.savefig(filename1)
+	pyplot.close()
+
+# update image pool for fake images
+def update_image_pool(pool, images, max_size=50):
+	selected = list()
+	for image in images:
+		if len(pool) < max_size:
+			# stock the pool
+			pool.append(image)
+			selected.append(image)
+		elif random() < 0.5:
+			# use image, but don't add it to the pool
+			selected.append(image)
+		else:
+			# replace an existing image and use replaced image
+			ix = randint(0, len(pool))
+			selected.append(pool[ix])
+			pool[ix] = image
+	return asarray(selected)
+
+# train cyclegan models
+def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):
+
+	# define properties of the training run
+	n_epochs, n_batch, = 100, 1
+	# determine the output square shape of the discriminator
+	n_patch = d_model_A.output_shape[1]
+	# unpack dataset
+	trainA, trainB = dataset
+	# prepare image pool for fakes
+	poolA, poolB = list(), list()
+	# calculate the number of batches per training epoch
+	bat_per_epo = int(len(trainA) / n_batch)
+	# calculate the number of training iterations
+	n_steps = bat_per_epo * n_epochs
+	# manually enumerate epochs
+	for i in range(n_steps):
+		# select a batch of real samples
+		X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)
+		X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)
+		# generate a batch of fake samples
+		X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)
+		X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)
+		# update fakes from pool
+		X_fakeA = update_image_pool(poolA, X_fakeA)
+		X_fakeB = update_image_pool(poolB, X_fakeB)
+		# update generator B->A via adversarial and cycle loss
+		g_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])
+		# update discriminator for A -> [real/fake]
+		dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)
+		dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)
+		# update generator A->B via adversarial and cycle loss
+		g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])
+		# update discriminator for B -> [real/fake]
+		dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)
+		dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)
+		#show_plot_train(X_realA, X_realB, X_fakeA,X_fakeB)
+		# summarize performance
+		print('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))
+		# evaluate the model performance every so often
+		if (i+1) % (bat_per_epo * 1) == 0:
+			# plot A->B translation
+			summarize_performance(i, g_model_AtoB, trainA, 'AtoB')
+			# plot B->A translation
+			summarize_performance(i, g_model_BtoA, trainB, 'BtoA')
+			save_models(i, g_model_AtoB, g_model_BtoA)
+		#if (i+1) % (bat_per_epo * 5) == 0:
+			# save the models
+			#save_models(i, g_model_AtoB, g_model_BtoA)
+
+# load image data
+dataset = load_real_samples('../../Documents/EITN35/day2night208all.npz')
+print('Loaded', dataset[0].shape, dataset[1].shape)
+# define input shape based on the loaded dataset
+image_shape = dataset[0].shape[1:]
+# generator: A -> B
+g_model_AtoB = define_generator(image_shape)
+# generator: B -> A
+g_model_BtoA = define_generator(image_shape)
+# discriminator: A -> [real/fake]
+d_model_A = define_discriminator(image_shape)
+# discriminator: B -> [real/fake]
+d_model_B = define_discriminator(image_shape)
+# composite: A -> B -> [real/fake, A]
+c_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)
+# composite: B -> A -> [real/fake, B]
+c_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)
+# train models
+train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)
\ No newline at end of file
Index: GAN-Mnist.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- GAN-Mnist.py	(date 1588067487777)
+++ GAN-Mnist.py	(date 1588067487777)
@@ -0,0 +1,189 @@
+import os
+os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"   # see issue #152
+os.environ["CUDA_VISIBLE_DEVICES"] = ""
+
+
+#Import
+from keras.datasets import mnist
+from keras.utils import np_utils
+from keras.models import Sequential, Model
+from keras.layers import Input, Dense, Dropout, Activation, Flatten
+from keras.layers.advanced_activations import LeakyReLU
+from tensorflow.keras.optimizers import Adam, RMSprop
+import tensorflow as tf
+
+import numpy as np
+import matplotlib.pyplot as plt
+from numpy.random import randn
+from numpy.random import randint
+from numpy.random import rand
+from numpy import zeros
+from numpy import ones
+from matplotlib import pyplot
+import random
+from numpy import vstack
+print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
+
+#Input dimension generator
+input_dim = 100
+
+# Optimizer
+adam = Adam(lr=0.0002, beta_1=0.5)
+
+discriminator = Sequential()
+discriminator.add(Dense(1024, input_dim=784, activation=LeakyReLU(alpha=0.2)))
+discriminator.add(Dropout(0.4))
+discriminator.add(Dense(512, activation=LeakyReLU(alpha=0.2)))
+discriminator.add(Dropout(0.4))
+discriminator.add(Dense(256, activation=LeakyReLU(alpha=0.2)))
+discriminator.add(Dropout(0.4))
+discriminator.add(Dense(1, activation='sigmoid'))
+discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
+
+discriminator.summary()
+
+#Generator-model
+generator = Sequential()
+generator.add(Dense(256, input_dim=input_dim, activation=LeakyReLU(alpha=0.2)))
+generator.add(Dense(512, activation=LeakyReLU(alpha=0.2)))
+generator.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))
+generator.add(Dense(784, activation='tanh'))
+generator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
+generator.summary()
+
+#GAN-model
+discriminator.trainable = False
+inputs = Input(shape=(input_dim, ))
+hidden = generator(inputs)
+output = discriminator(hidden)
+gan = Model(inputs, output)
+gan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
+gan.summary()
+
+#Load and preprocess data from MNIST
+(x_train, _), (_, _) = mnist.load_data()
+
+
+x_train = (x_train - 127.5)/127.5
+print(x_train.shape)
+x_train = x_train.reshape(60000, 784)
+print(x_train.shape)
+
+# select real samples
+def generate_real_samples(x_train, n_samples):
+    # choose random instances
+    ix = randint(0, x_train.shape[0], n_samples)
+    # retrieve selected images
+    X = x_train[ix]
+    # generate 'real' class labels (1)
+    y = ones((n_samples, 1))
+    return X, y
+# generate n noise samples with class labels
+def generate_noise_samples(n_samples):
+    # generate uniform random numbers in [0,1]
+    X = rand(784 * n_samples)
+    # reshape into a batch of grayscale images
+    X = X.reshape((n_samples, 784))
+    # generate 'fake' class labels (0)
+    y = zeros((n_samples, 1))
+    return X, y
+
+
+# generate points in latent space as input for the generator
+def generate_latent_points(input_dim, n_samples):
+    # generate points in the latent space
+    x_input = randn(input_dim * n_samples)
+    # reshape into a batch of inputs for the network
+    x_input = x_input.reshape(n_samples, input_dim)
+    return x_input
+# use the generator to generate n fake examples, with class labels
+def generate_fake_samples(model, latent_dim, n_samples):
+    # generate points in latent space
+    x_input = generate_latent_points(latent_dim, n_samples)
+    # predict outputs
+    x = model.predict(x_input)
+    # create 'fake' class labels (0)
+    y = zeros((n_samples, 1))
+    return x, y
+
+
+# Train discriminator
+def train_discriminator(model, dataset, number_iteration, batch_size):
+    half_batch = int(batch_size / 2)
+    for i in range(number_iteration):
+        # Sample from real images
+        x_real, y_real = generate_real_samples(x_train, (half_batch))
+        # Sample from noise
+        x_noise, y_noise = generate_noise_samples((half_batch))
+        # Train with real images
+        _, real_acc = model.train_on_batch(x_real, y_real)
+        print(x_real.shape)
+        # Train with noise
+        _, noise_acc = model.train_on_batch(x_noise, y_noise)
+        print('>%d real=%.0f%% noise=%.0f%%' % (i + 1, real_acc * 100, noise_acc * 100))
+
+
+train_discriminator(discriminator, x_train, 25, 256)
+
+n_samples = 25
+x, _ = generate_fake_samples(generator, input_dim, n_samples)
+x = x.reshape(n_samples, 28, 28)
+for i in range(n_samples):
+    pyplot.subplot(5, 5, 1 + i)
+    pyplot.axis('off')
+    pyplot.imshow(x[i], cmap='gray_r')
+
+pyplot.show()
+
+def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):
+    # prepare real samples
+    x_real, y_real = generate_real_samples(dataset, n_samples)
+    # evaluate discriminator on real examples
+    _, acc_real = d_model.evaluate(x_real, y_real, verbose=0)
+    # prepare fake examples
+    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)
+    # evaluate discriminator on fake examples
+    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)
+    # summarize discriminator performance
+    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))
+
+def train_gan(generator, discriminator, gan, x_train, input_dim, n_epochs, batch_size):
+    batch_per_epoch = int(x_train.shape[0]/batch_size)
+    half_batch = int(batch_size/2)
+    for i in range(n_epochs):
+        for j in range(batch_per_epoch):
+            x_real, y_real = generate_real_samples(x_train, half_batch)
+            x_fake, y_fake = generate_fake_samples(generator, input_dim, half_batch)
+            x, y = vstack((x_real, x_fake)), vstack((y_real, y_fake))
+            discriminator_loss = discriminator.train_on_batch(x, y)
+            x_gan = generate_latent_points(input_dim, batch_size)
+            y_gan = ones((batch_size, 1))
+            generator_loss = gan.train_on_batch(x_gan, y_gan)
+            print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, batch_per_epoch, discriminator_loss[0], generator_loss[0]))
+            # evaluate the model performance, sometimes
+            if (j==1):
+                summarize_performance(i, generator, discriminator, x_train, input_dim)
+
+train_gan(generator,discriminator,gan,x_train,input_dim, 20, 256)
+
+n_samples = 25
+pyplot.figure(figsize=(10, 10))
+x, _ = generate_fake_samples(generator, input_dim, n_samples)
+x = x.reshape(n_samples, 28, 28)
+for i in range(n_samples):
+    pyplot.subplot(5, 5, 1 + i)
+    pyplot.axis('off')
+    pyplot.imshow(x[i], cmap='gray_r')
+
+pyplot.show()
+
+n_samples = 25
+pyplot.figure(figsize=(10, 10))
+x, _ = generate_real_samples(x_train, n_samples)
+x = x.reshape(n_samples, 28, 28)
+for i in range(n_samples):
+    pyplot.subplot(5, 5, 1 + i)
+    pyplot.axis('off')
+    pyplot.imshow(x[i], cmap='gray_r')
+
+pyplot.show()
\ No newline at end of file
Index: CNN_baseline.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- CNN_baseline.py	(date 1591005416405)
+++ CNN_baseline.py	(date 1591005416405)
@@ -0,0 +1,366 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+# In[1]:
+
+
+import cv2
+import numpy as np
+import pandas as pd
+import sklearn
+import matplotlib.pyplot as plt
+
+import os
+import random
+import gc #garbage collector for cleaning deleted data from memory
+
+
+# In[2]:
+
+
+train_dir = 'C:/Users/eitn35/Documents/EITN35/video_files/frames/Small_train_set/'
+test_dir = 'C:/Users/eitn35/Documents/EITN35/video_files/frames/Small_test_set/'
+
+#rain_person = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'persons_1' in i]  #get person images
+#rain_dogs = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'dogs_1' in i]  #get dog images
+#rain_bikes = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'bikes_1' in i]  #get bike images
+#rain_empty = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if ('persons_0'and 'dogs_0' and 'bikes_0')in i]  #get bike images
+
+
+test_imgs = [test_dir+'{}'.format(i) for i in os.listdir(test_dir)] #get test images
+
+
+#train_imgs = train_person + train_dogs + train_bikes + train_empty   # slice the dataset and use 3 persons
+train_imgs = [train_dir+'{}'.format(i) for i in os.listdir(train_dir)] #get test images
+random.shuffle(train_imgs)  # shuffle it randomly
+random.shuffle(test_imgs)
+
+
+gc.collect()   #collect garbage to save memory
+
+
+# In[3]:
+
+
+import matplotlib.image as mpimg
+for ima in train_imgs[0:4]:
+     img=mpimg.imread(ima)
+     imgplot = plt.imshow(img)
+     plt.show()
+
+
+# In[4]:
+
+
+#Lets declare our image dimensions
+#we are using coloured images. 
+nrows = 640
+ncolumns = 360
+channels = 3  #change to 1 if you want to use grayscale image
+
+#A function to read and process the images to an acceptable format for our model
+def read_and_process_image(list_of_images):
+    """
+    Returns two arrays: 
+        X is an array of resized images
+        y is an array of labels
+    """
+    X = [] # images
+    y = []# labels
+    i = 0
+    for image in list_of_images:
+        #ändra här mellan COLOR och GRAYSCALE beroende på antal channels
+        X.append(cv2.resize(cv2.imread(image,cv2.IMREAD_COLOR), (nrows,ncolumns), interpolation=cv2.INTER_CUBIC))  #Read the image
+        #get the labels
+        if 'persons_1' in image:
+            y.append(1)
+        elif 'dogs_1' in image:
+            y.append(2)
+        elif 'bikes_1' in image:
+            y.append(3)
+        else:
+            y.append(0)
+        i += 1
+    return X, y
+
+
+# In[5]:
+
+
+class_names = ['empty', 'person', 'dogs', 'bikes']
+
+
+# In[6]:
+
+print('reading dataset...')
+X, y = read_and_process_image(train_imgs)
+X_test, y_test = read_and_process_image(test_imgs)
+
+
+# In[7]:
+
+
+y[0]
+
+
+# In[8]:
+
+
+#Lets view some of the pics
+plt.figure(figsize=(20,10))
+columns = 4
+for i in range(columns):
+    plt.subplot(5 / columns + 1, columns, i + 1)
+    plt.imshow(X[i])
+
+
+# In[9]:
+
+
+import seaborn as sns
+
+gc.collect()
+
+#Convert list to numpy array
+X = np.array(X)
+y = np.array(y)
+X_test = np.array(X_test)
+y_test = np.array(y_test)
+#Lets plot the label to be sure we just have two class
+#sns.countplot(y)
+#plt.title('Labels for Cats and Dogs')
+
+
+# In[10]:
+
+
+print("Shape of train images is:", X.shape)
+print("Shape of labels is:", y.shape)
+
+
+# In[11]:
+
+
+#Lets split the data into train and test set
+from sklearn.model_selection import train_test_split
+X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=2)
+
+print("Shape of train images is:", X_train.shape)
+print("Shape of validation images is:", X_val.shape)
+print("Shape of labels is:", y_train.shape)
+print("Shape of labels is:", y_val.shape)
+
+
+# In[12]:
+
+
+#clear memory
+#del X
+#del y
+gc.collect()
+
+#get the length of the train and validation data
+ntrain = len(X_train)
+nval = len(X_val)
+
+#We will use a batch size of 32. Note: batch size should be a factor of 2.***4,8,16,32,64...***
+batch_size = 32
+
+
+# In[13]:
+
+
+
+#from keras import models
+#from keras import optimizers
+from tensorflow.keras import models, layers
+from keras.preprocessing.image import ImageDataGenerator
+from keras.preprocessing.image import img_to_array, load_img
+import tensorflow as tf
+
+model = models.Sequential()
+model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(ncolumns, nrows, 3))) #input ska var (150, 150, 3)
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(64, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Flatten())
+model.add(layers.Dropout(0.4))  #Dropout for regularization
+model.add(layers.Dense(512, activation='relu'))
+model.add(layers.Dense(4))  #Sigmoid function at the end because we have just two classes
+
+
+# In[14]:
+
+
+#Lets see our model
+model.summary()
+
+
+# In[15]:
+
+
+#We'll use the RMSprop optimizer with a learning rate of 0.0001
+#We'll use binary_crossentropy loss because its a binary classification
+model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])
+
+
+# In[16]:
+
+
+#Lets create the augmentation configuration
+#This helps prevent overfitting, since we are using a small dataset
+#train_datagen = ImageDataGenerator(rescale=1./255,   #Scale the image between 0 and 1
+ #                                   rotation_range=40,
+  #                                  width_shift_range=0.2,
+   #                                 height_shift_range=0.2,
+    #                                shear_range=0.2,
+     #                               zoom_range=0.2,
+      #                              horizontal_flip=True,)
+
+#val_datagen = ImageDataGenerator(rescale=1./255)  #We do not augment validation data. we only perform rescale
+
+
+# In[17]:
+
+
+
+#Create the image generators
+#train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)
+#val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)
+
+
+# In[18]:
+
+
+
+#The training part
+#We train for 64 epochs with about 100 steps per epoch
+history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))
+
+
+# In[ ]:
+
+
+#Save the model
+model.save_weights('model_cat&dog1_weights.h5')
+model.save('model_cat&dog1_keras.h5')
+
+
+# In[ ]:
+
+
+#lets plot the train and val curve
+#get the details form the history object
+acc = history.history['acc']
+val_acc = history.history['val_acc']
+loss = history.history['loss']
+val_loss = history.history['val_loss']
+
+epochs = range(1, len(acc) + 1)
+
+#Train and validation accuracy
+#plt.plot(epochs, acc, 'b', label='Training accurarcy')
+#plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
+#plt.title('Training and Validation accurarcy')
+#plt.legend()
+
+#plt.figure()
+
+#Train and validation loss
+#plt.plot(epochs, loss, 'b', label='Training loss')
+#plt.plot(epochs, val_loss, 'r', label='Validation loss')
+#plt.title('Training and Validation loss')
+#plt.legend()
+
+#plt.show()
+
+
+# In[ ]:
+
+
+probability_model = tf.keras.Sequential([model,
+                                         tf.keras.layers.Softmax()])
+
+predictions = probability_model.predict(X_test)
+
+
+
+
+
+def plot_image(i, predictions_array, true_label, img):
+    predictions_array, true_label, img = predictions_array, true_label[i], img[i]
+    plt.grid(False)
+    plt.xticks([])
+    plt.yticks([])
+    plt.imshow(img, cmap=plt.cm.binary)
+
+    predicted_label = np.argmax(predictions_array)
+    if predicted_label == true_label:
+        color = 'blue'
+    else:
+        color = 'red'
+
+    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
+                                100*np.max(predictions_array),
+                                class_names[true_label]),
+                                color=color)
+
+def plot_value_array(i, predictions_array, true_label):
+    predictions_array, true_label = predictions_array, true_label[i]
+    plt.grid(False)
+    plt.xticks(range(4))
+    plt.yticks([])
+    thisplot = plt.bar(range(4), predictions_array, color="#777777")
+    plt.ylim([0, 1])
+    predicted_label = np.argmax(predictions_array)
+
+    thisplot[predicted_label].set_color('red')
+    thisplot[true_label].set_color('blue')
+
+# Plot the first X test images, their predicted labels, and the true labels.
+# Color correct predictions in blue and incorrect predictions in red.
+num_rows = 5
+num_cols = 3
+num_images = num_rows*num_cols
+plt.figure(figsize=(2*2*num_cols, 2*num_rows))
+for i in range(num_images):
+  plt.subplot(num_rows, 2*num_cols, 2*i+1)
+  plot_image(i, predictions[i], y_test, X_test)
+  plt.subplot(num_rows, 2*num_cols, 2*i+2)
+  plot_value_array(i, predictions[i], y_test)
+plt.tight_layout()
+plt.show()
+
+
+# In[ ]:
+
+
+probability_model = tf.keras.Sequential([model,
+                                         tf.keras.layers.Softmax()])
+
+predictions = probability_model.predict(X_train)
+
+predictions[0]
+
+np.argmax(predictions[0])
+
+y_train[0]
+
+num_rows = 5
+num_cols = 3
+num_images = num_rows*num_cols
+plt.figure(figsize=(2*2*num_cols, 2*num_rows))
+for i in range(num_images):
+  plt.subplot(num_rows, 2*num_cols, 2*i+1)
+  plot_image(i, predictions[i], y_train, X_train)
+  plt.subplot(num_rows, 2*num_cols, 2*i+2)
+  plot_value_array(i, predictions[i], y_train)
+plt.tight_layout()
+plt.show()
+
+
+
Index: manual_label.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- manual_label.py	(date 1591005416405)
+++ manual_label.py	(date 1591005416405)
@@ -0,0 +1,28 @@
+import cv2
+import numpy as np
+import os
+from matplotlib import pyplot
+from PIL import Image
+
+
+# Dir where video files to be split are located
+unlabeled_directory = '/Users/august/Documents/EITN35_AIQ/video_files/test_set/unlabeled_images/'
+
+for frame_file in os.listdir(unlabeled_directory):
+    if(frame_file.endswith("jpg")):
+        data = pyplot.imread(unlabeled_directory+frame_file)
+        # plot the image
+        pyplot.imshow(data)
+        pyplot.show()
+        # input
+        input1 = input()
+        print("First input was: " + input1)
+
+        # output
+        if input1 == str(1):
+            print("Input was: " + input1)
+            os.rename(
+                unlabeled_directory + frame_file,
+                unlabeled_directory + 'done' + frame_file)
+
+    pyplot.close()
\ No newline at end of file
Index: ImageRecognition/automated_layer_testing.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- ImageRecognition/automated_layer_testing.py	(date 1591012032039)
+++ ImageRecognition/automated_layer_testing.py	(date 1591012032039)
@@ -0,0 +1,48 @@
+import os
+
+#project file directory
+work_dir = 'C:/Users/eitn35/PycharmProjects/EITN35/'
+os.chdir(work_dir)
+
+#results directory, will be created if it non-existent
+results = work_dir + '/training_results/'
+
+# Create "training_results" folder if it does not exist
+try:
+    if not os.path.exists('training_results'):
+        os.makedirs('training_results')
+except OSError:
+    print('Error: Creating directory of data')
+
+#retrieves number of previous runs
+runNo = len(os.listdir(results))+1
+file_name = 'EVOLVE_Baseline_model_' + str(runNo) + '.txt'
+
+#creates new results file
+os.chdir(results)
+file = open(file_name, "w")
+file.write("EVOLVE_Baseline_model_" + str(runNo))
+file.write("\n")
+file.close()
+
+#trains and tests the models with different layer configurations, writes acc, acc_loss, loss and val_loss to file
+os.chdir(work_dir)
+##                                          L2 Drop Run_number
+os.system("python Baseline_model_V3_224x224.py 0 0 1")
+os.system("python Baseline_model_V3_224x224.py 0 0 2")
+os.system("python Baseline_model_V3_224x224.py 0 0 3")
+os.system("python Baseline_model_V3_224x224.py 0 0 4")
+os.system("python Baseline_model_V3_224x224.py 0 0 5")
+os.system("python Baseline_model_V3_224x224.py 0 0 6")
+os.system("python Baseline_model_V3_224x224.py 0 0 7")
+os.system("python Baseline_model_V3_224x224.py 0 0 8")
+os.system("python Baseline_model_V3_224x224.py 0 0 9")
+os.system("python Baseline_model_V3_224x224.py 0 0 10")
+
+#Open the file back and print the contents
+os.chdir(results)
+print("Trying to open " + file_name)
+f = open(file_name, "r")
+if f.mode == 'r':
+    contents =f.read()
+    print(contents)
\ No newline at end of file
Index: ImageRecognition/EVOLVE_model_V1_224x224.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- ImageRecognition/EVOLVE_model_V1_224x224.py	(date 1591012032036)
+++ ImageRecognition/EVOLVE_model_V1_224x224.py	(date 1591012032036)
@@ -0,0 +1,355 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+import cv2
+import numpy as np
+import pandas as pd
+import sklearn
+import matplotlib.pyplot as plt
+
+import os
+import random
+import gc  # garbage collector for cleaning deleted data from memory
+
+from keras.models import Model
+from tensorflow.keras import models, layers, regularizers
+from keras.preprocessing.image import ImageDataGenerator
+from keras.preprocessing.image import img_to_array, load_img
+import tensorflow as tf
+from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout
+import sys
+import math
+import time
+import getopt
+
+
+print ('Nbr of arguments: ' , len(sys.argv) , 'total')
+print ('Arguments list: ' , str(sys.argv))
+
+train_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/image_frames/train_set/'
+val_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/image_frames/val_set/'
+test_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/image_frames/test_set/'
+save_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/models_and_weights_EVOLVE/models/saved_models_and_weights/'
+work_dir = 'C:/Users/eitn35/PycharmProjects/EITN35/'
+
+test_imgs = [test_dir + '{}'.format(i) for i in os.listdir(test_dir)]  # get test images
+val_imgs = [val_dir + '{}'.format(i) for i in os.listdir(val_dir)]  # get val images
+train_imgs = [train_dir + '{}'.format(i) for i in os.listdir(train_dir)]  # get test images
+
+# shuffle it randomly
+random.shuffle(train_imgs)
+random.shuffle(val_imgs)
+random.shuffle(test_imgs)
+
+gc.collect()  # collect garbage to save memory
+
+import matplotlib.image as mpimg
+
+# Lets declare our image dimensions
+# we are using coloured images.
+nrows = 224
+ncolumns = 224
+channels = 3  # change to 1 if you want to use grayscale image
+
+
+# A function to read and process the images to an acceptable format for our model
+def read_and_process_image(list_of_images):
+    """
+    Returns two arrays:
+        X is an array of resized images
+        y is an array of labels
+    """
+    X = []  # images
+    y = []  # labels
+    i = 0
+    for image in list_of_images:
+        # ändra här mellan COLOR och GRAYSCALE beroende på antal channels
+        X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (nrows, ncolumns),
+                            interpolation=cv2.INTER_CUBIC))  # Read the image
+        # get the labels
+        if 'persons_1' in image:
+            y.append(1)
+        elif 'dogs_1' in image:
+            y.append(2)
+        elif 'bikes_1' in image:
+            y.append(3)
+        else:
+            y.append(0)
+        i += 1
+    return X, y
+
+class_names = ['empty', 'person', 'dogs', 'bikes']
+
+X_train, y_train = read_and_process_image(train_imgs)
+X_val, y_val = read_and_process_image(val_imgs)
+X_test, y_test = read_and_process_image(test_imgs)
+
+# Lets view some of the pics
+plt.figure(figsize=(20, 10))
+columns = 4
+for i in range(columns):
+    plt.subplot(5 / columns + 1, columns, i + 1)
+    plt.imshow(X_train[i])
+
+import seaborn as sns
+
+gc.collect()
+
+# Convert list to numpy array
+X_train = np.array(X_train)
+y_train = np.array(y_train)
+X_val = np.array(X_val)
+y_val = np.array(y_val)
+X_test = np.array(X_test)
+y_test = np.array(y_test)
+
+# Lets split the data into train and test set
+print("Shape of train images is:", X_train.shape)
+print("Shape of train labels is:", y_train.shape)
+print("Shape of validation images is:", X_val.shape)
+print("Shape of validation labels is:", y_val.shape)
+print("Shape of test images is:", X_test.shape)
+print("Shape of test labels is:", y_test.shape)
+
+gc.collect()
+
+# get the length of the train and validation data
+ntrain = len(X_train)
+nval = len(X_val)
+
+# We will use a batch size of 32. Note: batch size should be a factor of 2.***4,8,16,32,64...***
+batch_size = 32
+
+my_callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)]
+reg_param = float(sys.argv[1])
+drop_param =float(sys.argv[2])
+run_number =float(sys.argv[3])
+
+model = models.Sequential()
+model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(ncolumns, nrows, 3),kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))  # input ska var (150, 150, 3)
+model.add(layers.Conv2D(32, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Dropout(drop_param))
+model.add(layers.Conv2D(64, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(64, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Dropout(drop_param))
+model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Dropout(drop_param))
+model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Flatten())
+model.add(layers.Dropout(drop_param))
+model.add(layers.Dense(512, activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Dense(4))  # Sigmoid function at the end because we have just two classes
+
+# Lets see our model
+model.summary()
+
+# We'll use binary_crossentropy loss because its a binary classification
+opt = tf.keras.optimizers.Adam(learning_rate=0.001)
+model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=opt, metrics=['accuracy'])
+
+# Lets create the augmentation configuration
+# This helps prevent overfitting, since we are using a small dataset
+train_datagen = ImageDataGenerator(rescale=1. / 255,  # Scale the image between 0 and 1
+                                   rotation_range=40,
+                                   width_shift_range=0.2,
+                                   height_shift_range=0.2,
+                                   shear_range=0.2,
+                                   zoom_range=0.2,
+                                   horizontal_flip=True, )
+
+val_datagen = ImageDataGenerator(rescale=1. / 255)  # We do not augment validation data. we only perform rescale
+
+# Override current setup of ImageDataGenerator
+train_datagen = ImageDataGenerator()
+val_datagen = ImageDataGenerator()
+
+# Create the image generators
+train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)
+val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)
+
+start_time = time.time()
+
+history = model.fit_generator(train_generator, steps_per_epoch=ntrain // batch_size, epochs=150,
+                              validation_data=val_generator, validation_steps=nval // batch_size,
+                              callbacks=my_callbacks)
+
+end_time = time.time()
+
+# Save the model
+os.chdir(save_dir)
+model.save_weights('model_EVOLVE_v3_224x224_weights_run_'+ str(run_number)+ '.h5')
+model.save('model_EVOLVE_v3_224x224_keras_run_'+ str(run_number)+ '.h5')
+
+os.chdir(work_dir)
+
+# lets plot the train and val curve
+# get the details form the history object
+acc = history.history['acc']
+acc = acc[1:]
+val_acc = history.history['val_acc']
+val_acc = val_acc[1:]
+loss = history.history['loss']
+loss = loss[1:]
+val_loss = history.history['val_loss']
+val_loss = val_loss[1:]
+
+epochs = range(1, len(acc) + 1)
+
+# Train and validation accuracy
+plt.plot(epochs, acc, 'b', label='Training accurarcy')
+plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
+plt.title('Training and Validation accurarcy')
+plt.legend()
+
+plt.figure()
+# Train and validation loss
+plt.plot(epochs, loss, 'b', label='Training loss')
+plt.plot(epochs, val_loss, 'r', label='Validation loss')
+plt.title('Training and Validation loss')
+plt.legend()
+
+#plt.show()
+
+# Probability of test set
+probability_model = tf.keras.Sequential([model,
+                                         tf.keras.layers.Softmax()])
+
+predictions = probability_model.predict(X_test)
+
+
+def plot_image(i, predictions_array, true_label, img):
+    predictions_array, true_label, img = predictions_array, true_label[i], img[i]
+    plt.grid(False)
+    plt.xticks([])
+    plt.yticks([])
+    #plt.imshow(img, cmap=plt.cm.binary)
+
+    predicted_label = np.argmax(predictions_array)
+    if predicted_label == true_label:
+        color = 'blue'
+    else:
+        color = 'red'
+
+    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
+                                         100 * np.max(predictions_array),
+                                         class_names[true_label]),
+               color=color)
+
+
+def plot_value_array(i, predictions_array, true_label):
+    predictions_array, true_label = predictions_array, true_label[i]
+    plt.grid(False)
+    plt.xticks(range(4))
+    plt.yticks([])
+    thisplot = plt.bar(range(4), predictions_array, color="#777777")
+    plt.ylim([0, 1])
+    predicted_label = np.argmax(predictions_array)
+
+    thisplot[predicted_label].set_color('red')
+    thisplot[true_label].set_color('blue')
+
+
+# Plot the first X test images, their predicted labels, and the true labels.
+# Color correct predictions in blue and incorrect predictions in red.
+num_rows = 5
+num_cols = 3
+num_images = num_rows * num_cols
+plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))
+for i in range(num_images):
+    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)
+    plot_image(i, predictions[i], y_test, X_test)
+    plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)
+    plot_value_array(i, predictions[i], y_test)
+plt.tight_layout()
+#plt.show()
+
+# Probability of test set
+print("Shape of labels is:", predictions.shape)
+threshold = 0.6
+total_correct = 0
+dogs_correct = 0
+bikes_correct = 0
+persons_correct = 0
+empty_correct = 0
+for i in range(len(y_test)):
+    index_max = np.argmax(predictions[i])
+    if y_test[i] == index_max and predictions[i][index_max] > threshold:
+        total_correct += 1
+        if index_max == 1:
+            persons_correct += 1
+        if index_max == 2:
+            dogs_correct += 1
+        if index_max == 3:
+            bikes_correct += 1
+        if index_max == 0:
+            empty_correct += 1
+total_acc = total_correct / len(X_test)
+person_acc = persons_correct / 177
+dog_acc = dogs_correct / 34
+bike_acc = bikes_correct / 35
+empty_acc = empty_correct / 199
+
+print("Total accuracy ", total_acc)
+print("Person accuracy ", person_acc)
+print("Dog accuracy ", dog_acc)
+print("Bike accuracy ", bike_acc)
+print("Empty accuracy ", empty_acc)
+
+#project_directory = '/Users/august/PycharmProjects/EITN35/'
+results = work_dir + 'training_results/'
+
+def round_sig(x, sig=2):
+    return round(x, sig - int(math.floor(math.log10(abs(x)))) - 1)
+
+
+def print_to_file(acc, val_acc, loss, val_loss, total_acc, person_acc, dog_acc, bike_acc, empty_acc, epochs, reg_param):
+    # New CSV file
+    os.chdir(results)
+    current_results = os.listdir(results)
+    current_results.sort()
+    working_file = current_results[len(os.listdir(results)) - 1]
+    print("Latest file is: " + str(working_file))
+
+
+    final_acc = acc[len(acc) - 1]
+    final_val_acc = val_acc[len(val_acc)-1]
+    final_loss = loss[len(loss) - 1]
+    final_val_loss = val_loss[len(val_loss) - 1]
+    var = final_acc - final_val_acc
+    var = round_sig(var, 4)
+
+    total_time = end_time - start_time
+    total_time = round(total_time)
+
+    file = open(working_file, "a+")
+    file.write("\nRun Result," + str(run_number))
+    file.write("\nTraining," + str(final_acc))
+    file.write("\nValidation," + str(final_val_acc))
+    file.write("\nVariance," + str(var))
+    file.write("\nTotal_acc," + str(total_acc))
+    file.write("\nPerson_acc," + str(person_acc))
+    file.write("\nDog_acc," + str(dog_acc))
+    file.write("\nBike_acc," + str(bike_acc))
+    file.write("\nEmpty_acc," + str(empty_acc))
+    file.write("\nEpochs," + str(len(epochs)+1))
+    file.write("\nL2_reg," + str(reg_param))
+    file.write("\nDrop_rate," + str(drop_param))
+    file.write("\nRun time," + str(total_time))
+    file.write("\n")
+    file.close()
+
+
+print_to_file(acc, val_acc, loss, val_loss, total_acc, person_acc, dog_acc, bike_acc, empty_acc, epochs, reg_param)
+
+
+
+
+
Index: ImageRecognition/EVOLVE_model_V2_224x224.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- ImageRecognition/EVOLVE_model_V2_224x224.py	(date 1591012032033)
+++ ImageRecognition/EVOLVE_model_V2_224x224.py	(date 1591012032033)
@@ -0,0 +1,361 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+import cv2
+import numpy as np
+import pandas as pd
+import sklearn
+import matplotlib.pyplot as plt
+
+import os
+import random
+import gc  # garbage collector for cleaning deleted data from memory
+
+from keras.models import Model
+from tensorflow.keras import models, layers, regularizers
+from keras.preprocessing.image import ImageDataGenerator
+from keras.preprocessing.image import img_to_array, load_img
+import tensorflow as tf
+from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout
+import sys
+import math
+import time
+import getopt
+
+
+print ('Nbr of arguments: ' , len(sys.argv) , 'total')
+print ('Arguments list: ' , str(sys.argv))
+
+train_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/image_frames/train_set/'
+val_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/image_frames/val_set/'
+test_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/image_frames/test_set/'
+save_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/models_and_weights_EVOLVE/models/saved_models_and_weights/'
+work_dir = 'C:/Users/eitn35/PycharmProjects/EITN35/'
+
+test_imgs = [test_dir + '{}'.format(i) for i in os.listdir(test_dir)]  # get test images
+val_imgs = [val_dir + '{}'.format(i) for i in os.listdir(val_dir)]  # get val images
+train_imgs = [train_dir + '{}'.format(i) for i in os.listdir(train_dir)]  # get test images
+
+# shuffle it randomly
+random.shuffle(train_imgs)
+random.shuffle(val_imgs)
+random.shuffle(test_imgs)
+
+gc.collect()  # collect garbage to save memory
+
+import matplotlib.image as mpimg
+
+# Lets declare our image dimensions
+# we are using coloured images.
+nrows = 224
+ncolumns = 224
+channels = 3  # change to 1 if you want to use grayscale image
+
+
+# A function to read and process the images to an acceptable format for our model
+def read_and_process_image(list_of_images):
+    """
+    Returns two arrays:
+        X is an array of resized images
+        y is an array of labels
+    """
+    X = []  # images
+    y = []  # labels
+    i = 0
+    for image in list_of_images:
+        # ändra här mellan COLOR och GRAYSCALE beroende på antal channels
+        X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (nrows, ncolumns),
+                            interpolation=cv2.INTER_CUBIC))  # Read the image
+        # get the labels
+        if 'persons_1' in image:
+            y.append(1)
+        elif 'dogs_1' in image:
+            y.append(2)
+        elif 'bikes_1' in image:
+            y.append(3)
+        else:
+            y.append(0)
+        i += 1
+    return X, y
+
+class_names = ['empty', 'person', 'dogs', 'bikes']
+
+X_train, y_train = read_and_process_image(train_imgs)
+X_val, y_val = read_and_process_image(val_imgs)
+X_test, y_test = read_and_process_image(test_imgs)
+
+# Lets view some of the pics
+plt.figure(figsize=(20, 10))
+columns = 4
+for i in range(columns):
+    plt.subplot(5 / columns + 1, columns, i + 1)
+    plt.imshow(X_train[i])
+
+import seaborn as sns
+
+gc.collect()
+
+# Convert list to numpy array
+X_train = np.array(X_train)
+y_train = np.array(y_train)
+X_val = np.array(X_val)
+y_val = np.array(y_val)
+X_test = np.array(X_test)
+y_test = np.array(y_test)
+
+# Lets split the data into train and test set
+print("Shape of train images is:", X_train.shape)
+print("Shape of train labels is:", y_train.shape)
+print("Shape of validation images is:", X_val.shape)
+print("Shape of validation labels is:", y_val.shape)
+print("Shape of test images is:", X_test.shape)
+print("Shape of test labels is:", y_test.shape)
+
+gc.collect()
+
+# get the length of the train and validation data
+ntrain = len(X_train)
+nval = len(X_val)
+
+# We will use a batch size of 32. Note: batch size should be a factor of 2.***4,8,16,32,64...***
+batch_size = 32
+
+my_callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)]
+reg_param = float(sys.argv[1])
+drop_param =float(sys.argv[2])
+run_number =float(sys.argv[3])
+
+model = models.Sequential()
+model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(ncolumns, nrows, 3),kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))  # input ska var (150, 150, 3)
+model.add(layers.Conv2D(64, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Dropout(drop_param))
+model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Dropout(drop_param))
+model.add(layers.Conv2D(256, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(256, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(256, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Dropout(drop_param))
+model.add(layers.Conv2D(512, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(512, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(512, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Dropout(drop_param))
+model.add(layers.Conv2D(512, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(512, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Conv2D(512, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Flatten())
+model.add(layers.Dropout(drop_param))
+model.add(layers.Dense(512, activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Dense(512, activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Dense(4))  # Sigmoid function at the end because we have just two classes
+
+# Lets see our model
+model.summary()
+
+# We'll use binary_crossentropy loss because its a binary classification
+opt = tf.keras.optimizers.Adam(learning_rate=0.001)
+model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=opt, metrics=['accuracy'])
+
+# Lets create the augmentation configuration
+# This helps prevent overfitting, since we are using a small dataset
+train_datagen = ImageDataGenerator(rescale=1. / 255,  # Scale the image between 0 and 1
+                                   rotation_range=40,
+                                   width_shift_range=0.2,
+                                   height_shift_range=0.2,
+                                   shear_range=0.2,
+                                   zoom_range=0.2,
+                                   horizontal_flip=True, )
+
+val_datagen = ImageDataGenerator(rescale=1. / 255)  # We do not augment validation data. we only perform rescale
+
+# Override current setup of ImageDataGenerator
+train_datagen = ImageDataGenerator()
+val_datagen = ImageDataGenerator()
+
+# Create the image generators
+train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)
+val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)
+
+start_time = time.time()
+
+history = model.fit_generator(train_generator, steps_per_epoch=ntrain // batch_size, epochs=150,
+                              validation_data=val_generator, validation_steps=nval // batch_size,
+                              callbacks=my_callbacks)
+
+end_time = time.time()
+
+# Save the model
+os.chdir(save_dir)
+model.save_weights('model_EVOLVE_v3_224x224_weights_run_'+ str(run_number)+ '.h5')
+model.save('model_EVOLVE_v3_224x224_keras_run_'+ str(run_number)+ '.h5')
+
+os.chdir(work_dir)
+
+# lets plot the train and val curve
+# get the details form the history object
+acc = history.history['acc']
+acc = acc[1:]
+val_acc = history.history['val_acc']
+val_acc = val_acc[1:]
+loss = history.history['loss']
+loss = loss[1:]
+val_loss = history.history['val_loss']
+val_loss = val_loss[1:]
+
+epochs = range(1, len(acc) + 1)
+
+# Train and validation accuracy
+plt.plot(epochs, acc, 'b', label='Training accurarcy')
+plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
+plt.title('Training and Validation accurarcy')
+plt.legend()
+
+plt.figure()
+# Train and validation loss
+plt.plot(epochs, loss, 'b', label='Training loss')
+plt.plot(epochs, val_loss, 'r', label='Validation loss')
+plt.title('Training and Validation loss')
+plt.legend()
+
+#plt.show()
+
+# Probability of test set
+probability_model = tf.keras.Sequential([model,
+                                         tf.keras.layers.Softmax()])
+
+predictions = probability_model.predict(X_test)
+
+
+def plot_image(i, predictions_array, true_label, img):
+    predictions_array, true_label, img = predictions_array, true_label[i], img[i]
+    plt.grid(False)
+    plt.xticks([])
+    plt.yticks([])
+    #plt.imshow(img, cmap=plt.cm.binary)
+
+    predicted_label = np.argmax(predictions_array)
+    if predicted_label == true_label:
+        color = 'blue'
+    else:
+        color = 'red'
+
+    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
+                                         100 * np.max(predictions_array),
+                                         class_names[true_label]),
+               color=color)
+
+
+def plot_value_array(i, predictions_array, true_label):
+    predictions_array, true_label = predictions_array, true_label[i]
+    plt.grid(False)
+    plt.xticks(range(4))
+    plt.yticks([])
+    thisplot = plt.bar(range(4), predictions_array, color="#777777")
+    plt.ylim([0, 1])
+    predicted_label = np.argmax(predictions_array)
+
+    thisplot[predicted_label].set_color('red')
+    thisplot[true_label].set_color('blue')
+
+
+# Plot the first X test images, their predicted labels, and the true labels.
+# Color correct predictions in blue and incorrect predictions in red.
+num_rows = 5
+num_cols = 3
+num_images = num_rows * num_cols
+plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))
+for i in range(num_images):
+    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)
+    plot_image(i, predictions[i], y_test, X_test)
+    plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)
+    plot_value_array(i, predictions[i], y_test)
+plt.tight_layout()
+#plt.show()
+
+# Probability of test set
+print("Shape of labels is:", predictions.shape)
+threshold = 0.6
+total_correct = 0
+dogs_correct = 0
+bikes_correct = 0
+persons_correct = 0
+empty_correct = 0
+for i in range(len(y_test)):
+    index_max = np.argmax(predictions[i])
+    if y_test[i] == index_max and predictions[i][index_max] > threshold:
+        total_correct += 1
+        if index_max == 1:
+            persons_correct += 1
+        if index_max == 2:
+            dogs_correct += 1
+        if index_max == 3:
+            bikes_correct += 1
+        if index_max == 0:
+            empty_correct += 1
+total_acc = total_correct / len(X_test)
+person_acc = persons_correct / 177
+dog_acc = dogs_correct / 34
+bike_acc = bikes_correct / 35
+empty_acc = empty_correct / 199
+
+print("Total accuracy ", total_acc)
+print("Person accuracy ", person_acc)
+print("Dog accuracy ", dog_acc)
+print("Bike accuracy ", bike_acc)
+print("Empty accuracy ", empty_acc)
+
+#project_directory = '/Users/august/PycharmProjects/EITN35/'
+results = work_dir + 'training_results/'
+
+def round_sig(x, sig=2):
+    return round(x, sig - int(math.floor(math.log10(abs(x)))) - 1)
+
+
+def print_to_file(acc, val_acc, loss, val_loss, total_acc, person_acc, dog_acc, bike_acc, empty_acc, epochs, reg_param):
+    # New CSV file
+    os.chdir(results)
+    current_results = os.listdir(results)
+    current_results.sort()
+    working_file = current_results[len(os.listdir(results)) - 1]
+    print("Latest file is: " + str(working_file))
+
+
+    final_acc = acc[len(acc) - 1]
+    final_val_acc = val_acc[len(val_acc)-1]
+    final_loss = loss[len(loss) - 1]
+    final_val_loss = val_loss[len(val_loss) - 1]
+    var = final_acc - final_val_acc
+    var = round_sig(var, 4)
+
+    total_time = end_time - start_time
+    total_time = round(total_time)
+
+    file = open(working_file, "a+")
+    file.write("\nRun Result," + str(run_number))
+    file.write("\nTraining," + str(final_acc))
+    file.write("\nValidation," + str(final_val_acc))
+    file.write("\nVariance," + str(var))
+    file.write("\nTotal_acc," + str(total_acc))
+    file.write("\nPerson_acc," + str(person_acc))
+    file.write("\nDog_acc," + str(dog_acc))
+    file.write("\nBike_acc," + str(bike_acc))
+    file.write("\nEmpty_acc," + str(empty_acc))
+    file.write("\nEpochs," + str(len(epochs)+1))
+    file.write("\nL2_reg," + str(reg_param))
+    file.write("\nDrop_rate," + str(drop_param))
+    file.write("\nRun time," + str(total_time))
+    file.write("\n")
+    file.close()
+
+
+print_to_file(acc, val_acc, loss, val_loss, total_acc, person_acc, dog_acc, bike_acc, empty_acc, epochs, reg_param)
+
+
+
+
+
Index: ImageRecognition/Baseline_model_V3_224x224.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- ImageRecognition/Baseline_model_V3_224x224.py	(date 1591012032029)
+++ ImageRecognition/Baseline_model_V3_224x224.py	(date 1591012032029)
@@ -0,0 +1,349 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+import cv2
+import numpy as np
+import pandas as pd
+import sklearn
+import matplotlib.pyplot as plt
+
+import os
+import random
+import gc  # garbage collector for cleaning deleted data from memory
+
+from keras.models import Model
+from tensorflow.keras import models, layers, regularizers
+from keras.preprocessing.image import ImageDataGenerator
+from keras.preprocessing.image import img_to_array, load_img
+import tensorflow as tf
+from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout
+import sys
+import math
+import time
+import getopt
+
+
+print ('Nbr of arguments: ' , len(sys.argv) , 'total')
+print ('Arguments list: ' , str(sys.argv))
+
+train_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/image_frames/train_set/'
+val_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/image_frames/val_set/'
+test_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/image_frames/test_set/'
+save_dir = 'C:/Users/eitn35/Documents/EITN35_EVOLVE/models_and_weights_EVOLVE/models/saved_models_and_weights/'
+work_dir = 'C:/Users/eitn35/PycharmProjects/EITN35/'
+
+test_imgs = [test_dir + '{}'.format(i) for i in os.listdir(test_dir)]  # get test images
+val_imgs = [val_dir + '{}'.format(i) for i in os.listdir(val_dir)]  # get val images
+train_imgs = [train_dir + '{}'.format(i) for i in os.listdir(train_dir)]  # get test images
+
+# shuffle it randomly
+random.shuffle(train_imgs)
+random.shuffle(val_imgs)
+random.shuffle(test_imgs)
+
+gc.collect()  # collect garbage to save memory
+
+import matplotlib.image as mpimg
+
+# Lets declare our image dimensions
+# we are using coloured images.
+nrows = 224
+ncolumns = 224
+channels = 3  # change to 1 if you want to use grayscale image
+
+
+# A function to read and process the images to an acceptable format for our model
+def read_and_process_image(list_of_images):
+    """
+    Returns two arrays:
+        X is an array of resized images
+        y is an array of labels
+    """
+    X = []  # images
+    y = []  # labels
+    i = 0
+    for image in list_of_images:
+        # ändra här mellan COLOR och GRAYSCALE beroende på antal channels
+        X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (nrows, ncolumns),
+                            interpolation=cv2.INTER_CUBIC))  # Read the image
+        # get the labels
+        if 'persons_1' in image:
+            y.append(1)
+        elif 'dogs_1' in image:
+            y.append(2)
+        elif 'bikes_1' in image:
+            y.append(3)
+        else:
+            y.append(0)
+        i += 1
+    return X, y
+
+class_names = ['empty', 'person', 'dogs', 'bikes']
+
+X_train, y_train = read_and_process_image(train_imgs)
+X_val, y_val = read_and_process_image(val_imgs)
+X_test, y_test = read_and_process_image(test_imgs)
+
+# Lets view some of the pics
+plt.figure(figsize=(20, 10))
+columns = 4
+for i in range(columns):
+    plt.subplot(5 / columns + 1, columns, i + 1)
+    plt.imshow(X_train[i])
+
+import seaborn as sns
+
+gc.collect()
+
+# Convert list to numpy array
+X_train = np.array(X_train)
+y_train = np.array(y_train)
+X_val = np.array(X_val)
+y_val = np.array(y_val)
+X_test = np.array(X_test)
+y_test = np.array(y_test)
+
+# Lets split the data into train and test set
+print("Shape of train images is:", X_train.shape)
+print("Shape of train labels is:", y_train.shape)
+print("Shape of validation images is:", X_val.shape)
+print("Shape of validation labels is:", y_val.shape)
+print("Shape of test images is:", X_test.shape)
+print("Shape of test labels is:", y_test.shape)
+
+gc.collect()
+
+# get the length of the train and validation data
+ntrain = len(X_train)
+nval = len(X_val)
+
+# We will use a batch size of 32. Note: batch size should be a factor of 2.***4,8,16,32,64...***
+batch_size = 32
+
+my_callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)]
+reg_param = float(sys.argv[1])
+drop_param =float(sys.argv[2])
+run_number =float(sys.argv[3])
+
+model = models.Sequential()
+model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(ncolumns, nrows, 3),kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))  # input ska var (150, 150, 3)
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Dropout(drop_param))
+model.add(layers.Conv2D(64, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Dropout(drop_param))
+model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Dropout(drop_param))
+model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Flatten())
+model.add(layers.Dropout(drop_param))
+model.add(layers.Dense(512, activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=reg_param)))
+model.add(layers.Dense(4))  # Sigmoid function at the end because we have just two classes
+
+# Lets see our model
+model.summary()
+
+# We'll use binary_crossentropy loss because its a binary classification
+opt = tf.keras.optimizers.Adam(learning_rate=0.001)
+model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=opt, metrics=['accuracy'])
+
+# Lets create the augmentation configuration
+# This helps prevent overfitting, since we are using a small dataset
+train_datagen = ImageDataGenerator(rescale=1. / 255,  # Scale the image between 0 and 1
+                                   rotation_range=40,
+                                   width_shift_range=0.2,
+                                   height_shift_range=0.2,
+                                   shear_range=0.2,
+                                   zoom_range=0.2,
+                                   horizontal_flip=True, )
+
+val_datagen = ImageDataGenerator(rescale=1. / 255)  # We do not augment validation data. we only perform rescale
+
+# Override current setup of ImageDataGenerator
+train_datagen = ImageDataGenerator()
+val_datagen = ImageDataGenerator()
+
+# Create the image generators
+train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)
+val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)
+
+start_time = time.time()
+
+history = model.fit_generator(train_generator, steps_per_epoch=ntrain // batch_size, epochs=150,
+                              validation_data=val_generator, validation_steps=nval // batch_size,
+                              callbacks=my_callbacks)
+
+end_time = time.time()
+
+# Save the model
+os.chdir(save_dir)
+model.save_weights('model_EVOLVE_v3_224x224_weights_run_'+ str(run_number)+ '.h5')
+model.save('model_EVOLVE_v3_224x224_keras_run_'+ str(run_number)+ '.h5')
+
+os.chdir(work_dir)
+
+# lets plot the train and val curve
+# get the details form the history object
+acc = history.history['acc']
+acc = acc[1:]
+val_acc = history.history['val_acc']
+val_acc = val_acc[1:]
+loss = history.history['loss']
+loss = loss[1:]
+val_loss = history.history['val_loss']
+val_loss = val_loss[1:]
+
+epochs = range(1, len(acc) + 1)
+
+# Train and validation accuracy
+plt.plot(epochs, acc, 'b', label='Training accurarcy')
+plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
+plt.title('Training and Validation accurarcy')
+plt.legend()
+
+plt.figure()
+# Train and validation loss
+plt.plot(epochs, loss, 'b', label='Training loss')
+plt.plot(epochs, val_loss, 'r', label='Validation loss')
+plt.title('Training and Validation loss')
+plt.legend()
+
+#plt.show()
+
+# Probability of test set
+probability_model = tf.keras.Sequential([model,
+                                         tf.keras.layers.Softmax()])
+
+predictions = probability_model.predict(X_test)
+
+
+def plot_image(i, predictions_array, true_label, img):
+    predictions_array, true_label, img = predictions_array, true_label[i], img[i]
+    plt.grid(False)
+    plt.xticks([])
+    plt.yticks([])
+    #plt.imshow(img, cmap=plt.cm.binary)
+
+    predicted_label = np.argmax(predictions_array)
+    if predicted_label == true_label:
+        color = 'blue'
+    else:
+        color = 'red'
+
+    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
+                                         100 * np.max(predictions_array),
+                                         class_names[true_label]),
+               color=color)
+
+
+def plot_value_array(i, predictions_array, true_label):
+    predictions_array, true_label = predictions_array, true_label[i]
+    plt.grid(False)
+    plt.xticks(range(4))
+    plt.yticks([])
+    thisplot = plt.bar(range(4), predictions_array, color="#777777")
+    plt.ylim([0, 1])
+    predicted_label = np.argmax(predictions_array)
+
+    thisplot[predicted_label].set_color('red')
+    thisplot[true_label].set_color('blue')
+
+
+# Plot the first X test images, their predicted labels, and the true labels.
+# Color correct predictions in blue and incorrect predictions in red.
+num_rows = 5
+num_cols = 3
+num_images = num_rows * num_cols
+plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))
+for i in range(num_images):
+    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)
+    plot_image(i, predictions[i], y_test, X_test)
+    plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)
+    plot_value_array(i, predictions[i], y_test)
+plt.tight_layout()
+#plt.show()
+
+# Probability of test set
+print("Shape of labels is:", predictions.shape)
+threshold = 0.6
+total_correct = 0
+dogs_correct = 0
+bikes_correct = 0
+persons_correct = 0
+empty_correct = 0
+for i in range(len(y_test)):
+    index_max = np.argmax(predictions[i])
+    if y_test[i] == index_max and predictions[i][index_max] > threshold:
+        total_correct += 1
+        if index_max == 1:
+            persons_correct += 1
+        if index_max == 2:
+            dogs_correct += 1
+        if index_max == 3:
+            bikes_correct += 1
+        if index_max == 0:
+            empty_correct += 1
+total_acc = total_correct / len(X_test)
+person_acc = persons_correct / 177
+dog_acc = dogs_correct / 34
+bike_acc = bikes_correct / 35
+empty_acc = empty_correct / 199
+
+print("Total accuracy ", total_acc)
+print("Person accuracy ", person_acc)
+print("Dog accuracy ", dog_acc)
+print("Bike accuracy ", bike_acc)
+print("Empty accuracy ", empty_acc)
+
+#project_directory = '/Users/august/PycharmProjects/EITN35/'
+results = work_dir + 'training_results/'
+
+def round_sig(x, sig=2):
+    return round(x, sig - int(math.floor(math.log10(abs(x)))) - 1)
+
+
+def print_to_file(acc, val_acc, loss, val_loss, total_acc, person_acc, dog_acc, bike_acc, empty_acc, epochs, reg_param):
+    # New CSV file
+    os.chdir(results)
+    current_results = os.listdir(results)
+    current_results.sort()
+    working_file = current_results[len(os.listdir(results)) - 1]
+    print("Latest file is: " + str(working_file))
+
+
+    final_acc = acc[len(acc) - 1]
+    final_val_acc = val_acc[len(val_acc)-1]
+    final_loss = loss[len(loss) - 1]
+    final_val_loss = val_loss[len(val_loss) - 1]
+    var = final_acc - final_val_acc
+    var = round_sig(var, 4)
+
+    total_time = end_time - start_time
+    total_time = round(total_time)
+
+    file = open(working_file, "a+")
+    file.write("\nRun Result," + str(run_number))
+    file.write("\nTraining," + str(final_acc))
+    file.write("\nValidation," + str(final_val_acc))
+    file.write("\nVariance," + str(var))
+    file.write("\nTotal_acc," + str(total_acc))
+    file.write("\nPerson_acc," + str(person_acc))
+    file.write("\nDog_acc," + str(dog_acc))
+    file.write("\nBike_acc," + str(bike_acc))
+    file.write("\nEmpty_acc," + str(empty_acc))
+    file.write("\nEpochs," + str(len(epochs)+1))
+    file.write("\nL2_reg," + str(reg_param))
+    file.write("\nDrop_rate," + str(drop_param))
+    file.write("\nRun time," + str(total_time))
+    file.write("\n")
+    file.close()
+
+
+print_to_file(acc, val_acc, loss, val_loss, total_acc, person_acc, dog_acc, bike_acc, empty_acc, epochs, reg_param)
+
+
+
+
+
Index: requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- requirements.txt	(date 1591012032043)
+++ requirements.txt	(date 1591012032043)
@@ -0,0 +1,129 @@
+# This file may be used to create an environment using:
+# $ conda create --name <env> --file <this file>
+# platform: win-64
+_tflow_select=2.1.0=gpu
+absl-py=0.9.0=py37_0
+astor=0.8.0=py37_0
+attrs=19.3.0=py_0
+backcall=0.1.0=py37_0
+blas=1.0=mkl
+bleach=3.1.4=py_0
+ca-certificates=2020.1.1=0
+certifi=2020.4.5.1=py37_0
+colorama=0.4.3=py_0
+cudatoolkit=10.0.130=0
+cudnn=7.6.5=cuda10.0_0
+cycler=0.10.0=py37_0
+decorator=4.4.2=py_0
+defusedxml=0.6.0=py_0
+entrypoints=0.3=py37_0
+freetype=2.9.1=ha9979f8_1
+gast=0.3.3=py_0
+grpcio=1.27.2=py37h351948d_0
+h5py=2.8.0=py37hf7173ca_2
+hdf5=1.8.20=hac2f561_1
+icc_rt=2019.0.0=h0cc432a_1
+icu=58.2=ha925a31_3
+importlib_metadata=1.5.0=py37_0
+intel-openmp=2020.0=166
+ipykernel=5.1.4=py37h39e3cac_0
+ipython=7.13.0=py37h5ca1d4c_0
+ipython_genutils=0.2.0=py37_0
+jedi=0.17.0=py37_0
+jinja2=2.11.2=py_0
+joblib=0.14.1=py_0
+jpeg=9b=hb83a4c4_2
+jsonschema=3.2.0=py37_0
+jupyter_client=6.1.3=py_0
+jupyter_core=4.6.3=py37_0
+keras=2.3.1=0
+keras-applications=1.0.8=py_0
+keras-base=2.3.1=py37_0
+keras-preprocessing=1.1.0=py_1
+kiwisolver=1.2.0=py37h74a9793_0
+libopencv=3.4.2=h20b85fd_0
+libpng=1.6.37=h2a8f88b_0
+libprotobuf=3.11.4=h7bd577a_0
+libsodium=1.0.16=h9d3ae62_0
+libtiff=4.1.0=h56a325e_0
+m2w64-gcc-libgfortran=5.3.0=6
+m2w64-gcc-libs=5.3.0=7
+m2w64-gcc-libs-core=5.3.0=7
+m2w64-gmp=6.1.0=2
+m2w64-libwinpthread-git=5.0.0.4634.697f757=2
+markdown=3.1.1=py37_0
+markupsafe=1.1.1=py37he774522_0
+matplotlib=3.1.3=py37_0
+matplotlib-base=3.1.3=py37h64f37c6_0
+mistune=0.8.4=py37he774522_0
+mkl=2020.0=166
+mkl-service=2.3.0=py37hb782905_0
+mkl_fft=1.0.15=py37h14836fe_0
+mkl_random=1.1.0=py37h675688f_0
+msys2-conda-epoch=20160418=1
+nbconvert=5.6.1=py37_0
+nbformat=5.0.4=py_0
+notebook=6.0.3=py37_0
+numpy=1.15.4=py37h19fb1c0_0
+numpy-base=1.15.4=py37hc3f5095_0
+olefile=0.46=py37_0
+opencv=3.4.2=py37h40b0b35_0
+openssl=1.1.1g=he774522_0
+pandas=1.0.3=py37h47e9c7a_0
+pandoc=2.2.3.2=0
+pandocfilters=1.4.2=py37_1
+parso=0.7.0=py_0
+pickleshare=0.7.5=py37_0
+pillow=7.1.2=py37hcc1f983_0
+pip=20.0.2=py37_1
+prometheus_client=0.7.1=py_0
+prompt-toolkit=3.0.4=py_0
+prompt_toolkit=3.0.4=0
+protobuf=3.11.4=py37h33f27b4_0
+py-opencv=3.4.2=py37hc319ecb_0
+pygments=2.6.1=py_0
+pyparsing=2.4.7=py_0
+pyqt=5.9.2=py37h6538335_2
+pyrsistent=0.16.0=py37he774522_0
+python=3.7.7=h60c2a47_2
+python-dateutil=2.8.1=py_0
+pytz=2020.1=py_0
+pywin32=227=py37he774522_1
+pywinpty=0.5.7=py37_0
+pyyaml=5.3.1=py37he774522_0
+pyzmq=18.1.1=py37ha925a31_0
+qt=5.9.7=vc14h73c81de_0
+scikit-learn=0.22.1=py37h6288b17_0
+scipy=1.4.1=py37h9439919_0
+seaborn=0.10.1=py_0
+send2trash=1.5.0=py37_0
+setuptools=46.1.3=py37_0
+sip=4.19.8=py37h6538335_0
+six=1.14.0=py37_0
+sqlite=3.31.1=h2a8f88b_1
+tensorboard=1.14.0=py37he3c9ec2_0
+tensorflow=1.14.0=gpu_py37h5512b17_0
+tensorflow-base=1.14.0=gpu_py37h55fc52a_0
+tensorflow-estimator=1.14.0=py_0
+tensorflow-gpu=1.14.0=h0d30ee6_0
+termcolor=1.1.0=py37_1
+terminado=0.8.3=py37_0
+testpath=0.4.4=py_0
+tk=8.6.8=hfa6e2cd_0
+tornado=6.0.4=py37he774522_1
+traitlets=4.3.3=py37_0
+vc=14.1=h0510ff6_4
+vs2015_runtime=14.16.27012=hf0eaf9b_1
+wcwidth=0.1.9=py_0
+webencodings=0.5.1=py37_1
+werkzeug=1.0.1=py_0
+wheel=0.34.2=py37_0
+wincertstore=0.2=py37_0
+winpty=0.4.3=4
+wrapt=1.12.1=py37he774522_1
+xz=5.2.5=h62dcd97_0
+yaml=0.1.7=hc54c509_2
+zeromq=4.3.1=h33f27b4_3
+zipp=3.1.0=py_0
+zlib=1.2.11=h62dcd97_4
+zstd=1.3.7=h508b16e_0
Index: training_results/EVOLVE_Baseline_model_1.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- training_results/EVOLVE_Baseline_model_1.txt	(date 1591012032017)
+++ training_results/EVOLVE_Baseline_model_1.txt	(date 1591012032017)
@@ -0,0 +1,66 @@
+EVOLVE_Baseline_model_1
+
+Run Result,1.0
+Training,0.91018385
+Validation,0.87215906
+Variance,0.03802
+Total_acc,0.7595505617977528
+Person_acc,0.7457627118644068
+Dog_acc,0.058823529411764705
+Bike_acc,0.22857142857142856
+Empty_acc,0.9849246231155779
+Epochs,113
+L2_reg,0.1
+Drop_rate,0.0
+
+Run Result,2.0
+Training,0.9653465
+Validation,0.8948864
+Variance,0.07046
+Total_acc,0.8764044943820225
+Person_acc,0.903954802259887
+Dog_acc,0.4411764705882353
+Bike_acc,0.6285714285714286
+Empty_acc,0.9698492462311558
+Epochs,150
+L2_reg,0.05
+Drop_rate,0.0
+
+Run Result,3.0
+Training,0.97171146
+Validation,0.86079544
+Variance,0.1109
+Total_acc,0.8404494382022472
+Person_acc,0.8192090395480226
+Dog_acc,0.38235294117647056
+Bike_acc,0.5428571428571428
+Empty_acc,0.9899497487437185
+Epochs,143
+L2_reg,0.01
+Drop_rate,0.0
+
+Run Result,4.0
+Training,1.0
+Validation,0.90625
+Variance,0.09375
+Total_acc,0.8898876404494382
+Person_acc,0.8926553672316384
+Dog_acc,0.4411764705882353
+Bike_acc,0.7714285714285715
+Empty_acc,0.9849246231155779
+Epochs,112
+L2_reg,0.005
+Drop_rate,0.0
+
+Run Result,5.0
+Training,1.0
+Validation,0.90625
+Variance,0.09375
+Total_acc,0.8853932584269663
+Person_acc,0.8813559322033898
+Dog_acc,0.47058823529411764
+Bike_acc,0.7428571428571429
+Empty_acc,0.9849246231155779
+Epochs,61
+L2_reg,0.001
+Drop_rate,0.0
Index: training_results/EVOLVE_Baseline_model_2.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- training_results/EVOLVE_Baseline_model_2.txt	(date 1591012032014)
+++ training_results/EVOLVE_Baseline_model_2.txt	(date 1591012032014)
@@ -0,0 +1,66 @@
+EVOLVE_Baseline_model_2
+
+Run Result,6.0
+Training,0.44908062
+Validation,0.44886363
+Variance,0.000217
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,44
+L2_reg,0.0
+Drop_rate,0.5
+
+Run Result,7.0
+Training,0.43352193
+Validation,0.44602272
+Variance,-0.0125
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,43
+L2_reg,0.0
+Drop_rate,0.4
+
+Run Result,8.0
+Training,0.44483733
+Validation,0.44886363
+Variance,-0.004026
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,27
+L2_reg,0.0
+Drop_rate,0.3
+
+Run Result,9.0
+Training,0.92939484
+Validation,0.8636364
+Variance,0.06576
+Total_acc,0.8314606741573034
+Person_acc,0.864406779661017
+Dog_acc,0.08823529411764706
+Bike_acc,0.42857142857142855
+Empty_acc,1.0
+Epochs,61
+L2_reg,0.0
+Drop_rate,0.2
+
+Run Result,10.0
+Training,0.9328147
+Validation,0.8693182
+Variance,0.0635
+Total_acc,0.8269662921348314
+Person_acc,0.7966101694915254
+Dog_acc,0.47058823529411764
+Bike_acc,0.6
+Empty_acc,0.9547738693467337
+Epochs,51
+L2_reg,0.0
+Drop_rate,0.1
Index: training_results/EVOLVE_Baseline_model_3.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- training_results/EVOLVE_Baseline_model_3.txt	(date 1591012032011)
+++ training_results/EVOLVE_Baseline_model_3.txt	(date 1591012032011)
@@ -0,0 +1,141 @@
+EVOLVE_Baseline_model_3
+
+Run Result,1.0
+Training,0.4413013
+Validation,0.45454547
+Variance,-0.01324
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,150
+L2_reg,0.1
+Drop_rate,0.0
+Run time,282
+
+Run Result,2.0
+Training,0.43876082
+Validation,0.44886363
+Variance,-0.0101
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,150
+L2_reg,0.05
+Drop_rate,0.0
+Run time,284
+
+Run Result,3.0
+Training,0.98302686
+Validation,0.89772725
+Variance,0.0853
+Total_acc,0.8651685393258427
+Person_acc,0.8305084745762712
+Dog_acc,0.6176470588235294
+Bike_acc,0.5714285714285714
+Empty_acc,0.9899497487437185
+Epochs,98
+L2_reg,0.01
+Drop_rate,0.0
+Run time,185
+
+Run Result,4.0
+Training,0.98585576
+Validation,0.87784094
+Variance,0.108
+Total_acc,0.8853932584269663
+Person_acc,0.8926553672316384
+Dog_acc,0.47058823529411764
+Bike_acc,0.7428571428571429
+Empty_acc,0.9748743718592965
+Epochs,131
+L2_reg,0.005
+Drop_rate,0.0
+Run time,246
+
+Run Result,5.0
+Training,0.9893918
+Validation,0.8948864
+Variance,0.09451
+Total_acc,0.8741573033707866
+Person_acc,0.8587570621468926
+Dog_acc,0.4411764705882353
+Bike_acc,0.6857142857142857
+Empty_acc,0.9949748743718593
+Epochs,90
+L2_reg,0.001
+Drop_rate,0.0
+Run time,171
+
+Run Result,6.0
+Training,0.4533239
+Validation,0.45170453
+Variance,0.001619
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,33
+L2_reg,0.0
+Drop_rate,0.5
+Run time,70
+
+Run Result,7.0
+Training,0.45120227
+Validation,0.4431818
+Variance,0.00802
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,34
+L2_reg,0.0
+Drop_rate,0.4
+Run time,72
+
+Run Result,8.0
+Training,0.44483733
+Validation,0.44602272
+Variance,-0.001185
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,46
+L2_reg,0.0
+Drop_rate,0.3
+Run time,94
+
+Run Result,9.0
+Training,0.47736916
+Validation,0.44886363
+Variance,0.02851
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,31
+L2_reg,0.0
+Drop_rate,0.2
+Run time,65
+
+Run Result,10.0
+Training,0.44791666
+Validation,0.44602272
+Variance,0.001894
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,51
+L2_reg,0.0
+Drop_rate,0.1
+Run time,104
Index: training_results/EVOLVE_Baseline_model_4.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- training_results/EVOLVE_Baseline_model_4.txt	(date 1591012032008)
+++ training_results/EVOLVE_Baseline_model_4.txt	(date 1591012032008)
@@ -0,0 +1,141 @@
+EVOLVE_Baseline_model_4
+
+Run Result,1.0
+Training,0.8606789
+Validation,0.86079544
+Variance,-0.0001165
+Total_acc,0.7595505617977528
+Person_acc,0.7570621468926554
+Dog_acc,0.029411764705882353
+Bike_acc,0.17142857142857143
+Empty_acc,0.9899497487437185
+Epochs,150
+L2_reg,0.1
+Drop_rate,0.0
+Run time,283
+
+Run Result,2.0
+Training,0.9646393
+Validation,0.8948864
+Variance,0.06975
+Total_acc,0.8786516853932584
+Person_acc,0.8700564971751412
+Dog_acc,0.5
+Bike_acc,0.6571428571428571
+Empty_acc,0.9899497487437185
+Epochs,150
+L2_reg,0.05
+Drop_rate,0.0
+Run time,282
+
+Run Result,3.0
+Training,0.9879774
+Validation,0.89204544
+Variance,0.09593
+Total_acc,0.8651685393258427
+Person_acc,0.8531073446327684
+Dog_acc,0.47058823529411764
+Bike_acc,0.6857142857142857
+Empty_acc,0.9748743718592965
+Epochs,150
+L2_reg,0.01
+Drop_rate,0.0
+Run time,280
+
+Run Result,4.0
+Training,0.99027777
+Validation,0.89772725
+Variance,0.09255
+Total_acc,0.8719101123595505
+Person_acc,0.8700564971751412
+Dog_acc,0.3235294117647059
+Bike_acc,0.7428571428571429
+Empty_acc,0.9899497487437185
+Epochs,150
+L2_reg,0.005
+Drop_rate,0.0
+Run time,283
+
+Run Result,5.0
+Training,0.99434227
+Validation,0.8948864
+Variance,0.09946
+Total_acc,0.8808988764044944
+Person_acc,0.8587570621468926
+Dog_acc,0.47058823529411764
+Bike_acc,0.7714285714285715
+Empty_acc,0.9899497487437185
+Epochs,83
+L2_reg,0.001
+Drop_rate,0.0
+Run time,160
+
+Run Result,6.0
+Training,0.44413012
+Validation,0.44886363
+Variance,-0.004734
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,29
+L2_reg,0.0
+Drop_rate,0.5
+Run time,60
+
+Run Result,7.0
+Training,0.446959
+Validation,0.44886363
+Variance,-0.001905
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,41
+L2_reg,0.0
+Drop_rate,0.4
+Run time,85
+
+Run Result,8.0
+Training,0.927157
+Validation,0.88920456
+Variance,0.03795
+Total_acc,0.8314606741573034
+Person_acc,0.7796610169491526
+Dog_acc,0.4117647058823529
+Bike_acc,0.5714285714285714
+Empty_acc,0.9949748743718593
+Epochs,100
+L2_reg,0.0
+Drop_rate,0.3
+Run time,201
+
+Run Result,9.0
+Training,0.44668588
+Validation,0.44602272
+Variance,0.0006632
+Total_acc,0.0
+Person_acc,0.0
+Dog_acc,0.0
+Bike_acc,0.0
+Empty_acc,0.0
+Epochs,27
+L2_reg,0.0
+Drop_rate,0.2
+Run time,57
+
+Run Result,10.0
+Training,0.91230553
+Validation,0.86647725
+Variance,0.04583
+Total_acc,0.8247191011235955
+Person_acc,0.8870056497175142
+Dog_acc,0.17647058823529413
+Bike_acc,0.14285714285714285
+Empty_acc,1.0
+Epochs,43
+L2_reg,0.0
+Drop_rate,0.1
+Run time,89
Index: training_results/EVOLVE_Baseline_model_5.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- training_results/EVOLVE_Baseline_model_5.txt	(date 1591012032005)
+++ training_results/EVOLVE_Baseline_model_5.txt	(date 1591012032005)
@@ -0,0 +1,57 @@
+EVOLVE_Baseline_model_5
+
+Run Result,1.0
+Training,0.82956153
+Validation,0.7073864
+Variance,0.1222
+Total_acc,0.6202247191011236
+Person_acc,0.8361581920903954
+Dog_acc,0.0
+Bike_acc,0.02857142857142857
+Empty_acc,0.6381909547738693
+Epochs,150
+L2_reg,0.0
+Drop_rate,0.0
+Run time,2515
+
+Run Result,2.0
+Training,0.79844415
+Validation,0.8068182
+Variance,-0.008374
+Total_acc,0.18202247191011237
+Person_acc,0.2598870056497175
+Dog_acc,0.0
+Bike_acc,1.0
+Empty_acc,0.0
+Epochs,90
+L2_reg,0.0
+Drop_rate,0.0
+Run time,1490
+
+Run Result,3.0
+Training,0.811174
+Validation,0.85227275
+Variance,-0.0411
+Total_acc,0.14831460674157304
+Person_acc,0.20903954802259886
+Dog_acc,0.29411764705882354
+Bike_acc,0.5428571428571428
+Empty_acc,0.0
+Epochs,121
+L2_reg,0.0
+Drop_rate,0.0
+Run time,1734
+
+Run Result,4.0
+Training,0.80905235
+Validation,0.8068182
+Variance,0.002234
+Total_acc,0.350561797752809
+Person_acc,0.7966101694915254
+Dog_acc,0.2647058823529412
+Bike_acc,0.17142857142857143
+Empty_acc,0.0
+Epochs,114
+L2_reg,0.0
+Drop_rate,0.0
+Run time,1686
Index: mp4_conversion.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- mp4_conversion.py	(date 1591005416405)
+++ mp4_conversion.py	(date 1591005416405)
@@ -0,0 +1,30 @@
+import os
+import ffmpy
+
+PRINT_DEBUG = True
+
+#Dir where video files to be converted are located
+directory = 'C:/Users/eitn35/Documents/EITN35/video_files/'
+os.chdir(directory)
+
+#Create "converted" folder if it does not exist
+try:
+    if not os.path.exists('converted'):
+        os.makedirs('converted')
+except OSError:
+    print ('Error: Creating directory of data')
+
+#Iterate over video files and convert, then move to converted folder
+for filename in os.listdir(directory):
+    if (filename.endswith(".asf") | filename.endswith(".avi")):
+        if PRINT_DEBUG: print("Starting conversion of " + filename + "...")
+        ff = ffmpy.FFmpeg(
+            inputs={filename: None},
+            outputs={"converted_" + filename.split('.')[0]+'.mp4': None}
+        )
+        ff.run()
+        if PRINT_DEBUG: print("Finished conversion of " + filename + ".")
+        os.rename(
+            directory + "/"+ "converted_" + filename.split(".")[0]+".mp4",
+            directory + "/converted/" + "converted_" +filename.split(".")[0]+".mp4")
+        if PRINT_DEBUG: print("File " + filename + " moved to converted folder.")
\ No newline at end of file
Index: run_all_scripts.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- run_all_scripts.py	(date 1591005416421)
+++ run_all_scripts.py	(date 1591005416421)
@@ -0,0 +1,8 @@
+import os
+directory_1 = '/Users/august/PycharmProjects/EITN35'
+
+os.chdir(directory_1)
+os.system("python mp4_conversion.py")
+os.system("python video_splitting.py")
+os.system("python test_set_creator.py")
+os.system("python object_detection_v5.py")
\ No newline at end of file
Index: video_splitting.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- video_splitting.py	(date 1591005416421)
+++ video_splitting.py	(date 1591005416421)
@@ -0,0 +1,63 @@
+import cv2
+import numpy as np
+import os
+
+# Dir where video files to be split are located
+#directory_1 = '../../Documents/EITN35/video_files/converted'
+directory_1 = 'C:/Users/eitn35/Documents/EITN35/video_files/converted'
+# Dir where frames should be saved
+#directory_2 = '../../Documents/EITN35/video_files'
+
+directory_2 = 'C:/Users/eitn35/Documents/EITN35/video_files'
+os.chdir(directory_2)
+
+# Display time stamps of saved frames and progress
+PRINT_DEBUG = True
+
+# Create "frames" folder if it does not exist
+try:
+    if not os.path.exists('frames'):
+        os.makedirs('frames')
+except OSError:
+    print('Error: Creating directory of data')
+
+
+# Splitting one video file. Input: video fiel, Output: frames
+def split_video(video_file):
+    vidcap = cv2.VideoCapture(directory_1 + video_file)
+    sec = 0
+    frameRate = 1  # number of seconds between each capture
+    count = 1
+
+    def getFrame(sec):
+        vidcap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000)
+        hasFrames, image = vidcap.read()
+        if hasFrames:
+            date_stamp = video_file.split('_')[2]
+            time_stamp = video_file.split('_')[3].split('.')[0] + "+" + str(sec)
+            if PRINT_DEBUG: print("Time stamp " + time_stamp)
+
+            # save frame as JPG file
+            cv2.imwrite("./frames/frame_"+date_stamp+"_"+time_stamp+"nr"+str(count)+"_"+".jpg", image)
+            #cv2.imwrite("./frames/frame_" + str(count) + ".jpg", image)
+        return hasFrames
+
+    success = getFrame(sec)
+    while success:
+        count = count + 1
+        sec = sec + frameRate
+        sec = round(sec, 2)
+        success = getFrame(sec)
+
+    # When everything done, release the capture
+    vidcap.release()
+    cv2.destroyAllWindows()
+
+#Dir where video files to be split are located
+
+
+for video_file in os.listdir(directory_1):
+    if(video_file.endswith("mp4")):
+        if PRINT_DEBUG: print("Starting splitting of " + video_file + "...")
+        split_video('/' + video_file)
+        if PRINT_DEBUG: print("Splitting of " + video_file + " done.")
\ No newline at end of file
Index: test_set_creator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- test_set_creator.py	(date 1591005416421)
+++ test_set_creator.py	(date 1591005416421)
@@ -0,0 +1,38 @@
+import os
+import random as rand
+
+#Dir where the test_set folder should be placed
+directory_1 = '/Users/august/Documents/EITN35_AIQ/video_files/'
+
+#Dir where the frames folder is located
+directory_2 = '/Users/august/Documents/EITN35_AIQ/video_files/frames/'
+os.chdir(directory_1)
+
+# Create "frames" folder if it does not exist
+try:
+    if not os.path.exists('test_set'):
+        os.makedirs('test_set')
+except OSError:
+    print('Error: Creating directory of data')
+
+os.chdir(directory_2)
+
+count = 0
+wantedSplit = [0.8, 0.1, 0.1] #training, validation, test
+noExtract = len(os.listdir(directory_2))*wantedSplit[2]
+PRINT_DEBUG = True
+
+while count < noExtract:
+    index = rand.randint(1, len(os.listdir(directory_2)))
+    file_list = os.listdir(directory_2)
+
+    #if PRINT_DEBUG : print(str(file_list[index]) + " exported to test_set...")
+
+    os.rename(
+        directory_2 + str(file_list[index]),
+        directory_1 + '/test_set/' + str(file_list[index])
+    )
+    count += 1
+
+print(str(noExtract) + " files exported to test_set")
+
Index: result_run_printer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- result_run_printer.py	(date 1591005416405)
+++ result_run_printer.py	(date 1591005416405)
@@ -0,0 +1,22 @@
+import os
+
+results = os.getcwd() + '/training_results/'
+
+def print_to_file(acc, val_acc, loss, val_loss, epochs, layers, model):
+    os.chdir(results)
+    current_results = os.listdir(results)
+    current_results.sort()
+    working_file = current_results[len(os.listdir(results)) - 1]
+    print("Latest file is: " + str(working_file))
+
+    file = open(working_file, "a+")
+    file.write("\n \n -----------------------------------")
+    file.write("\n Model " + str(model) + " Run Result")
+    file.write("\n -----------------------------------")
+    file.write("\n Hidden Layers:           " + str(layers))
+    file.write("\n Epochs:                  " + str(epochs))
+    file.write("\n Training Set Accuracy:   " + str(acc))
+    file.write("\n Validation Set Accuracy: " + str(val_acc))
+    file.write("\n Training Loss:           " + str(loss))
+    file.write("\n Validation Loss:         " + str(val_loss))
+    file.close()
\ No newline at end of file
Index: object_detection_v4.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- object_detection_v4.py	(date 1591005416405)
+++ object_detection_v4.py	(date 1591005416405)
@@ -0,0 +1,420 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+# Object Detection With YOLOv3 in Keras
+
+# load yolov3 model and perform object detection
+# based on https://github.com/experiencor/keras-yolo3
+import numpy as np
+from numpy import expand_dims
+from keras.models import load_model
+from keras.preprocessing.image import load_img
+from keras.preprocessing.image import img_to_array
+from matplotlib import pyplot
+from matplotlib.patches import Rectangle
+import pandas as pd
+import os
+import math
+#import tensorflow.compat.v1 as tf
+#tf.disable_v2_behavior()
+
+#directory_1 = "/Users/august/Documents/EITN35_AIQ"
+directory_1 = "../EITN35_Resources/"
+os.chdir(directory_1)
+
+
+class BoundBox:
+    def __init__(self, xmin, ymin, xmax, ymax, objness=None, classes=None):
+        self.xmin = xmin
+        self.ymin = ymin
+        self.xmax = xmax
+        self.ymax = ymax
+        self.objness = objness
+        self.classes = classes
+        self.label = -1
+        self.score = -1
+
+    def get_label(self):
+        if self.label == -1:
+            self.label = np.argmax(self.classes)
+
+        return self.label
+
+    def get_score(self):
+        if self.score == -1:
+            self.score = self.classes[self.get_label()]
+
+        return self.score
+
+
+def _sigmoid(x):
+    return 1. / (1. + np.exp(-x))
+
+
+def decode_netout(netout, anchors, obj_thresh, net_h, net_w):
+    grid_h, grid_w = netout.shape[:2]
+    nb_box = 3
+    netout = netout.reshape((grid_h, grid_w, nb_box, -1))
+    nb_class = netout.shape[-1] - 5
+    boxes = []
+    netout[..., :2] = _sigmoid(netout[..., :2])
+    netout[..., 4:] = _sigmoid(netout[..., 4:])
+    netout[..., 5:] = netout[..., 4][..., np.newaxis] * netout[..., 5:]
+    netout[..., 5:] *= netout[..., 5:] > obj_thresh
+
+    for i in range(grid_h * grid_w):
+        row = i / grid_w
+        col = i % grid_w
+        for b in range(nb_box):
+            # 4th element is objectness score
+            objectness = netout[int(row)][int(col)][b][4]
+            if (objectness.all() <= obj_thresh): continue
+            # first 4 elements are x, y, w, and h
+            x, y, w, h = netout[int(row)][int(col)][b][:4]
+            x = (col + x) / grid_w  # center position, unit: image width
+            y = (row + y) / grid_h  # center position, unit: image height
+            w = anchors[2 * b + 0] * np.exp(w) / net_w  # unit: image width
+            h = anchors[2 * b + 1] * np.exp(h) / net_h  # unit: image height
+            # last elements are class probabilities
+            classes = netout[int(row)][col][b][5:]
+            box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness, classes)
+            boxes.append(box)
+    return boxes
+
+
+def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):
+    new_w, new_h = net_w, net_h
+    for i in range(len(boxes)):
+        x_offset, x_scale = (net_w - new_w) / 2. / net_w, float(new_w) / net_w
+        y_offset, y_scale = (net_h - new_h) / 2. / net_h, float(new_h) / net_h
+        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)
+        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)
+        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)
+        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)
+
+
+def _interval_overlap(interval_a, interval_b):
+    x1, x2 = interval_a
+    x3, x4 = interval_b
+    if x3 < x1:
+        if x4 < x1:
+            return 0
+        else:
+            return min(x2, x4) - x1
+    else:
+        if x2 < x3:
+            return 0
+        else:
+            return min(x2, x4) - x3
+
+
+def bbox_iou(box1, box2):
+    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])
+    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])
+    intersect = intersect_w * intersect_h
+    w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin
+    w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin
+    union = w1 * h1 + w2 * h2 - intersect
+    return float(intersect) / union
+
+
+def do_nms(boxes, nms_thresh):
+    if len(boxes) > 0:
+        nb_class = len(boxes[0].classes)
+    else:
+        return
+    for c in range(nb_class):
+        sorted_indices = np.argsort([-box.classes[c] for box in boxes])
+        for i in range(len(sorted_indices)):
+            index_i = sorted_indices[i]
+            if boxes[index_i].classes[c] == 0: continue
+            for j in range(i + 1, len(sorted_indices)):
+                index_j = sorted_indices[j]
+                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:
+                    boxes[index_j].classes[c] = 0
+
+
+# load and prepare an image
+def load_image_pixels(filename, shape):
+    # load the image to get its shape
+    image = load_img(filename)
+    width, height = image.size
+    # load the image with the required size
+    image = load_img(filename, target_size=shape)
+    # convert to numpy array
+    image = img_to_array(image)
+    # scale pixel values to [0, 1]
+    image = image.astype('float32')
+    image /= 255.0
+    # add a dimension so that we have one sample
+    image = expand_dims(image, 0)
+    return image, width, height
+
+
+# get all of the results above a threshold
+def get_boxes(boxes, labels, thresh):
+    v_boxes, v_labels, v_scores = list(), list(), list()
+    # enumerate all boxes
+    for box in boxes:
+        # enumerate all possible labels
+        for i in range(len(labels)):
+            # check if the threshold for this label is high enough
+            if box.classes[i] > thresh:
+                v_boxes.append(box)
+                v_labels.append(labels[i])
+                v_scores.append(box.classes[i] * 100)
+            # don't break, many labels may trigger for one box
+    return v_boxes, v_labels, v_scores
+
+
+# draw all results
+def draw_boxes(filename, v_boxes, v_labels, v_scores):
+    # load the image
+    data = pyplot.imread(filename)
+    # plot the image
+    pyplot.imshow(data)
+    # get the context for drawing boxes
+    ax = pyplot.gca()
+    # plot each box
+    for i in range(len(v_boxes)):
+        box = v_boxes[i]
+        # get coordinates
+        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax
+        # calculate width and height of the box
+        width, height = x2 - x1, y2 - y1
+        # create the shape
+        rect = Rectangle((x1, y1), width, height, fill=False, color='white', lw=2)
+        # draw the box
+        ax.add_patch(rect)
+        # draw text and score in top left corner
+        label = "%s (%.3f)" % (v_labels[i], v_scores[i])
+        pyplot.text(x1, y1, label, color='white')
+    # show the plot
+    #pyplot.show()
+
+
+# load yolov3 model
+model = load_model('model.h5', compile=False)
+# define the expected input shape for the model
+input_w, input_h = 416, 416
+
+# In[77]:
+
+
+# define the labels
+labels = ["person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck",
+          "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench",
+          "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe",
+          "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard",
+          "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",
+          "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana",
+          "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake",
+          "chair", "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse",
+          "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator",
+          "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"]
+
+# define the probability threshold for detected objects
+class_threshold = 0.6
+dist_threshold = 150  # gör denna beroende på framerate som valts
+
+
+# In[78]:
+
+
+class Mem_Object:
+    def __init__(self, label, xmin, xmax, ymin, ymax):
+        self.label = label
+        self.xmin = xmin
+        self.xmax = xmax
+        self.ymin = ymin
+        self.ymax = ymax
+        self.mid = [(xmin + xmax) / 2, (ymin + ymax) / 2]
+
+
+# In[79]:
+
+
+def dist(p1, p2):
+    distance = math.sqrt(((p1[0] - p2[0]) ** 2) + ((p1[1] - p2[1]) ** 2))
+    return distance
+
+
+# In[80]:
+
+
+def check_new_object(v_labels, v_boxes, v_scores, frames_memory):
+    print("Inside check_new_object")
+
+    # create temp counter for this frames
+    temp_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+    new_objects = pd.DataFrame(data=0, columns=range(10), index=['T1'])
+
+    # add all new objects from v_labels with attributes
+    for i in range(len(v_labels)):
+        new_objects[i]['T1'] = Mem_Object(v_labels[i], v_boxes[i].xmin, v_boxes[i].xmax, v_boxes[i].ymin,
+                                          v_boxes[i].ymax)
+        print("Label i objects" + v_labels[i])
+
+    print("Length of new_objects: " + str(len(new_objects)))
+    print("Content of new_objects" + str(new_objects) + "... End of contents")
+    # print("Content of new_objects" + new_objects)
+
+    # loop through new objects and compare to last frame
+    for i in new_objects:
+        # loop until zero -> no more objects
+        if new_objects[i][0] == 0: break  # eller continue
+
+        temp_counter.loc[str(v_labels[i])] += 1
+        # print("New object up counted!")
+        # print("Temp counter inside function: ")
+        # print(temp_counter)
+
+        # placehold_count += 1 #hundar eller persons
+        uniqueObj = True
+
+        # current frame obj
+        cf_obj = new_objects[i][0]
+        print("Mid new object" + str(cf_obj.mid))
+
+        # Check memory for objects in last frame, if same type of object but too close, count down!
+        for j in frames_memory:
+            lf_obj = frames_memory[j][0]  # index 0 is 'F1'
+
+            if (lf_obj != 0):
+
+                if (lf_obj.label == cf_obj.label):
+                    print("Mid old object" + str(lf_obj.mid))
+                    distance = dist(lf_obj.mid, cf_obj.mid)
+                    print("Distance between objects is: " + str(distance))
+
+                    # if same type of object is too close, conclude same object, count down.
+                    if (distance < dist_threshold):
+                        temp_counter.loc[str(v_labels[i])] -= 1
+                        print("Same object!")
+                        uniqueObj = False
+
+        if uniqueObj:
+            print("New object!")  # syns ej på första
+
+    # Update frames_memory
+    for i in range(4):
+        frames_memory.loc['F' + str(5 - i)] = frames_memory.loc['F' + str(4 - i)]
+
+    # Overwrite first row with new objetcts
+    frames_memory.loc['F1'] = new_objects.loc['T1']
+
+    # frames_memory[rand.randint(5,9)]['F1'] = Mem_Object("person", 1, 1, 1, 1)
+    # print("Temp counter")
+    # print(temp_counter)
+    # print("Frames memory")
+    # print(frames_memory)
+    return temp_counter
+
+frames_memory = pd.DataFrame(data=0, columns=range(10), index=['F1', 'F2', 'F3', 'F4', 'F5'])
+# Ladda in ett objekt
+frames_memory
+
+big_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+
+#Dir to be iterated
+directory_2 = "/Users/august/Documents/EITN35_AIQ/video_files/frames/"
+directory_3 = "/Users/august/Documents/EITN35_AIQ/video_files/test_set/"
+os.chdir(directory_2)
+
+# Create "unlabeled_images" folder if it does not exist
+try:
+    if not os.path.exists('unlabeled_images'):
+        os.makedirs('unlabeled_images')
+    if not os.path.exists('autolabeled_images'):
+        os.makedirs('autolabeled_images')
+except OSError:
+    print('Error: Creating directory of data')
+
+#MÅSTE GÖRAS LÄNGRE OM VI HITTAR MER ÄN ETT OBJEKT I VARJE BILD, GÖR HELLRE FÖR LÅNG OCH DROPPA
+annotation_df = pd.DataFrame(data=0,index=np.arange(len(os.listdir(directory_3))),columns="image xmin ymin xmax ymax label".split())
+index = 0
+
+# Store found objects and their positions from previous frame in vector.
+# When new object found compare type and position, then decide to count or not
+# lf_memory = pd.DataFrame(0,columns=['Label','Middle'],index=range(10))
+
+
+for photo_filename in os.listdir(directory_2):
+    if not photo_filename.endswith('jpg'):continue
+    #photo_filename = 'frame_' + str(i + 8) + '.jpg'
+    # define our new photoc
+    # photo_filename = 'man_on_scooter.jpg'
+    # load and prepare image
+    image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))
+    # make prediction
+    yhat = model.predict(image)
+
+    # summarize the shape of the list of arrays
+    print([a.shape for a in yhat])
+
+    # define the anchors
+    anchors = [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]
+
+    boxes = list()
+    for i in range(len(yhat)):
+        # decode the output of the network
+        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)
+    # correct the sizes of the bounding boxes for the shape of the image
+    correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)
+    # suppress non-maximal boxes
+    do_nms(boxes, 0.5)
+
+    # get the details of the detected objects
+    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)
+
+    contains_person = False
+    #load dataframe for csv export
+    for i in range(len(v_labels)):
+        if v_labels[i] == 'person':
+            annotation_df['image'][index] = photo_filename
+            annotation_df['xmin'][index] = v_boxes[i].xmin
+            annotation_df['ymin'][index] = v_boxes[i].ymin
+            annotation_df['xmax'][index] = v_boxes[i].xmax
+            annotation_df['ymax'][index] = v_boxes[i].ymax
+            annotation_df['label'][index] = v_labels[i]
+            index += 1
+            contains_person = True #Om vi har hittat en instance av person flytta inte file till unlabled_files
+
+    # summarize what we found
+    temp_2_counter = check_new_object(v_labels, v_boxes, v_scores, frames_memory)
+
+    # print(temp_2_counter)
+    big_counter.loc["person"] = big_counter.loc["person"] + temp_2_counter.loc["person"]
+    big_counter.loc["dog"] = big_counter.loc["dog"] + temp_2_counter.loc["dog"]
+
+    # draw what we found
+    draw_boxes(photo_filename, v_boxes, v_labels, v_scores)
+    print(str(big_counter))
+
+    #move files that couldn't be labeled with YOLO
+    if (not contains_person):
+        os.rename(
+            directory_3 + photo_filename,
+            directory_3 + 'unlabeled_images/' + photo_filename
+        )
+
+    #print("THIS IS ANNOTATIONS DF")
+    print(str(annotation_df))
+
+
+
+
+#Make CSV-file from auto-annotations
+annotation_df.to_csv('/Users/august/Documents/EITN35_AIQ/Annotations-export_YOLO_auto.csv', index=False, header=True)
+
+
+# In[90]:
+
+
+big_counter.loc['dog']
+
+# In[ ]:
+
+
+
+
Index: object_detection_v5.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- object_detection_v5.py	(date 1591005416405)
+++ object_detection_v5.py	(date 1591005416405)
@@ -0,0 +1,429 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+# Object Detection With YOLOv3 in Keras
+
+# load yolov3 model and perform object detection
+# based on https://github.com/experiencor/keras-yolo3
+import numpy as np
+from numpy import expand_dims
+from keras.models import load_model
+from keras.preprocessing.image import load_img
+from keras.preprocessing.image import img_to_array
+from matplotlib import pyplot
+from matplotlib.patches import Rectangle
+import pandas as pd
+import os
+import math
+#import tensorflow.compat.v1 as tf
+#tf.disable_v2_behavior()
+
+#directory_1 = "/Users/august/Documents/EITN35_AIQ"
+directory_1 = "../EITN35_Resources/"
+os.chdir(directory_1)
+
+
+class BoundBox:
+    def __init__(self, xmin, ymin, xmax, ymax, objness=None, classes=None):
+        self.xmin = xmin
+        self.ymin = ymin
+        self.xmax = xmax
+        self.ymax = ymax
+        self.objness = objness
+        self.classes = classes
+        self.label = -1
+        self.score = -1
+
+    def get_label(self):
+        if self.label == -1:
+            self.label = np.argmax(self.classes)
+
+        return self.label
+
+    def get_score(self):
+        if self.score == -1:
+            self.score = self.classes[self.get_label()]
+
+        return self.score
+
+
+def _sigmoid(x):
+    return 1. / (1. + np.exp(-x))
+
+
+def decode_netout(netout, anchors, obj_thresh, net_h, net_w):
+    grid_h, grid_w = netout.shape[:2]
+    nb_box = 3
+    netout = netout.reshape((grid_h, grid_w, nb_box, -1))
+    nb_class = netout.shape[-1] - 5
+    boxes = []
+    netout[..., :2] = _sigmoid(netout[..., :2])
+    netout[..., 4:] = _sigmoid(netout[..., 4:])
+    netout[..., 5:] = netout[..., 4][..., np.newaxis] * netout[..., 5:]
+    netout[..., 5:] *= netout[..., 5:] > obj_thresh
+
+    for i in range(grid_h * grid_w):
+        row = i / grid_w
+        col = i % grid_w
+        for b in range(nb_box):
+            # 4th element is objectness score
+            objectness = netout[int(row)][int(col)][b][4]
+            if (objectness.all() <= obj_thresh): continue
+            # first 4 elements are x, y, w, and h
+            x, y, w, h = netout[int(row)][int(col)][b][:4]
+            x = (col + x) / grid_w  # center position, unit: image width
+            y = (row + y) / grid_h  # center position, unit: image height
+            w = anchors[2 * b + 0] * np.exp(w) / net_w  # unit: image width
+            h = anchors[2 * b + 1] * np.exp(h) / net_h  # unit: image height
+            # last elements are class probabilities
+            classes = netout[int(row)][col][b][5:]
+            box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness, classes)
+            boxes.append(box)
+    return boxes
+
+
+def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):
+    new_w, new_h = net_w, net_h
+    for i in range(len(boxes)):
+        x_offset, x_scale = (net_w - new_w) / 2. / net_w, float(new_w) / net_w
+        y_offset, y_scale = (net_h - new_h) / 2. / net_h, float(new_h) / net_h
+        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)
+        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)
+        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)
+        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)
+
+
+def _interval_overlap(interval_a, interval_b):
+    x1, x2 = interval_a
+    x3, x4 = interval_b
+    if x3 < x1:
+        if x4 < x1:
+            return 0
+        else:
+            return min(x2, x4) - x1
+    else:
+        if x2 < x3:
+            return 0
+        else:
+            return min(x2, x4) - x3
+
+
+def bbox_iou(box1, box2):
+    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])
+    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])
+    intersect = intersect_w * intersect_h
+    w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin
+    w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin
+    union = w1 * h1 + w2 * h2 - intersect
+    return float(intersect) / union
+
+
+def do_nms(boxes, nms_thresh):
+    if len(boxes) > 0:
+        nb_class = len(boxes[0].classes)
+    else:
+        return
+    for c in range(nb_class):
+        sorted_indices = np.argsort([-box.classes[c] for box in boxes])
+        for i in range(len(sorted_indices)):
+            index_i = sorted_indices[i]
+            if boxes[index_i].classes[c] == 0: continue
+            for j in range(i + 1, len(sorted_indices)):
+                index_j = sorted_indices[j]
+                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:
+                    boxes[index_j].classes[c] = 0
+
+
+# load and prepare an image
+def load_image_pixels(filename, shape):
+    # load the image to get its shape
+    image = load_img(filename)
+    width, height = image.size
+    # load the image with the required size
+    image = load_img(filename, target_size=shape)
+    # convert to numpy array
+    image = img_to_array(image)
+    # scale pixel values to [0, 1]
+    image = image.astype('float32')
+    image /= 255.0
+    # add a dimension so that we have one sample
+    image = expand_dims(image, 0)
+    return image, width, height
+
+
+# get all of the results above a threshold
+def get_boxes(boxes, labels, thresh):
+    v_boxes, v_labels, v_scores = list(), list(), list()
+    # enumerate all boxes
+    for box in boxes:
+        # enumerate all possible labels
+        for i in range(len(labels)):
+            # check if the threshold for this label is high enough
+            if box.classes[i] > thresh:
+                v_boxes.append(box)
+                v_labels.append(labels[i])
+                v_scores.append(box.classes[i] * 100)
+            # don't break, many labels may trigger for one box
+    return v_boxes, v_labels, v_scores
+
+
+# draw all results
+def draw_boxes(filename, v_boxes, v_labels, v_scores):
+    # load the image
+    data = pyplot.imread(filename)
+    # plot the image
+    pyplot.imshow(data)
+    # get the context for drawing boxes
+    ax = pyplot.gca()
+    # plot each box
+    for i in range(len(v_boxes)):
+        box = v_boxes[i]
+        # get coordinates
+        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax
+        # calculate width and height of the box
+        width, height = x2 - x1, y2 - y1
+        # create the shape
+        rect = Rectangle((x1, y1), width, height, fill=False, color='white', lw=2)
+        # draw the box
+        ax.add_patch(rect)
+        # draw text and score in top left corner
+        label = "%s (%.3f)" % (v_labels[i], v_scores[i])
+        pyplot.text(x1, y1, label, color='white')
+    # show the plot
+    #pyplot.show()
+
+
+# load yolov3 model
+model = load_model('model.h5', compile=False)
+# define the expected input shape for the model
+input_w, input_h = 416, 416
+
+# In[77]:
+
+
+# define the labels
+labels = ["person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck",
+          "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench",
+          "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe",
+          "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard",
+          "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",
+          "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana",
+          "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake",
+          "chair", "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse",
+          "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator",
+          "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"]
+
+# define the probability threshold for detected objects
+class_threshold = 0.6
+dist_threshold = 150  # gör denna beroende på framerate som valts
+
+
+# In[78]:
+
+
+class Mem_Object:
+    def __init__(self, label, xmin, xmax, ymin, ymax):
+        self.label = label
+        self.xmin = xmin
+        self.xmax = xmax
+        self.ymin = ymin
+        self.ymax = ymax
+        self.mid = [(xmin + xmax) / 2, (ymin + ymax) / 2]
+
+
+# In[79]:
+
+
+def dist(p1, p2):
+    distance = math.sqrt(((p1[0] - p2[0]) ** 2) + ((p1[1] - p2[1]) ** 2))
+    return distance
+
+
+# In[80]:
+
+
+def check_new_object(v_labels, v_boxes, v_scores, frames_memory):
+    print("Inside check_new_object")
+
+    # create temp counter for this frames
+    temp_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+    new_objects = pd.DataFrame(data=0, columns=range(10), index=['T1'])
+
+    # add all new objects from v_labels with attributes
+    for i in range(len(v_labels)):
+        new_objects[i]['T1'] = Mem_Object(v_labels[i], v_boxes[i].xmin, v_boxes[i].xmax, v_boxes[i].ymin,
+                                          v_boxes[i].ymax)
+        print("Label i objects" + v_labels[i])
+
+    print("Length of new_objects: " + str(len(new_objects)))
+    print("Content of new_objects" + str(new_objects) + "... End of contents")
+    # print("Content of new_objects" + new_objects)
+
+    # loop through new objects and compare to last frame
+    for i in new_objects:
+        # loop until zero -> no more objects
+        if new_objects[i][0] == 0: break  # eller continue
+
+        temp_counter.loc[str(v_labels[i])] += 1
+        # print("New object up counted!")
+        # print("Temp counter inside function: ")
+        # print(temp_counter)
+
+        # placehold_count += 1 #hundar eller persons
+        uniqueObj = True
+
+        # current frame obj
+        cf_obj = new_objects[i][0]
+        print("Mid new object" + str(cf_obj.mid))
+
+        # Check memory for objects in last frame, if same type of object but too close, count down!
+        for j in frames_memory:
+            lf_obj = frames_memory[j][0]  # index 0 is 'F1'
+
+            if (lf_obj != 0):
+
+                if (lf_obj.label == cf_obj.label):
+                    print("Mid old object" + str(lf_obj.mid))
+                    distance = dist(lf_obj.mid, cf_obj.mid)
+                    print("Distance between objects is: " + str(distance))
+
+                    # if same type of object is too close, conclude same object, count down.
+                    if (distance < dist_threshold):
+                        temp_counter.loc[str(v_labels[i])] -= 1
+                        print("Same object!")
+                        uniqueObj = False
+
+        if uniqueObj:
+            print("New object!")  # syns ej på första
+
+    # Update frames_memory
+    for i in range(4):
+        frames_memory.loc['F' + str(5 - i)] = frames_memory.loc['F' + str(4 - i)]
+
+    # Overwrite first row with new objetcts
+    frames_memory.loc['F1'] = new_objects.loc['T1']
+
+    # frames_memory[rand.randint(5,9)]['F1'] = Mem_Object("person", 1, 1, 1, 1)
+    # print("Temp counter")
+    # print(temp_counter)
+    # print("Frames memory")
+    # print(frames_memory)
+    return temp_counter
+
+frames_memory = pd.DataFrame(data=0, columns=range(10), index=['F1', 'F2', 'F3', 'F4', 'F5'])
+# Ladda in ett objekt
+frames_memory
+
+big_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+
+#Dir to be iterated
+directory_2 = 'C:/Users/eitn35/Documents/EITN35/video_files/frames'
+#directory_2 = "/Users/august/Documents/EITN35_AIQ/video_files/frames/"
+directory_3 = "C:/Users/eitn35/Documents/EITN35/video_files/test_set/"
+os.chdir(directory_3)
+
+# Create "unlabeled_images" folder if it does not exist
+try:
+    if not os.path.exists('unlabeled_images'):
+        os.makedirs('unlabeled_images')
+    if not os.path.exists('autolabeled_images'):
+        os.makedirs('autolabeled_images')
+except OSError:
+    print('Error: Creating directory of data')
+
+#MÅSTE GÖRAS LÄNGRE OM VI HITTAR MER ÄN ETT OBJEKT I VARJE BILD, GÖR HELLRE FÖR LÅNG OCH DROPPA
+annotation_df = pd.DataFrame(data=0,index=np.arange(len(os.listdir(directory_3))),columns="image xmin ymin xmax ymax label".split())
+index = 0
+
+# Store found objects and their positions from previous frame in vector.
+# When new object found compare type and position, then decide to count or not
+# lf_memory = pd.DataFrame(0,columns=['Label','Middle'],index=range(10))
+
+
+for photo_filename in os.listdir(directory_3):
+    if not photo_filename.endswith('jpg'):continue
+    #photo_filename = 'frame_' + str(i + 8) + '.jpg'
+    # define our new photoc
+    # photo_filename = 'man_on_scooter.jpg'
+    # load and prepare image
+    image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))
+    # make prediction
+    yhat = model.predict(image)
+
+    # summarize the shape of the list of arrays
+    print([a.shape for a in yhat])
+
+    # define the anchors
+    anchors = [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]
+
+    boxes = list()
+    for i in range(len(yhat)):
+        # decode the output of the network
+        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)
+    # correct the sizes of the bounding boxes for the shape of the image
+    correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)
+    # suppress non-maximal boxes
+    do_nms(boxes, 0.5)
+
+    # get the details of the detected objects
+    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)
+
+    contains_person = False
+    #load dataframe for csv export
+    for i in range(len(v_labels)):
+        if v_labels[i] == 'person':
+            annotation_df['image'][index] = photo_filename
+            annotation_df['xmin'][index] = v_boxes[i].xmin
+            annotation_df['ymin'][index] = v_boxes[i].ymin
+            annotation_df['xmax'][index] = v_boxes[i].xmax
+            annotation_df['ymax'][index] = v_boxes[i].ymax
+            annotation_df['label'][index] = v_labels[i]
+            index += 1
+            contains_person = True #Om vi har hittat en instance av person flytta inte file till unlabled_files
+
+    # summarize what we found
+    temp_2_counter = check_new_object(v_labels, v_boxes, v_scores, frames_memory)
+
+    # print(temp_2_counter)
+    big_counter.loc["person"] = big_counter.loc["person"] + temp_2_counter.loc["person"]
+    big_counter.loc["dog"] = big_counter.loc["dog"] + temp_2_counter.loc["dog"]
+
+    # draw what we found
+    draw_boxes(photo_filename, v_boxes, v_labels, v_scores)
+    print(str(big_counter))
+
+    #move files that couldn't be labeled with YOLO
+    if (not contains_person):
+        os.rename(
+            directory_3 + photo_filename,
+            directory_3 + 'unlabeled_images/' + photo_filename
+        )
+
+
+
+    #print("THIS IS ANNOTATIONS DF")
+    print(str(annotation_df))
+
+
+
+
+#Make CSV-file from auto-annotations
+annotation_df.to_csv('/Users/august/Documents/EITN35_AIQ/Annotations-export_YOLO_auto.csv', index=False, header=True)
+
+for labeled in os.listdir(directory_3):
+    if not labeled.endswith('jpg'): continue
+    os.rename(
+        directory_3 + labeled,
+        directory_3 + 'autolabeled_images/' + labeled
+    )
+
+# In[90]:
+
+
+big_counter.loc['dog']
+
+# In[ ]:
+
+
+
+
Index: Cycle-ImageGenerator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Cycle-ImageGenerator.py	(date 1591005416405)
+++ Cycle-ImageGenerator.py	(date 1591005416405)
@@ -0,0 +1,63 @@
+
+# example of using saved cyclegan models for image translation
+from keras.models import load_model
+from numpy import load
+from numpy import vstack
+from matplotlib import pyplot
+from numpy.random import randint
+from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization
+
+# load and prepare training images
+def load_real_samples(filename):
+	# load the dataset
+	data = load(filename)
+	# unpack arrays
+	X1, X2 = data['arr_0'], data['arr_1']
+	# scale from [0,255] to [-1,1]
+	X1 = (X1 - 127.5) / 127.5
+	X2 = (X2 - 127.5) / 127.5
+	return [X1, X2]
+
+# select a random sample of images from the dataset
+def select_sample(dataset, n_samples):
+	# choose random instances
+	ix = randint(0, dataset.shape[0], n_samples)
+	# retrieve selected images
+	X = dataset[ix]
+	return X
+
+# plot the image, the translation, and the reconstruction
+def show_plot(imagesX, imagesY1, imagesY2):
+	images = vstack((imagesX, imagesY1, imagesY2))
+	titles = ['Real', 'Generated', 'Reconstructed']
+	# scale from [-1,1] to [0,1]
+	images = (images + 1) / 2.0
+	# plot images row by row
+	for i in range(len(images)):
+		# define subplot
+		pyplot.subplot(1, len(images), 1 + i)
+		# turn off axis
+		pyplot.axis('off')
+		# plot raw pixel data
+		pyplot.imshow(images[i])
+		# title
+		pyplot.title(titles[i])
+	pyplot.show()
+
+# load dataset
+A_data, B_data = load_real_samples('../../Documents/EITN35/day2night208all.npz')
+print('Loaded', A_data.shape, B_data.shape)
+# load the models
+cust = {'InstanceNormalization': InstanceNormalization}
+model_AtoB = load_model('g_model_AtoB_000493_D2N208allv3.h5', cust)
+model_BtoA = load_model('g_model_BtoA_000493_D2N208allv3.h5', cust)
+# plot A->B->A
+A_real = select_sample(A_data, 1)
+B_generated  = model_AtoB.predict(A_real)
+A_reconstructed = model_BtoA.predict(B_generated)
+show_plot(A_real, B_generated, A_reconstructed)
+# plot B->A->B
+B_real = select_sample(B_data, 1)
+A_generated  = model_BtoA.predict(B_real)
+B_reconstructed = model_AtoB.predict(A_generated)
+show_plot(B_real, A_generated, B_reconstructed)
Index: automated_layer_testing.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- automated_layer_testing.py	(date 1591005416405)
+++ automated_layer_testing.py	(date 1591005416405)
@@ -0,0 +1,39 @@
+import os
+#project file directory
+project_directory = '/Users/august/PycharmProjects/EITN35/'
+os.chdir(project_directory)
+
+#results directory, will be created if it non-existent
+results = project_directory + '/training_results/'
+
+# Create "training_results" folder if it does not exist
+try:
+    if not os.path.exists('training_results'):
+        os.makedirs('training_results')
+except OSError:
+    print('Error: Creating directory of data')
+
+#retrieves number of previous runs
+runNo = len(os.listdir(results))+1
+file_name = 'object_detection_run_' + str(runNo) + '.txt'
+
+#creates new results file
+os.chdir(results)
+file = open(file_name, "w")
+file.write("Object Detection Run, No:" + str(runNo))
+file.close()
+
+#trains and tests the models with different layer configurations, writes acc, acc_loss, loss and val_loss to file
+os.chdir(project_directory)
+os.system("python CNN_from_scratch_cats_dogs.py")
+# os.system("python model2.py")
+# os.system("python model3.py")
+# os.system("python model4.py")
+
+#Open the file back and print the contents
+os.chdir(results)
+print("Trying to open " + 'object_detection_run_' + str(runNo) + '.txt')
+f = open('object_detection_run_' + str(runNo) + '.txt', "r")
+if f.mode == 'r':
+    contents =f.read()
+    print(contents)
\ No newline at end of file
Index: object_detection_labeling.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- object_detection_labeling.py	(date 1591005416405)
+++ object_detection_labeling.py	(date 1591005416405)
@@ -0,0 +1,442 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+# Object Detection With YOLOv3 in Keras
+
+# load yolov3 model and perform object detection
+# based on https://github.com/experiencor/keras-yolo3
+import numpy as np
+from numpy import expand_dims
+from keras.models import load_model
+from keras.preprocessing.image import load_img
+from keras.preprocessing.image import img_to_array
+from matplotlib import pyplot
+from matplotlib.patches import Rectangle
+import pandas as pd
+import os
+import math
+#import tensorflow.compat.v1 as tf
+#tf.disable_v2_behavior()
+
+#directory_1 = "/Users/august/Documents/EITN35_AIQ"
+directory_1 = "../EITN35_Resources/"
+os.chdir(directory_1)
+
+
+class BoundBox:
+    def __init__(self, xmin, ymin, xmax, ymax, objness=None, classes=None):
+        self.xmin = xmin
+        self.ymin = ymin
+        self.xmax = xmax
+        self.ymax = ymax
+        self.objness = objness
+        self.classes = classes
+        self.label = -1
+        self.score = -1
+
+    def get_label(self):
+        if self.label == -1:
+            self.label = np.argmax(self.classes)
+
+        return self.label
+
+    def get_score(self):
+        if self.score == -1:
+            self.score = self.classes[self.get_label()]
+
+        return self.score
+
+
+def _sigmoid(x):
+    return 1. / (1. + np.exp(-x))
+
+
+def decode_netout(netout, anchors, obj_thresh, net_h, net_w):
+    grid_h, grid_w = netout.shape[:2]
+    nb_box = 3
+    netout = netout.reshape((grid_h, grid_w, nb_box, -1))
+    nb_class = netout.shape[-1] - 5
+    boxes = []
+    netout[..., :2] = _sigmoid(netout[..., :2])
+    netout[..., 4:] = _sigmoid(netout[..., 4:])
+    netout[..., 5:] = netout[..., 4][..., np.newaxis] * netout[..., 5:]
+    netout[..., 5:] *= netout[..., 5:] > obj_thresh
+
+    for i in range(grid_h * grid_w):
+        row = i / grid_w
+        col = i % grid_w
+        for b in range(nb_box):
+            # 4th element is objectness score
+            objectness = netout[int(row)][int(col)][b][4]
+            if (objectness.all() <= obj_thresh): continue
+            # first 4 elements are x, y, w, and h
+            x, y, w, h = netout[int(row)][int(col)][b][:4]
+            x = (col + x) / grid_w  # center position, unit: image width
+            y = (row + y) / grid_h  # center position, unit: image height
+            w = anchors[2 * b + 0] * np.exp(w) / net_w  # unit: image width
+            h = anchors[2 * b + 1] * np.exp(h) / net_h  # unit: image height
+            # last elements are class probabilities
+            classes = netout[int(row)][col][b][5:]
+            box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness, classes)
+            boxes.append(box)
+    return boxes
+
+
+def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):
+    new_w, new_h = net_w, net_h
+    for i in range(len(boxes)):
+        x_offset, x_scale = (net_w - new_w) / 2. / net_w, float(new_w) / net_w
+        y_offset, y_scale = (net_h - new_h) / 2. / net_h, float(new_h) / net_h
+        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)
+        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)
+        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)
+        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)
+
+
+def _interval_overlap(interval_a, interval_b):
+    x1, x2 = interval_a
+    x3, x4 = interval_b
+    if x3 < x1:
+        if x4 < x1:
+            return 0
+        else:
+            return min(x2, x4) - x1
+    else:
+        if x2 < x3:
+            return 0
+        else:
+            return min(x2, x4) - x3
+
+
+def bbox_iou(box1, box2):
+    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])
+    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])
+    intersect = intersect_w * intersect_h
+    w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin
+    w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin
+    union = w1 * h1 + w2 * h2 - intersect
+    return float(intersect) / union
+
+
+def do_nms(boxes, nms_thresh):
+    if len(boxes) > 0:
+        nb_class = len(boxes[0].classes)
+    else:
+        return
+    for c in range(nb_class):
+        sorted_indices = np.argsort([-box.classes[c] for box in boxes])
+        for i in range(len(sorted_indices)):
+            index_i = sorted_indices[i]
+            if boxes[index_i].classes[c] == 0: continue
+            for j in range(i + 1, len(sorted_indices)):
+                index_j = sorted_indices[j]
+                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:
+                    boxes[index_j].classes[c] = 0
+
+
+# load and prepare an image
+def load_image_pixels(filename, shape):
+    # load the image to get its shape
+    image = load_img(filename)
+    width, height = image.size
+    # load the image with the required size
+    image = load_img(filename, target_size=shape)
+    # convert to numpy array
+    image = img_to_array(image)
+    # scale pixel values to [0, 1]
+    image = image.astype('float32')
+    image /= 255.0
+    # add a dimension so that we have one sample
+    image = expand_dims(image, 0)
+    return image, width, height
+
+
+# get all of the results above a threshold
+def get_boxes(boxes, labels, thresh):
+    v_boxes, v_labels, v_scores = list(), list(), list()
+    # enumerate all boxes
+    for box in boxes:
+        # enumerate all possible labels
+        for i in range(len(labels)):
+            # check if the threshold for this label is high enough
+            if box.classes[i] > thresh:
+                v_boxes.append(box)
+                v_labels.append(labels[i])
+                v_scores.append(box.classes[i] * 100)
+            # don't break, many labels may trigger for one box
+    return v_boxes, v_labels, v_scores
+
+
+# draw all results
+def draw_boxes(filename, v_boxes, v_labels, v_scores):
+    # load the image
+    data = pyplot.imread(filename)
+    # plot the image
+    pyplot.imshow(data)
+    # get the context for drawing boxes
+    ax = pyplot.gca()
+    # plot each box
+    for i in range(len(v_boxes)):
+        box = v_boxes[i]
+        # get coordinates
+        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax
+        # calculate width and height of the box
+        width, height = x2 - x1, y2 - y1
+        # create the shape
+        rect = Rectangle((x1, y1), width, height, fill=False, color='white', lw=2)
+        # draw the box
+        ax.add_patch(rect)
+        # draw text and score in top left corner
+        label = "%s (%.3f)" % (v_labels[i], v_scores[i])
+        pyplot.text(x1, y1, label, color='white')
+    # show the plot
+    #pyplot.show()
+
+
+# load yolov3 model
+model = load_model('model.h5', compile=False)
+# define the expected input shape for the model
+input_w, input_h = 416, 416
+
+# In[77]:
+
+
+# define the labels
+labels = ["person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck",
+          "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench",
+          "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe",
+          "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard",
+          "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",
+          "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana",
+          "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake",
+          "chair", "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse",
+          "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator",
+          "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"]
+
+# define the probability threshold for detected objects
+class_threshold = 0.6
+dist_threshold = 150  # gör denna beroende på framerate som valts
+
+
+# In[78]:
+
+
+class Mem_Object:
+    def __init__(self, label, xmin, xmax, ymin, ymax):
+        self.label = label
+        self.xmin = xmin
+        self.xmax = xmax
+        self.ymin = ymin
+        self.ymax = ymax
+        self.mid = [(xmin + xmax) / 2, (ymin + ymax) / 2]
+
+
+# In[79]:
+
+
+def dist(p1, p2):
+    distance = math.sqrt(((p1[0] - p2[0]) ** 2) + ((p1[1] - p2[1]) ** 2))
+    return distance
+
+
+# In[80]:
+
+
+def check_new_object(v_labels, v_boxes, v_scores, frames_memory):
+    print("Inside check_new_object")
+
+    # create temp counter for this frames
+    temp_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+    new_objects = pd.DataFrame(data=0, columns=range(10), index=['T1'])
+
+    # add all new objects from v_labels with attributes
+    for i in range(len(v_labels)):
+        new_objects[i]['T1'] = Mem_Object(v_labels[i], v_boxes[i].xmin, v_boxes[i].xmax, v_boxes[i].ymin,
+                                          v_boxes[i].ymax)
+        print("Label i objects" + v_labels[i])
+
+    print("Length of new_objects: " + str(len(new_objects)))
+    print("Content of new_objects" + str(new_objects) + "... End of contents")
+    # print("Content of new_objects" + new_objects)
+
+    # loop through new objects and compare to last frame
+    for i in new_objects:
+        # loop until zero -> no more objects
+        if new_objects[i][0] == 0: break  # eller continue
+
+        temp_counter.loc[str(v_labels[i])] += 1
+        # print("New object up counted!")
+        # print("Temp counter inside function: ")
+        # print(temp_counter)
+
+        # placehold_count += 1 #hundar eller persons
+        uniqueObj = True
+
+        # current frame obj
+        cf_obj = new_objects[i][0]
+        print("Mid new object" + str(cf_obj.mid))
+
+        # Check memory for objects in last frame, if same type of object but too close, count down!
+        for j in frames_memory:
+            lf_obj = frames_memory[j][0]  # index 0 is 'F1'
+
+            if (lf_obj != 0):
+
+                if (lf_obj.label == cf_obj.label):
+                    print("Mid old object" + str(lf_obj.mid))
+                    distance = dist(lf_obj.mid, cf_obj.mid)
+                    print("Distance between objects is: " + str(distance))
+
+                    # if same type of object is too close, conclude same object, count down.
+                    if (distance < dist_threshold):
+                        temp_counter.loc[str(v_labels[i])] -= 1
+                        print("Same object!")
+                        uniqueObj = False
+
+        if uniqueObj:
+            print("New object!")  # syns ej på första
+
+    # Update frames_memory
+    for i in range(4):
+        frames_memory.loc['F' + str(5 - i)] = frames_memory.loc['F' + str(4 - i)]
+
+    # Overwrite first row with new objetcts
+    frames_memory.loc['F1'] = new_objects.loc['T1']
+
+    # frames_memory[rand.randint(5,9)]['F1'] = Mem_Object("person", 1, 1, 1, 1)
+    # print("Temp counter")
+    # print(temp_counter)
+    # print("Frames memory")
+    # print(frames_memory)
+    return temp_counter
+
+frames_memory = pd.DataFrame(data=0, columns=range(10), index=['F1', 'F2', 'F3', 'F4', 'F5'])
+# Ladda in ett objekt
+frames_memory
+
+big_counter = pd.DataFrame(0, columns=['Number'], index=labels)
+
+#Dir to be iterated
+directory_2 = 'C:/Users/eitn35/Documents/EITN35/video_files/frames/'
+#directory_2 = "/Users/august/Documents/EITN35_AIQ/video_files/frames/"
+directory_3 = "C:/Users/eitn35/Documents/EITN35/video_files/test_set/"
+os.chdir(directory_2)
+
+# Create "unlabeled_images" folder if it does not exist
+try:
+    if not os.path.exists('unlabeled_images'):
+        os.makedirs('unlabeled_images')
+    if not os.path.exists('autolabeled_images'):
+        os.makedirs('autolabeled_images')
+except OSError:
+    print('Error: Creating directory of data')
+
+#MÅSTE GÖRAS LÄNGRE OM VI HITTAR MER ÄN ETT OBJEKT I VARJE BILD, GÖR HELLRE FÖR LÅNG OCH DROPPA
+annotation_df = pd.DataFrame(data=0,index=np.arange(len(os.listdir(directory_2))),columns="image xmin ymin xmax ymax label".split())
+index = 0
+
+# Store found objects and their positions from previous frame in vector.
+# When new object found compare type and position, then decide to count or not
+# lf_memory = pd.DataFrame(0,columns=['Label','Middle'],index=range(10))
+
+
+for photo_filename in os.listdir(directory_2):
+    if not photo_filename.endswith('jpg'):continue
+    #photo_filename = 'frame_' + str(i + 8) + '.jpg'
+    # define our new photoc
+    # photo_filename = 'man_on_scooter.jpg'
+    # load and prepare image
+    image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))
+    # make prediction
+    yhat = model.predict(image)
+
+    # summarize the shape of the list of arrays
+    #print([a.shape for a in yhat])
+
+    # define the anchors
+    anchors = [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]
+
+    boxes = list()
+    for i in range(len(yhat)):
+        # decode the output of the network
+        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)
+    # correct the sizes of the bounding boxes for the shape of the image
+    correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)
+    # suppress non-maximal boxes
+    do_nms(boxes, 0.5)
+
+    # get the details of the detected objects
+    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)
+
+    contains_person = False
+    #load dataframe for csv export
+
+    persons = 0
+    bikes = 0
+    dogs = 0
+
+    for i in range(len(v_labels)):
+       if v_labels[i] == 'person':
+           persons += 1
+
+       if v_labels[i] == 'bicycle':
+           bikes += 1
+
+       if v_labels[i] == 'dog':
+           dogs += 1
+
+
+
+    # summarize what we found
+    # temp_2_counter = check_new_object(v_labels, v_boxes, v_scores, frames_memory)
+
+    # print(temp_2_counter)
+    # big_counter.loc["person"] = big_counter.loc["person"] + temp_2_counter.loc["person"]
+    # big_counter.loc["dog"] = big_counter.loc["dog"] + temp_2_counter.loc["dog"]
+
+    # draw what we found
+    # draw_boxes(photo_filename, v_boxes, v_labels, v_scores)
+    # print(str(big_counter))
+
+    #move files that couldn't be labeled with YOLO
+    counter = persons + bikes + dogs
+    if (counter == 0):
+
+        os.rename(
+            directory_2 + photo_filename,
+            directory_2 + 'unlabeled_images/' + 'persons_0_dogs_0_bikes_0_' + photo_filename
+        )
+    else:
+        os.rename(
+            directory_2 + photo_filename,
+            directory_2 + 'autolabeled_images/' + 'persons_' + str(persons) + '_dogs_' + str(dogs) + '_bikes_' + str(bikes) + '_' + photo_filename
+        )
+
+
+
+    #print("THIS IS ANNOTATIONS DF")
+    #print(str(annotation_df))
+    print('labeling file '+ photo_filename)
+
+
+
+#Make CSV-file from auto-annotations
+#annotation_df.to_csv('/Users/august/Documents/EITN35_AIQ/Annotations-export_YOLO_auto.csv', index=False, header=True)
+
+#for labeled in os.listdir(directory_3):
+ #   if not labeled.endswith('jpg'): continue
+  #  os.rename(
+   #     directory_3 + labeled,
+    #    directory_3 + 'autolabeled_images/' + labeled
+    #)
+
+# In[90]:
+
+
+#big_counter.loc['dog']
+
+# In[ ]:
+
+
+
+
Index: CNN_from_scratch_cats_dogs.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- CNN_from_scratch_cats_dogs.py	(date 1591005416405)
+++ CNN_from_scratch_cats_dogs.py	(date 1591005416405)
@@ -0,0 +1,360 @@
+#!/usr/bin/env python
+# coding: utf-8
+
+# In[1]:
+
+
+import cv2
+import numpy as np
+import pandas as pd
+import sklearn
+import matplotlib.pyplot as plt
+
+import os
+import random
+import gc #garbage collector for cleaning deleted data from memory
+
+
+# In[2]:
+
+
+#train_dir = 'C:/Users/eitn35/Documents/EITN35/video_files/frames/train_images/'
+#test_dir = 'C:/Users/eitn35/Documents/EITN35/video_files/frames/test_images/'
+train_dir = '/Users/august/PycharmProjects/EITN35_Resources/temp_train/'
+test_dir = '/Users/august/PycharmProjects/EITN35_Resources/temp_test/'
+
+#rain_person = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'persons_1' in i]  #get person images
+#rain_dogs = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'dogs_1' in i]  #get dog images
+#rain_bikes = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if 'bikes_1' in i]  #get bike images
+#rain_empty = [train_dir+'{}'.format(i) for i in os.listdir(train_dir) if ('persons_0'and 'dogs_0' and 'bikes_0')in i]  #get bike images
+
+
+test_imgs = [test_dir+'{}'.format(i) for i in os.listdir(test_dir)] #get test images
+
+
+#train_imgs = train_person + train_dogs + train_bikes + train_empty   # slice the dataset and use 3 persons
+train_imgs = [train_dir+'{}'.format(i) for i in os.listdir(train_dir)] #get test images
+random.shuffle(train_imgs)  # shuffle it randomly
+random.shuffle(test_imgs)
+
+
+gc.collect()   #collect garbage to save memory
+
+
+# In[3]:
+
+
+import matplotlib.image as mpimg
+for ima in train_imgs[0:4]:
+     img=mpimg.imread(ima)
+     imgplot = plt.imshow(img)
+     plt.show()
+
+
+# In[4]:
+
+
+#Lets declare our image dimensions
+#we are using coloured images. 
+nrows = 416
+ncolumns = 416
+channels = 3  #change to 1 if you want to use grayscale image
+
+#A function to read and process the images to an acceptable format for our model
+def read_and_process_image(list_of_images):
+    """
+    Returns two arrays: 
+        X is an array of resized images
+        y is an array of labels
+    """
+    X = [] # images
+    y = []# labels
+    i = 0
+    for image in list_of_images:
+        #ändra här mellan COLOR och GRAYSCALE beroende på antal channels
+        X.append(cv2.resize(cv2.imread(image,cv2.IMREAD_COLOR), (nrows,ncolumns), interpolation=cv2.INTER_CUBIC))  #Read the image
+        #get the labels
+        if 'persons_1' in image:
+            y.append(1)
+        elif 'dogs_1' in image:
+            y.append(2)
+        elif 'bikes_1' in image:
+            y.append(3)
+        else:
+            y.append(0)
+        i += 1
+    return X, y
+
+
+# In[5]:
+
+
+class_names = ['empty', 'person', 'dogs', 'bikes']
+
+
+# In[6]:
+
+
+X, y = read_and_process_image(train_imgs)
+X_test, y_test = read_and_process_image(test_imgs)
+
+
+# In[7]:
+
+
+y[0]
+
+
+# In[8]:
+
+
+#Lets view some of the pics
+plt.figure(figsize=(20,10))
+columns = 4
+for i in range(columns):
+    plt.subplot(5 / columns + 1, columns, i + 1)
+    plt.imshow(X[i])
+
+
+# In[9]:
+
+
+import seaborn as sns
+
+gc.collect()
+
+#Convert list to numpy array
+X = np.array(X)
+y = np.array(y)
+X_test = np.array(X_test)
+y_test = np.array(y_test)
+#Lets plot the label to be sure we just have two class
+#sns.countplot(y)
+#plt.title('Labels for Cats and Dogs')
+
+
+# In[10]:
+
+
+print("Shape of train images is:", X.shape)
+print("Shape of labels is:", y.shape)
+
+
+# In[11]:
+
+
+#Lets split the data into train and test set
+from sklearn.model_selection import train_test_split
+X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=2)
+
+print("Shape of train images is:", X_train.shape)
+print("Shape of validation images is:", X_val.shape)
+print("Shape of labels is:", y_train.shape)
+print("Shape of labels is:", y_val.shape)
+
+
+# In[12]:
+
+
+#clear memory
+#del X
+#del y
+gc.collect()
+
+#get the length of the train and validation data
+ntrain = len(X_train)
+nval = len(X_val)
+
+#We will use a batch size of 32. Note: batch size should be a factor of 2.***4,8,16,32,64...***
+batch_size = 32
+
+
+# In[13]:
+
+
+
+#from keras import models
+from keras import optimizers
+from tensorflow.keras import models, layers
+from keras.preprocessing.image import ImageDataGenerator
+from keras.preprocessing.image import img_to_array, load_img
+import tensorflow as tf
+
+model = models.Sequential()
+model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(ncolumns, nrows, 3))) #input ska var (150, 150, 3)
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(64, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Conv2D(128, (3, 3), activation='relu'))
+model.add(layers.MaxPooling2D((2, 2)))
+model.add(layers.Flatten())
+model.add(layers.Dropout(0.5))  #Dropout for regularization
+model.add(layers.Dense(512, activation='relu'))
+model.add(layers.Dense(4))  #Sigmoid function at the end because we have just two classes
+
+
+# In[14]:
+
+
+#Lets see our model
+model.summary()
+
+
+# In[15]:
+
+
+#We'll use the RMSprop optimizer with a learning rate of 0.0001
+#We'll use binary_crossentropy loss because its a binary classification
+model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['acc'])
+
+
+# In[16]:
+
+
+#Lets create the augmentation configuration
+#This helps prevent overfitting, since we are using a small dataset
+train_datagen = ImageDataGenerator(rescale=1./255,   #Scale the image between 0 and 1
+                                    rotation_range=40,
+                                    width_shift_range=0.2,
+                                    height_shift_range=0.2,
+                                    shear_range=0.2,
+                                    zoom_range=0.2,
+                                    horizontal_flip=True,)
+
+val_datagen = ImageDataGenerator(rescale=1./255)  #We do not augment validation data. we only perform rescale
+
+
+# In[17]:
+
+
+
+#Create the image generators
+train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)
+val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)
+
+
+# In[18]:
+
+#The training part
+#We train for 64 epochs with about 100 steps per epoch
+history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))
+
+
+# In[19]:
+
+
+#Save the model
+model.save_weights('model_cat&dog1_weights.h5')
+model.save('model_cat&dog1_keras.h5')
+
+
+# In[20]:
+
+#lets plot the train and val curve
+#get the details form the history object
+acc = history.history['acc']
+val_acc = history.history['val_acc']
+loss = history.history['loss']
+val_loss = history.history['val_loss']
+
+epochs = range(1, len(acc) + 1)
+
+#Train and validation accuracy
+plt.plot(epochs, acc, 'b', label='Training accurarcy')
+plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
+plt.title('Training and Validation accurarcy')
+plt.legend()
+
+plt.figure()
+#Train and validation loss
+plt.plot(epochs, loss, 'b', label='Training loss')
+plt.plot(epochs, val_loss, 'r', label='Validation loss')
+plt.title('Training and Validation loss')
+plt.legend()
+
+plt.show()
+
+
+# In[22]:
+
+
+probability_model = tf.keras.Sequential([model,
+                                         tf.keras.layers.Softmax()])
+
+predictions = probability_model.predict(X_test)
+
+predictions[0]
+
+np.argmax(predictions[0])
+
+y_test[0]
+
+def plot_image(i, predictions_array, true_label, img):
+    predictions_array, true_label, img = predictions_array, true_label[i], img[i]
+    plt.grid(False)
+    plt.xticks([])
+    plt.yticks([])
+    plt.imshow(img, cmap=plt.cm.binary)
+
+    predicted_label = np.argmax(predictions_array)
+    if predicted_label == true_label:
+        color = 'blue'
+    else:
+        color = 'red'
+
+    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
+                                100*np.max(predictions_array),
+                                class_names[true_label]),
+                                color=color)
+
+def plot_value_array(i, predictions_array, true_label):
+    predictions_array, true_label = predictions_array, true_label[i]
+    plt.grid(False)
+    plt.xticks(range(4))
+    plt.yticks([])
+    thisplot = plt.bar(range(4), predictions_array, color="#777777")
+    plt.ylim([0, 1])
+    predicted_label = np.argmax(predictions_array)
+
+    thisplot[predicted_label].set_color('red')
+    thisplot[true_label].set_color('blue')
+
+# Plot the first X test images, their predicted labels, and the true labels.
+# Color correct predictions in blue and incorrect predictions in red.
+num_rows = 5
+num_cols = 3
+num_images = num_rows*num_cols
+plt.figure(figsize=(2*2*num_cols, 2*num_rows))
+for i in range(num_images):
+  plt.subplot(num_rows, 2*num_cols, 2*i+1)
+  plot_image(i, predictions[i], y_test, X_test)
+  plt.subplot(num_rows, 2*num_cols, 2*i+2)
+  plot_value_array(i, predictions[i], y_test)
+plt.tight_layout()
+plt.show()
+
+
+import result_run_printer as printer
+layers = 12
+model_no = 3
+pr = printer
+#printer.print_to_file(0.88, 0.65, 0.01, 0.02, 400, 12, 1)
+
+#caculate the values from averages of the no_av last values
+v_size = len(acc)
+no_av = 5
+acc_av = 0
+val_acc_av = 0
+loss_av = 0
+val_loss_av = 0
+
+for i in range(no_av):
+    acc_av += acc[v_size-i-1]/no_av
+    val_acc_av += val_acc[v_size-i-1]/no_av
+    loss_av += loss[v_size-i-1]/no_av
+    val_loss_av += val_loss[v_size-i-1]/no_av
+
+#print the values of the run to a file
+printer.print_to_file(acc_av, val_acc_av, loss_av, val_loss_av, len(acc), layers, model_no)
\ No newline at end of file
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/misc.xml	(date 1591012185947)
+++ .idea/misc.xml	(date 1591012185947)
@@ -0,0 +1,6 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="PyCharmProfessionalAdvertiser">
+    <option name="shown" value="true" />
+  </component>
+</project>
\ No newline at end of file
Index: .idea/EITN35.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/EITN35.iml	(date 1591012032062)
+++ .idea/EITN35.iml	(date 1591012032062)
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module type="PYTHON_MODULE" version="4">
+  <component name="NewModuleRootManager">
+    <content url="file://$MODULE_DIR$" />
+    <orderEntry type="jdk" jdkName="Python 3.7 (EITN35)" jdkType="Python SDK" />
+    <orderEntry type="sourceFolder" forTests="false" />
+  </component>
+</module>
\ No newline at end of file
diff --git Old/modeltrainer.py Old/modeltrainer.py
new file mode 100644
diff --git .idea/vcs.xml .idea/vcs.xml
new file mode 100644
diff --git .idea/.gitignore .idea/.gitignore
new file mode 100644
diff --git .idea/modules.xml .idea/modules.xml
new file mode 100644
diff --git .idea/inspectionProfiles/profiles_settings.xml .idea/inspectionProfiles/profiles_settings.xml
new file mode 100644
